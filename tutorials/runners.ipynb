{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install --no-build-isolation --extra-index-url https://pypi.org/simple .. > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-07 15:04:10,052 56800:140704380690048][patches.py:9 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n",
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import todd\n",
    "from todd.runners import Memo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register_()\n",
    "class RunnerModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def _forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        runner: todd.runners.BaseRunner,\n",
    "        batch,\n",
    "        memo: Memo,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> Memo:\n",
    "        log: dict[str, Any] | None = memo.get(\"log\")\n",
    "        y = self._forward(batch[\"x\"])\n",
    "        loss = F.l1_loss(y, batch[\"y\"])\n",
    "        memo[\"loss\"] = loss\n",
    "        if log is not None:\n",
    "            log[\"batch\"] = str(batch)\n",
    "            log[\"weight\"] = f\"{self._weight.item():.3f}\"\n",
    "            log[\"loss\"] = f\"{loss:.3f}\"\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register_()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:12,282 56800:140704380690048][base.py:55 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpivghh32q\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:12,628 56800:140704380690048][base.py:55 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:12,633 56800:140704380690048][log.py:91 todd.Validator.validator after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-07 15:04:12,637 56800:140704380690048][log.py:91 todd.Validator.validator after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-07 15:04:12,640 56800:140704380690048][log.py:91 todd.Validator.validator after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-07 15:04:12,644 56800:140704380690048][log.py:91 todd.Validator.validator after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpkoyphpsp\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[dict(type='LogCallback', interval=5)],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:12,960 56800:140704380690048][base.py:55 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:12,965 56800:140704380690048][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [1/8] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.000 loss=14.000\n",
      "[2024-02-07 15:04:12,967 56800:140704380690048][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [2/8] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.000 loss=7.000\n",
      "[2024-02-07 15:04:12,969 56800:140704380690048][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [3/8] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.000 loss=5.000\n",
      "[2024-02-07 15:04:12,971 56800:140704380690048][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [4/8] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.000 loss=17.000\n",
      "[2024-02-07 15:04:12,973 56800:140704380690048][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [5/8] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.000 loss=12.000\n",
      "[2024-02-07 15:04:12,976 56800:140704380690048][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [6/8] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.000 loss=14.000\n",
      "[2024-02-07 15:04:12,978 56800:140704380690048][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [7/8] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.000 loss=7.000\n",
      "[2024-02-07 15:04:12,979 56800:140704380690048][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [8/8] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.000 loss=5.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"iter_based_trainer\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:12,996 56800:140704380690048][base.py:55 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:12,998 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-07 15:04:13,001 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [1/15] batch={'x': tensor([3, 6]), 'y': tensor([ 6, 12])} weight=0.000 loss=9.000\n",
      "[2024-02-07 15:04:13,003 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [2/15] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.000 loss=13.000\n",
      "[2024-02-07 15:04:13,005 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [3/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.000 loss=3.000\n",
      "[2024-02-07 15:04:13,007 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [4/15] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.000 loss=19.000\n",
      "[2024-02-07 15:04:13,010 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [5/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.000 loss=11.000\n",
      "[2024-02-07 15:04:13,011 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-07 15:04:13,014 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [6/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.000 loss=15.000\n",
      "[2024-02-07 15:04:13,016 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [7/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.000 loss=14.000\n",
      "[2024-02-07 15:04:13,018 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [8/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.000 loss=11.000\n",
      "[2024-02-07 15:04:13,021 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [9/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.000 loss=3.000\n",
      "[2024-02-07 15:04:13,023 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [10/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.000 loss=12.000\n",
      "[2024-02-07 15:04:13,024 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-07 15:04:13,026 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [11/15] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.000 loss=3.000\n",
      "[2024-02-07 15:04:13,027 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [12/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.000 loss=18.000\n",
      "[2024-02-07 15:04:13,029 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [13/15] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.000 loss=14.000\n",
      "[2024-02-07 15:04:13,030 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [14/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.000 loss=9.000\n",
      "[2024-02-07 15:04:13,032 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [15/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.000 loss=11.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"epoch_based_trainer\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-07 15:04:13,185 56800:140704380690048][log.py:53 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.7.0\n",
      "todd_version: 0.4.0\n",
      "cuda_home: None\n",
      "git_commit_id: f453075\n",
      "git_status: \n",
      "M  pyproject.toml\n",
      "M  todd/base/__init__.py\n",
      "R  todd/utils/envs.py -> todd/base/envs.py\n",
      "M  todd/base/registries.py\n",
      "M  todd/runners/callbacks/__init__.py\n",
      "A  todd/runners/callbacks/git.py\n",
      "M  todd/runners/callbacks/log.py\n",
      "M  todd/runners/types.py\n",
      "M  todd/utils/__init__.py\n",
      "M  todd/utils/generic_tensors.py\n",
      "MM tutorials/runners.ipynb\n",
      "\u001b[2m[2024-02-07 15:04:13,186 56800:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:13,192 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-07 15:04:13,195 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-07 15:04:13,198 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-07 15:04:13,201 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:13,223 56800:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:13,227 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-07 15:04:13,229 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-07 15:04:13,232 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-07 15:04:13,235 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjurzi5ju\u001b[0m\n",
      "└── \u001b[1;36mlog_callback\u001b[0m\n",
      "    └── 2024-02-07T15-04-13_223241-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2024-02-07 15:04:13,223 56800:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2024-02-07 15:04:13,227 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-07 15:04:13,229 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-07 15:04:13,232 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-07 15:04:13,235 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='log_callback',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type='LogCallback',\n",
    "            interval=5,\n",
    "            with_file_handler=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/log_callback/*.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:13,829 56800:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:14,348 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:01 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-07 15:04:14,864 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:01 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-07 15:04:15,386 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-07 15:04:15,910 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:15,926 56800:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:17,451 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:03 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-07 15:04:21,469 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:04 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-07 15:04:26,497 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:03 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-07 15:04:31,517 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"EMA_ETA\", decay=0.2),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1 * min(10, runner.iter_))\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-07 15:04:31,658 56800:140704380690048][log.py:53 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.7.0\n",
      "todd_version: 0.4.0\n",
      "cuda_home: None\n",
      "git_commit_id: f453075\n",
      "git_status: \n",
      "M  pyproject.toml\n",
      "M  todd/base/__init__.py\n",
      "R  todd/utils/envs.py -> todd/base/envs.py\n",
      "M  todd/base/registries.py\n",
      "M  todd/runners/callbacks/__init__.py\n",
      "A  todd/runners/callbacks/git.py\n",
      "M  todd/runners/callbacks/log.py\n",
      "M  todd/runners/types.py\n",
      "M  todd/utils/__init__.py\n",
      "M  todd/utils/generic_tensors.py\n",
      "MM tutorials/runners.ipynb\n",
      "\u001b[2m[2024-02-07 15:04:31,660 56800:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:31,665 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:00 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-07 15:04:31,668 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:00 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-07 15:04:31,670 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-07 15:04:31,673 56800:140704380690048][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "            with_file_handler=True,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-07 15:04:31,746 56800:140704380690048][git.py:50 todd.Validator.git_callback init] INFO: Saving git diff to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpky6migyw/git_callback/git_diff_2024-02-07T15-04-31_746637-08-00.log\n",
      "\u001b[2m[2024-02-07 15:04:31,749 56800:140704380690048][base.py:55 todd.Validator.git_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff --git a/pyproject.toml b/pyproject.toml\n",
      "index 83382b7..aa38442 100644\n",
      "--- a/pyproject.toml\n",
      "+++ b/pyproject.toml\n",
      "@@ -78,6 +78,7 @@ test = [\n",
      " \n",
      " [project.scripts]\n",
      " configs_diff = 'todd.base.configs:diff_cli'\n",
      "+collect_env = 'todd.base.envs:collect_env_cli'\n",
      " \n",
      " [tool.setuptools.packages.find]\n",
      " include = [\n",
      "diff --git a/todd/base/__init__.py b/todd/base/__init__.py\n",
      "index d68fe67..2f66838 100644\n",
      "--- a/todd/base/__init__.py\n",
      "+++ b/todd/base/__init__.py\n",
      "@@ -3,6 +3,7 @@\n",
      " from . import patches\n",
      " from .bboxes import *\n",
      " from .configs import *\n",
      "+from .envs import *\n",
      " from .eta import *\n",
      " from .filters import *\n",
      " from .logger import *\n",
      "diff --git a/todd/utils/envs.py b/todd/base/envs.py\n",
      "similarity index 73%\n",
      "rename from todd/utils/envs.py\n",
      "rename to todd/base/envs.py\n",
      "index 7360366..1d53439 100644\n",
      "--- a/todd/utils/envs.py\n",
      "+++ b/todd/base/envs.py\n",
      "@@ -11,18 +11,25 @@ __all__ = [\n",
      "     'cuda_home',\n",
      "     'git_commit_id',\n",
      "     'git_status',\n",
      "+    'collect_env',\n",
      " ]\n",
      " \n",
      "+import argparse\n",
      " import importlib.util\n",
      " import os\n",
      " import subprocess  # nosec B404\n",
      " \n",
      "+from .logger import logger\n",
      "+from .registries import EnvRegistry\n",
      " \n",
      "+\n",
      "+@EnvRegistry.register_()\n",
      " def platform(verbose: bool = False) -> str | None:\n",
      "     from platform import platform as _platform\n",
      "     return _platform(terse=not verbose)\n",
      " \n",
      " \n",
      "+@EnvRegistry.register_()\n",
      " def nvidia_smi(verbose: bool = False) -> str | None:\n",
      "     args = 'nvidia-smi -q' if verbose else 'nvidia-smi -L'\n",
      "     try:\n",
      "@@ -38,16 +45,19 @@ def nvidia_smi(verbose: bool = False) -> str | None:\n",
      "         return None\n",
      " \n",
      " \n",
      "+@EnvRegistry.register_()\n",
      " def python_version(verbose: bool = False) -> str | None:\n",
      "     import sys\n",
      "     return sys.version\n",
      " \n",
      " \n",
      "+@EnvRegistry.register_()\n",
      " def pytorch_version(verbose: bool = False) -> str | None:\n",
      "     import torch\n",
      "     return torch.__version__\n",
      " \n",
      " \n",
      "+@EnvRegistry.register_()\n",
      " def torchvision_version(verbose: bool = False) -> str | None:\n",
      "     if not importlib.util.find_spec('torchvision'):\n",
      "         return None\n",
      "@@ -55,6 +65,7 @@ def torchvision_version(verbose: bool = False) -> str | None:\n",
      "     return torchvision.__version__\n",
      " \n",
      " \n",
      "+@EnvRegistry.register_()\n",
      " def opencv_version(verbose: bool = False) -> str | None:\n",
      "     if not importlib.util.find_spec('cv2'):\n",
      "         return None\n",
      "@@ -62,16 +73,19 @@ def opencv_version(verbose: bool = False) -> str | None:\n",
      "     return cv2.__version__\n",
      " \n",
      " \n",
      "+@EnvRegistry.register_()\n",
      " def todd_version(verbose: bool = False) -> str | None:\n",
      "     from .. import __version__\n",
      "     return __version__\n",
      " \n",
      " \n",
      "+@EnvRegistry.register_()\n",
      " def cuda_home(verbose: bool = False) -> str | None:\n",
      "     from torch.utils.cpp_extension import CUDA_HOME\n",
      "     return CUDA_HOME\n",
      " \n",
      " \n",
      "+@EnvRegistry.register_()\n",
      " def git_commit_id(verbose: bool = False) -> str | None:\n",
      "     args = 'git rev-parse HEAD' if verbose else 'git rev-parse --short HEAD'\n",
      "     try:\n",
      "@@ -87,6 +101,7 @@ def git_commit_id(verbose: bool = False) -> str | None:\n",
      "         return os.getenv('GIT_COMMIT_ID')\n",
      " \n",
      " \n",
      "+@EnvRegistry.register_()\n",
      " def git_status(verbose: bool = False) -> str | None:\n",
      "     args = 'git status'\n",
      "     if not verbose:\n",
      "@@ -102,3 +117,22 @@ def git_status(verbose: bool = False) -> str | None:\n",
      "         ).stdout\n",
      "     except subprocess.CalledProcessError:\n",
      "         return None\n",
      "+\n",
      "+\n",
      "+def collect_env(*args, **kwargs) -> str:\n",
      "+    envs = ['']\n",
      "+    for k, v in EnvRegistry.items():\n",
      "+        env = v(*args, **kwargs)\n",
      "+        env = str(env).strip()\n",
      "+        if '\\n' in env:\n",
      "+            env = '\\n' + env\n",
      "+        envs.append(f'{k}: {env}')\n",
      "+    return '\\n'.join(envs)\n",
      "+\n",
      "+\n",
      "+def collect_env_cli() -> None:\n",
      "+    parser = argparse.ArgumentParser(description=\"Collect Environment\")\n",
      "+    parser.add_argument('-v', '--verbose', action='store_true', default=False)\n",
      "+    args = parser.parse_args()\n",
      "+    env = collect_env(verbose=args.verbose)\n",
      "+    logger.info(env)\n",
      "diff --git a/todd/base/registries.py b/todd/base/registries.py\n",
      "index 04b0556..20f82b7 100644\n",
      "--- a/todd/base/registries.py\n",
      "+++ b/todd/base/registries.py\n",
      "@@ -139,7 +139,11 @@ class RegistryMeta(  # type: ignore[misc]\n",
      "             'BritishShorthair'\n",
      "         \"\"\"\n",
      "         if not forced and key in cls:  # noqa: E501 pylint: disable=unsupported-membership-test\n",
      "-            logger.error(\"%s already exist in %s\", key, cls.__name__)\n",
      "+            logger.error(\n",
      "+                \"%s already exist in %s\",  # add more info\n",
      "+                key,\n",
      "+                cls.__name__,\n",
      "+            )\n",
      "             raise KeyError(key)\n",
      "         return super().__setitem__(key, item)\n",
      " \n",
      "@@ -599,19 +603,6 @@ NormRegistry.update(\n",
      " for _, c in inspect.getmembers(tf, inspect.isclass):\n",
      "     TransformRegistry.register_()(c)\n",
      " \n",
      "-EnvRegistry.update({\n",
      "-    'Platform': utils.platform,\n",
      "-    'NVIDIA SMI': utils.nvidia_smi,\n",
      "-    'Python version': utils.python_version,\n",
      "-    'PyTorch version': utils.pytorch_version,\n",
      "-    'TorchVision version': utils.torchvision_version,\n",
      "-    'OpenCV version': utils.opencv_version,\n",
      "-    'Todd version': utils.todd_version,\n",
      "-    'CUDA_HOME': utils.cuda_home,\n",
      "-    'Git commit ID': utils.git_commit_id,\n",
      "-    'Git status': utils.git_status,\n",
      "-})\n",
      "-\n",
      " ClipGradRegistry.register_()(nn.utils.clip_grad_norm_)\n",
      " ClipGradRegistry.register_()(nn.utils.clip_grad_value_)\n",
      " \n",
      "diff --git a/todd/runners/callbacks/__init__.py b/todd/runners/callbacks/__init__.py\n",
      "index c6486b0..c5ebdbb 100644\n",
      "--- a/todd/runners/callbacks/__init__.py\n",
      "+++ b/todd/runners/callbacks/__init__.py\n",
      "@@ -3,6 +3,7 @@ from .base import *\n",
      " from .check import *\n",
      " from .checkpoint import *\n",
      " from .composed import *\n",
      "+from .git import *\n",
      " from .interval import *\n",
      " from .log import *\n",
      " from .lr import *\n",
      "diff --git a/todd/runners/callbacks/git.py b/todd/runners/callbacks/git.py\n",
      "new file mode 100644\n",
      "index 0000000..be3aadd\n",
      "--- /dev/null\n",
      "+++ b/todd/runners/callbacks/git.py\n",
      "@@ -0,0 +1,51 @@\n",
      "+__all__ = [\n",
      "+    'GitCallback',\n",
      "+]\n",
      "+\n",
      "+import subprocess\n",
      "+\n",
      "+from ...base import (\n",
      "+    CallbackRegistry,\n",
      "+)\n",
      "+from ...utils import get_rank, get_timestamp\n",
      "+from .base import BaseCallback\n",
      "+\n",
      "+\n",
      "+@CallbackRegistry.register_()\n",
      "+class GitCallback(BaseCallback):\n",
      "+\n",
      "+    def __init__(\n",
      "+        self,\n",
      "+        *args,\n",
      "+        diff: str | None = None,\n",
      "+        **kwargs,\n",
      "+    ) -> None:\n",
      "+        super().__init__(*args, **kwargs)\n",
      "+        self._diff = diff\n",
      "+\n",
      "+    def init(self) -> None:\n",
      "+        super().init()\n",
      "+        if get_rank() > 0:\n",
      "+            return\n",
      "+        if self._diff is not None:\n",
      "+            args = 'git diff'\n",
      "+            if self._diff:\n",
      "+                args += f' {self._diff}'\n",
      "+            try:\n",
      "+                diff = subprocess.run(\n",
      "+                    args,\n",
      "+                    check=True,\n",
      "+                    stdout=subprocess.PIPE,\n",
      "+                    stderr=subprocess.PIPE,\n",
      "+                    shell=True,  # nosec B602\n",
      "+                    text=True,\n",
      "+                ).stdout\n",
      "+            except subprocess.CalledProcessError as e:\n",
      "+                diff = str(e)\n",
      "+                self._runner.logger.error(e)\n",
      "+            else:\n",
      "+                file = (\n",
      "+                    self._runner.work_dir / f'git_diff_{get_timestamp()}.log'\n",
      "+                )\n",
      "+                self._runner.logger.info('Saving git diff to %s', file)\n",
      "+                file.write_text(diff)\n",
      "diff --git a/todd/runners/callbacks/log.py b/todd/runners/callbacks/log.py\n",
      "index 1d04c9a..a571417 100644\n",
      "--- a/todd/runners/callbacks/log.py\n",
      "+++ b/todd/runners/callbacks/log.py\n",
      "@@ -12,7 +12,6 @@ from ...base import (\n",
      "     BaseETA,\n",
      "     CallbackRegistry,\n",
      "     Config,\n",
      "-    EnvRegistry,\n",
      "     ETARegistry,\n",
      "     Formatter,\n",
      "     Store,\n",
      "@@ -22,8 +21,6 @@ from ..types import Memo\n",
      " from .base import BaseCallback\n",
      " from .interval import IntervalMixin\n",
      " \n",
      "-# TODO: save git diff\n",
      "-\n",
      " \n",
      " @CallbackRegistry.register_()\n",
      " class LogCallback(IntervalMixin, BaseCallback):\n",
      "@@ -51,14 +48,9 @@ class LogCallback(IntervalMixin, BaseCallback):\n",
      "             handler.setFormatter(Formatter())\n",
      "             self._runner.logger.addHandler(handler)\n",
      "         if self._collect_env is not None:\n",
      "-            envs = ['']\n",
      "-            for k, v in EnvRegistry.items():\n",
      "-                env = str(v(**self._collect_env))\n",
      "-                env = env.strip()\n",
      "-                if '\\n' in env:\n",
      "-                    env = '\\n' + env\n",
      "-                envs.append(f'{k}: {env}')\n",
      "-            self._runner.logger.info('\\n'.join(envs))\n",
      "+            from ...base import collect_env  # noqa: E501 pylint: disable=import-outside-toplevel\n",
      "+            env = collect_env(**self._collect_env)\n",
      "+            self._runner.logger.info(env)\n",
      " \n",
      "     def before_run(self, memo: Memo) -> None:\n",
      "         super().before_run(memo)\n",
      "diff --git a/todd/runners/types.py b/todd/runners/types.py\n",
      "index f09f57a..81307e4 100644\n",
      "--- a/todd/runners/types.py\n",
      "+++ b/todd/runners/types.py\n",
      "@@ -4,4 +4,4 @@ __all__ = [\n",
      " \n",
      " from typing import Any\n",
      " \n",
      "-Memo = dict[str, Any]\n",
      "+Memo = dict[str, Any]  # TODO: refine\n",
      "diff --git a/todd/utils/__init__.py b/todd/utils/__init__.py\n",
      "index 68d8266..85d1d65 100644\n",
      "--- a/todd/utils/__init__.py\n",
      "+++ b/todd/utils/__init__.py\n",
      "@@ -1,5 +1,4 @@\n",
      " from .enums import *\n",
      "-from .envs import *\n",
      " from .generic_tensors import *\n",
      " from .metas import *\n",
      " from .misc import *\n",
      "diff --git a/todd/utils/generic_tensors.py b/todd/utils/generic_tensors.py\n",
      "index 130f635..f0e4c54 100644\n",
      "--- a/todd/utils/generic_tensors.py\n",
      "+++ b/todd/utils/generic_tensors.py\n",
      "@@ -8,6 +8,8 @@ from typing import Callable, Generic, Iterable, Literal, TypeVar\n",
      " \n",
      " import torch\n",
      " \n",
      "+# TODO: refactor\n",
      "+\n",
      " T = TypeVar('T', torch.Tensor, list, tuple, dict)\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"git_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"GitCallback\", diff='HEAD -- \":(exclude)*.ipynb\"'),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/git_callback/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:32,067 56800:140704380690048][base.py:55 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:32,071 56800:140704380690048][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.000 loss=7.000\n",
      "[2024-02-07 15:04:32,074 56800:140704380690048][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.018 loss=11.895\n",
      "[2024-02-07 15:04:32,076 56800:140704380690048][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([8, 6]), 'y': tensor([16, 12])} weight=0.047 loss=13.667\n",
      "[2024-02-07 15:04:32,079 56800:140704380690048][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.082 loss=11.505\n",
      "[2024-02-07 15:04:32,081 56800:140704380690048][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.112 loss=9.438\n",
      "[2024-02-07 15:04:32,082 56800:140704380690048][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.137 loss=6.519\n",
      "[2024-02-07 15:04:32,084 56800:140704380690048][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.155 loss=11.070\n",
      "[2024-02-07 15:04:32,086 56800:140704380690048][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([8, 6]), 'y': tensor([16, 12])} weight=0.185 loss=12.705\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"optimize_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:32,100 56800:140704380690048][base.py:55 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:32,102 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.000 loss=14.000 lr=['1.667e-03']\n",
      "[2024-02-07 15:04:32,104 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.012 loss=11.930 lr=['2.333e-03']\n",
      "[2024-02-07 15:04:32,106 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.026 loss=9.872 lr=['3.000e-03']\n",
      "[2024-02-07 15:04:32,108 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.041 loss=8.817 lr=['3.667e-03']\n",
      "[2024-02-07 15:04:32,109 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.057 loss=9.714 lr=['4.333e-03']\n",
      "[2024-02-07 15:04:32,111 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.079 loss=13.448 lr=['5.000e-03']\n",
      "[2024-02-07 15:04:32,113 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.114 loss=11.317 lr=['5.000e-03']\n",
      "[2024-02-07 15:04:32,114 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.144 loss=9.281 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=5),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:32,129 56800:140704380690048][base.py:55 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:32,130 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "[2024-02-07 15:04:32,133 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.000 loss=3.000 lr=['1.667e-03']\n",
      "[2024-02-07 15:04:32,135 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/10] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.002 loss=6.991 lr=['1.667e-03']\n",
      "[2024-02-07 15:04:32,136 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [2/5]\n",
      "[2024-02-07 15:04:32,138 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/10] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.008 loss=2.987 lr=['2.778e-03']\n",
      "[2024-02-07 15:04:32,140 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.012 loss=6.956 lr=['2.778e-03']\n",
      "[2024-02-07 15:04:32,141 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [3/5]\n",
      "[2024-02-07 15:04:32,143 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/10] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.022 loss=5.933 lr=['3.889e-03']\n",
      "[2024-02-07 15:04:32,145 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/10] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.034 loss=3.932 lr=['3.889e-03']\n",
      "[2024-02-07 15:04:32,147 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [4/5]\n",
      "[2024-02-07 15:04:32,148 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/10] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.042 loss=3.917 lr=['5.000e-03']\n",
      "[2024-02-07 15:04:32,150 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/10] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.052 loss=5.845 lr=['5.000e-03']\n",
      "[2024-02-07 15:04:32,151 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [5/5]\n",
      "[2024-02-07 15:04:32,153 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [9/10] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.067 loss=4.833 lr=['5.000e-03']\n",
      "[2024-02-07 15:04:32,155 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [10/10] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.079 loss=4.802 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=4),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=3),\n",
    "            by_epoch=True,\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=5,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-07 15:04:32,169 56800:140704380690048][lr.py:93 todd.IterBasedTrainer.lr_scale_callback _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-02-07 15:04:32,170 56800:140704380690048][base.py:55 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:32,173 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.000 loss=12.000\n",
      "[2024-02-07 15:04:32,175 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.060 loss=11.640\n",
      "[2024-02-07 15:04:32,177 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.120 loss=9.400\n",
      "[2024-02-07 15:04:32,178 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.170 loss=15.555\n",
      "[2024-02-07 15:04:32,180 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.255 loss=3.490\n",
      "[2024-02-07 15:04:32,182 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.275 loss=10.350\n",
      "[2024-02-07 15:04:32,184 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.335 loss=9.990\n",
      "[2024-02-07 15:04:32,186 56800:140704380690048][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.395 loss=8.025\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_scale_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScaleCallback\",\n",
    "            lr_scaler=dict(base_batch_size=1),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:32,209 56800:140704380690048][base.py:55 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:32,211 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([10,  3]), 'y': tensor([20,  6])} weight=0.000 loss=13.000\n",
      "[2024-02-07 15:04:32,212 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_1\n",
      "[2024-02-07 15:04:32,217 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.032 loss=14.756\n",
      "[2024-02-07 15:04:32,218 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-02-07 15:04:32,222 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.070 loss=11.580\n",
      "[2024-02-07 15:04:32,223 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_3\n",
      "[2024-02-07 15:04:32,227 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.100 loss=4.750\n",
      "[2024-02-07 15:04:32,228 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-02-07 15:04:32,232 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.112 loss=9.438\n",
      "[2024-02-07 15:04:32,233 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-02-07 15:04:32,237 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([10,  3]), 'y': tensor([20,  6])} weight=0.137 loss=12.106\n",
      "[2024-02-07 15:04:32,238 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-07 15:04:32,242 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.170 loss=13.725\n",
      "[2024-02-07 15:04:32,243 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-02-07 15:04:32,247 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.207 loss=10.755\n",
      "[2024-02-07 15:04:32,248 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_5\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_7\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_8\u001b[0m\n",
      "\n",
      "12 directories, 40 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-07 15:04:32,686 56800:140704380690048][checkpoint.py:54 todd.IterBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-02-07 15:04:32,690 56800:140704380690048][base.py:82 todd.IterBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-07 15:04:32,691 56800:140704380690048][base.py:55 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:32,694 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.137 loss=12.106\n",
      "[2024-02-07 15:04:32,695 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-07 15:04:32,700 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.170 loss=10.065\n",
      "[2024-02-07 15:04:32,701 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-02-07 15:04:32,705 56800:140704380690048][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.197 loss=9.012\n",
      "[2024-02-07 15:04:32,706 56800:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpwprlf/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {0: {'momentum_buffer': None}}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 5}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.1375))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_5 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_5'\n",
    "    for f in iter_5.glob('*.pth'):\n",
    "        print(f'{f.name}:')\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()\n",
    "\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_5),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:32,747 56800:140704380690048][base.py:55 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:32,748 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-07 15:04:32,750 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.000 loss=6.000\n",
      "[2024-02-07 15:04:32,753 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.015 loss=12.903\n",
      "[2024-02-07 15:04:32,770 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-02-07 15:04:32,774 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([3, 6]), 'y': tensor([ 6, 12])} weight=0.047 loss=8.786\n",
      "[2024-02-07 15:04:32,777 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.070 loss=9.650\n",
      "[2024-02-07 15:04:32,778 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-02-07 15:04:32,782 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.095 loss=16.192\n",
      "[2024-02-07 15:04:32,783 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-07 15:04:32,785 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.138 loss=15.831\n",
      "[2024-02-07 15:04:32,786 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-07 15:04:32,790 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.180 loss=8.190\n",
      "[2024-02-07 15:04:32,792 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.203 loss=15.279\n",
      "[2024-02-07 15:04:32,793 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-02-07 15:04:32,797 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.245 loss=3.510\n",
      "[2024-02-07 15:04:32,799 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.255 loss=6.980\n",
      "[2024-02-07 15:04:32,800 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-07 15:04:32,803 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-07 15:04:32,805 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.275 loss=4.312\n",
      "[2024-02-07 15:04:32,807 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.287 loss=4.281\n",
      "[2024-02-07 15:04:32,808 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-07 15:04:32,812 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.300 loss=11.050\n",
      "[2024-02-07 15:04:32,814 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([7, 9]), 'y': tensor([14, 18])} weight=0.332 loss=13.340\n",
      "[2024-02-07 15:04:32,815 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-07 15:04:32,819 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.372 loss=13.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_12\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_14\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_14\u001b[0m\n",
      "\n",
      "11 directories, 35 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-07 15:04:33,245 56800:140704380690048][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-02-07 15:04:33,249 56800:140704380690048][base.py:82 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-07 15:04:33,250 56800:140704380690048][base.py:55 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:33,251 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-07 15:04:33,255 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.245 loss=7.898\n",
      "[2024-02-07 15:04:33,257 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.268 loss=8.662\n",
      "[2024-02-07 15:04:33,258 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-07 15:04:33,262 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-07 15:04:33,264 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.293 loss=8.538\n",
      "[2024-02-07 15:04:33,266 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.318 loss=8.413\n",
      "[2024-02-07 15:04:33,267 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-07 15:04:33,270 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.343 loss=9.116\n",
      "[2024-02-07 15:04:33,272 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.370 loss=9.780\n",
      "[2024-02-07 15:04:33,294 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-07 15:04:33,315 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.400 loss=9.600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-07 15:04:33,743 56800:140704380690048][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-07 15:04:33,747 56800:140704380690048][base.py:82 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-07 15:04:33,748 56800:140704380690048][base.py:55 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:33,749 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-07 15:04:33,752 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.293 loss=11.099\n",
      "[2024-02-07 15:04:33,754 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([1, 5]), 'y': tensor([ 2, 10])} weight=0.325 loss=5.025\n",
      "[2024-02-07 15:04:33,755 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-07 15:04:33,760 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.340 loss=7.470\n",
      "[2024-02-07 15:04:33,762 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.363 loss=14.738\n",
      "[2024-02-07 15:04:33,763 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2giquc4/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-07 15:04:33,767 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.407 loss=7.166\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=2),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_8 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_8'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_8),\n",
    "        )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    iter_10 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_10'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_10),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:33,796 56800:140704380690048][base.py:55 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:33,797 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-07 15:04:33,799 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.000 loss=4.000\n",
      "[2024-02-07 15:04:33,801 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([10,  5]), 'y': tensor([20, 10])} weight=0.010 loss=14.925\n",
      "[2024-02-07 15:04:33,803 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.047 loss=12.691\n",
      "[2024-02-07 15:04:33,805 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.080 loss=12.480\n",
      "[2024-02-07 15:04:33,808 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.112 loss=9.438\n",
      "[2024-02-07 15:04:33,809 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpbnsfxij1/checkpoint_callback/checkpoints/epoch_1\n",
      "[2024-02-07 15:04:33,813 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-07 15:04:33,815 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.137 loss=11.175\n",
      "[2024-02-07 15:04:33,817 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([8, 6]), 'y': tensor([16, 12])} weight=0.167 loss=12.827\n",
      "[2024-02-07 15:04:33,819 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.202 loss=5.392\n",
      "[2024-02-07 15:04:33,821 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.217 loss=7.130\n",
      "[2024-02-07 15:04:33,823 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.237 loss=13.219\n",
      "[2024-02-07 15:04:33,824 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpbnsfxij1/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-02-07 15:04:33,828 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-07 15:04:33,831 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([8, 7]), 'y': tensor([16, 14])} weight=0.275 loss=12.938\n",
      "[2024-02-07 15:04:33,833 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([3, 6]), 'y': tensor([ 6, 12])} weight=0.312 loss=7.594\n",
      "[2024-02-07 15:04:33,835 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.335 loss=5.827\n",
      "[2024-02-07 15:04:33,838 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.352 loss=10.709\n",
      "[2024-02-07 15:04:33,841 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.385 loss=8.882\n",
      "[2024-02-07 15:04:33,842 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpbnsfxij1/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpbnsfxij1\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36mepoch_3\u001b[0m\n",
      "\n",
      "7 directories, 15 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-07 15:04:34,273 56800:140704380690048][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpbnsfxij1/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-02-07 15:04:34,276 56800:140704380690048][base.py:82 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-07 15:04:34,277 56800:140704380690048][base.py:55 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:34,278 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-07 15:04:34,282 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([8, 7]), 'y': tensor([16, 14])} weight=0.275 loss=12.938\n",
      "[2024-02-07 15:04:34,284 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.312 loss=7.594\n",
      "[2024-02-07 15:04:34,286 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.335 loss=12.487\n",
      "[2024-02-07 15:04:34,288 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.372 loss=8.137\n",
      "[2024-02-07 15:04:34,290 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.397 loss=4.807\n",
      "[2024-02-07 15:04:34,291 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpbnsfxij1/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'epoch_2'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(epoch_2),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register_()\n",
    "class FaultyValidator(todd.runners.Validator):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError(\"faulty runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:34,330 56800:140704380690048][base.py:55 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-02-07 15:04:34,332 56800:140704380690048][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x1575fcb90>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 223, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_56800/1715875531.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2024-02-07 15:04:34,330 56800:140704380690048][base.py:55 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2024-02-07 15:04:34,332 56800:140704380690048][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x1575fcb90>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 223, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_56800/1715875531.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='FaultyValidator',\n",
    "    name='monitor_callback',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[\n",
    "        dict(type='MonitorCallback'),\n",
    "        dict(type='LogCallback', interval=5, with_file_handler=True),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/monitor_callback/*.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:34,648 56800:140704380690048][base.py:55 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:34,650 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-07 15:04:34,653 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.000 loss=9.000\n",
      "[2024-02-07 15:04:34,656 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.022 loss=2.966\n",
      "[2024-02-07 15:04:34,658 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([8, 3]), 'y': tensor([16,  6])} weight=0.030 loss=10.835\n",
      "[2024-02-07 15:04:34,660 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([7, 9]), 'y': tensor([14, 18])} weight=0.057 loss=15.540\n",
      "[2024-02-07 15:04:34,662 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.097 loss=15.220\n",
      "[2024-02-07 15:04:34,664 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpo8hstrjt/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-02-07 15:04:34,667 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-07 15:04:34,669 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.137 loss=6.519\n",
      "[2024-02-07 15:04:34,671 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.155 loss=9.225\n",
      "[2024-02-07 15:04:34,673 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.180 loss=8.190\n",
      "[2024-02-07 15:04:34,675 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.202 loss=8.988\n",
      "[2024-02-07 15:04:34,676 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.227 loss=16.839\n",
      "[2024-02-07 15:04:34,677 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpo8hstrjt/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-02-07 15:04:34,681 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-07 15:04:34,683 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.275 loss=6.038\n",
      "[2024-02-07 15:04:34,685 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.292 loss=6.830\n",
      "[2024-02-07 15:04:34,687 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.312 loss=12.656\n",
      "[2024-02-07 15:04:34,689 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.350 loss=8.250\n",
      "[2024-02-07 15:04:34,691 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([10,  5]), 'y': tensor([20, 10])} weight=0.375 loss=12.188\n",
      "[2024-02-07 15:04:34,692 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpo8hstrjt/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-07 15:04:35,115 56800:140704380690048][base.py:55 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-07 15:04:35,116 56800:140704380690048][base.py:97 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpo8hstrjt/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-02-07 15:04:35,119 56800:140704380690048][base.py:82 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "[2024-02-07 15:04:35,120 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-07 15:04:35,123 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.275 loss=9.488\n",
      "[2024-02-07 15:04:35,125 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([7, 3]), 'y': tensor([14,  6])} weight=0.302 loss=8.488\n",
      "[2024-02-07 15:04:35,127 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.327 loss=9.199\n",
      "[2024-02-07 15:04:35,129 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.355 loss=11.515\n",
      "[2024-02-07 15:04:35,130 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.390 loss=7.245\n",
      "[2024-02-07 15:04:35,132 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpo8hstrjt/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-02-07 15:04:35,135 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-07 15:04:35,139 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.412 loss=7.938\n",
      "[2024-02-07 15:04:35,144 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([2, 5]), 'y': tensor([ 4, 10])} weight=0.438 loss=5.469\n",
      "[2024-02-07 15:04:35,149 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.455 loss=12.360\n",
      "[2024-02-07 15:04:35,150 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([8, 7]), 'y': tensor([16, 14])} weight=0.495 loss=11.288\n",
      "[2024-02-07 15:04:35,180 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.533 loss=5.136\n",
      "[2024-02-07 15:04:35,189 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpo8hstrjt/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-02-07 15:04:35,194 56800:140704380690048][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-07 15:04:35,210 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.550 loss=6.525\n",
      "[2024-02-07 15:04:35,216 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.572 loss=7.138\n",
      "[2024-02-07 15:04:35,220 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.597 loss=11.921\n",
      "[2024-02-07 15:04:35,226 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.640 loss=7.480\n",
      "[2024-02-07 15:04:35,229 56800:140704380690048][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.667 loss=5.330\n",
      "[2024-02-07 15:04:35,230 56800:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpo8hstrjt/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"strategy_load_model_from\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = (pathlib.Path(work_dirs) / 'strategy_load_model_from' / 'checkpoints' / 'epoch_2' / 'model.pth')\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.strategy.load_model_from(epoch_2)\n",
    "    runner.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
