{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Models and Datasets as Usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-10 19:18:16,554 36763:4308452736][loggers.py:110 todd.base.patches.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-10 19:18:16,560 36763:4308452736][patches.py:36 todd.base.patches.<module>] INFO: `ipdb` is installed. Using it for debugging.\n",
      "\u001b[2m[2023-01-10 19:18:18,122 36763:4308452736][loggers.py:110 todd.base.registries.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import todd\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models should be built by users.\n",
    "The same model can be used by multiple runners, such as a trainer and a validator, simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class Model(todd.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(2.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> float:\n",
    "        return self._weight.item()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to models, datasets are built inside runners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> int:\n",
    "        return self._data[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Mixin for All Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerMixin(todd.utils.BaseRunner):\n",
    "\n",
    "    def _build_dataloader(\n",
    "        self,\n",
    "        config: todd.Config,\n",
    "    ) -> torch.utils.data.DataLoader:\n",
    "        dataset = Dataset(**config.pop('dataset'))\n",
    "        return torch.utils.data.DataLoader(dataset, **config)\n",
    "\n",
    "    def _run_iter(self, i: int, batch, memo: todd.utils.Memo) -> torch.Tensor:\n",
    "        y: torch.Tensor = self._model(batch)\n",
    "        loss = y.sum().abs()\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DRY_RUN` is turned on by default when CUDA devices are not available.\n",
    "To override this setting, manually set `DRY_RUN` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.utils.BaseRunner.Store.DRY_RUN = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import cast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and register the validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.utils.RunnerRegistry.register()\n",
    "class CustomValidator(RunnerMixin, todd.utils.Validator):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the validator config. \n",
    "`config` will be reused by trainers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = todd.Config(\n",
    "    model=model,\n",
    "    log=dict(interval=5),\n",
    "    load_state_dict=dict(model=dict(strict=False)),\n",
    "    state_dict=dict(model=dict()),\n",
    ")\n",
    "validator = todd.Config(\n",
    "    type='CustomValidator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(n=20)),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and run the validator.\n",
    "Logs will be saved to the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-10 19:18:22,090 36763:4308452736][loggers.py:110 todd.utils.runners.4407437312.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-10 19:18:22,092 36763:4308452736][runners.py:166 todd.utils.runners.4407437312._run] INFO: Iter [5/20] Loss 10.000\n",
      "[2023-01-10 19:18:22,093 36763:4308452736][runners.py:166 todd.utils.runners.4407437312._run] INFO: Iter [10/20] Loss 20.000\n",
      "[2023-01-10 19:18:22,095 36763:4308452736][runners.py:166 todd.utils.runners.4407437312._run] INFO: Iter [15/20] Loss 30.000\n",
      "[2023-01-10 19:18:22,097 36763:4308452736][runners.py:166 todd.utils.runners.4407437312._run] INFO: Iter [20/20] Loss 40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20230110T191822f.log']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    validator.name = work_dir\n",
    "    runner = todd.utils.RunnerRegistry.build(validator, config)\n",
    "    cast(CustomValidator, runner).run()\n",
    "    print(os.listdir(work_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = dict(\n",
    "    dataloader=dict(batch_size=2, dataset=dict(n=67)),\n",
    "    optimizer=dict(type='SGD', lr=0.01),\n",
    "    load_state_dict=dict(optimizer=dict()),\n",
    "    state_dict=dict(optimizer=dict(), interval=20),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.utils.RunnerRegistry.register()\n",
    "class CustomIterBasedTrainer(RunnerMixin, todd.utils.IterBasedTrainer):\n",
    "\n",
    "    def _before_run_iter_log(\n",
    "        self,\n",
    "        i: int,\n",
    "        batch,\n",
    "        memo: todd.utils.Memo,\n",
    "    ) -> str | None:\n",
    "        info = super()._before_run_iter_log(i, batch, memo)\n",
    "        if info is None:\n",
    "            info = ''\n",
    "        model: Model = self.model\n",
    "        info += f\" Weight {model.weight:.3f}\"\n",
    "        info += f\" Batch {batch}\"\n",
    "        return info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `_before_run_iter_log` returns `None`, meaning that no message will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-10 19:18:22,189 36763:4308452736][loggers.py:110 todd.utils.runners.4407343136.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-10 19:18:22,191 36763:4308452736][runners.py:154 todd.utils.runners.4407343136._run] INFO:  Weight 1.640 Batch tensor([ 9, 10])\n",
      "[2023-01-10 19:18:22,192 36763:4308452736][runners.py:166 todd.utils.runners.4407343136._run] INFO: Iter [6/53] Loss 31.160\n",
      "[2023-01-10 19:18:22,194 36763:4308452736][runners.py:154 todd.utils.runners.4407343136._run] INFO:  Weight 0.290 Batch tensor([19, 20])\n",
      "[2023-01-10 19:18:22,195 36763:4308452736][runners.py:166 todd.utils.runners.4407343136._run] INFO: Iter [11/53] Loss 11.310\n",
      "[2023-01-10 19:18:22,198 36763:4308452736][runners.py:154 todd.utils.runners.4407343136._run] INFO:  Weight -0.180 Batch tensor([29, 30])\n",
      "[2023-01-10 19:18:22,199 36763:4308452736][runners.py:166 todd.utils.runners.4407343136._run] INFO: Iter [16/53] Loss 10.620\n",
      "[2023-01-10 19:18:22,201 36763:4308452736][runners.py:154 todd.utils.runners.4407343136._run] INFO:  Weight 0.490 Batch tensor([39, 40])\n",
      "[2023-01-10 19:18:22,202 36763:4308452736][runners.py:270 todd.utils.runners.4407343136.validate] INFO: Skipping validation\n",
      "[2023-01-10 19:18:22,203 36763:4308452736][runners.py:188 todd.utils.runners.4407343136.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpc3wm5x97/iter_20.pth\n",
      "[2023-01-10 19:18:22,205 36763:4308452736][runners.py:166 todd.utils.runners.4407343136._run] INFO: Iter [21/53] Loss 38.710\n",
      "[2023-01-10 19:18:22,207 36763:4308452736][runners.py:154 todd.utils.runners.4407343136._run] INFO:  Weight -0.380 Batch tensor([49, 50])\n",
      "[2023-01-10 19:18:22,208 36763:4308452736][runners.py:166 todd.utils.runners.4407343136._run] INFO: Iter [26/53] Loss 37.620\n",
      "[2023-01-10 19:18:22,209 36763:4308452736][runners.py:154 todd.utils.runners.4407343136._run] INFO:  Weight 0.690 Batch tensor([59, 60])\n",
      "[2023-01-10 19:18:22,210 36763:4308452736][runners.py:166 todd.utils.runners.4407343136._run] INFO: Iter [31/53] Loss 82.110\n",
      "[2023-01-10 19:18:22,213 36763:4308452736][runners.py:154 todd.utils.runners.4407343136._run] INFO:  Weight -0.040 Batch tensor([ 9, 10])\n",
      "[2023-01-10 19:18:22,214 36763:4308452736][runners.py:166 todd.utils.runners.4407343136._run] INFO: Iter [40/53] Loss 0.760\n",
      "[2023-01-10 19:18:22,215 36763:4308452736][runners.py:270 todd.utils.runners.4407343136.validate] INFO: Skipping validation\n",
      "[2023-01-10 19:18:22,216 36763:4308452736][runners.py:188 todd.utils.runners.4407343136.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpc3wm5x97/iter_40.pth\n",
      "[2023-01-10 19:18:22,219 36763:4308452736][runners.py:154 todd.utils.runners.4407343136._run] INFO:  Weight 0.230 Batch tensor([19, 20])\n",
      "[2023-01-10 19:18:22,220 36763:4308452736][runners.py:166 todd.utils.runners.4407343136._run] INFO: Iter [45/53] Loss 8.970\n",
      "[2023-01-10 19:18:22,221 36763:4308452736][runners.py:154 todd.utils.runners.4407343136._run] INFO:  Weight -0.240 Batch tensor([29, 30])\n",
      "[2023-01-10 19:18:22,222 36763:4308452736][runners.py:166 todd.utils.runners.4407343136._run] INFO: Iter [50/53] Loss 14.160\n",
      "[2023-01-10 19:18:22,223 36763:4308452736][runners.py:188 todd.utils.runners.4407343136.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpc3wm5x97/latest.pth\n",
      "[2023-01-10 19:18:22,225 36763:4308452736][runners.py:270 todd.utils.runners.4407343136.validate] INFO: Skipping validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iter_20.pth', 'iter_40.pth', 'latest.pth', '20230110T191822f.log']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    iter_based_trainer = trainer.copy()\n",
    "    iter_based_trainer.update(\n",
    "        type='CustomIterBasedTrainer',\n",
    "        name=work_dir,\n",
    "        iters=53,\n",
    "    )\n",
    "    runner = todd.utils.RunnerRegistry.build(iter_based_trainer, config)\n",
    "    cast(CustomIterBasedTrainer, runner).run()\n",
    "    print(os.listdir(work_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainers increment `todd.Store.ITER` to keep track of the training progress.\n",
    "If multiple trainers are to be run, `todd.Store.ITER` must be manually reset to zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.utils.RunnerRegistry.register()\n",
    "class CustomEpochBasedTrainer(RunnerMixin, todd.utils.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-10 19:18:22,284 36763:4308452736][loggers.py:110 todd.utils.runners.4407439232.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-10 19:18:22,285 36763:4308452736][runners.py:423 todd.utils.runners.4407439232._run] INFO: Epoch [1/3] beginning\n",
      "[2023-01-10 19:18:22,287 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [5/34] Loss 1.330\n",
      "[2023-01-10 19:18:22,289 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [10/34] Loss 7.800\n",
      "[2023-01-10 19:18:22,291 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [15/34] Loss 15.930\n",
      "[2023-01-10 19:18:22,292 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [20/34] Loss 31.600\n",
      "[2023-01-10 19:18:22,294 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [25/34] Loss 46.530\n",
      "[2023-01-10 19:18:22,296 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [30/34] Loss 71.400\n",
      "[2023-01-10 19:18:22,298 36763:4308452736][runners.py:188 todd.utils.runners.4407439232.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmppw22pjz0/epoch_1.pth\n",
      "[2023-01-10 19:18:22,300 36763:4308452736][runners.py:436 todd.utils.runners.4407439232._run] INFO: Epoch [2/3] ended\n",
      "[2023-01-10 19:18:22,301 36763:4308452736][runners.py:423 todd.utils.runners.4407439232._run] INFO: Epoch [2/3] beginning\n",
      "[2023-01-10 19:18:22,303 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [5/34] Loss 1.710\n",
      "[2023-01-10 19:18:22,304 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [10/34] Loss 7.020\n",
      "[2023-01-10 19:18:22,305 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [15/34] Loss 17.110\n",
      "[2023-01-10 19:18:22,307 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [20/34] Loss 30.020\n",
      "[2023-01-10 19:18:22,308 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [25/34] Loss 48.510\n",
      "[2023-01-10 19:18:22,309 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [30/34] Loss 69.020\n",
      "[2023-01-10 19:18:22,311 36763:4308452736][runners.py:188 todd.utils.runners.4407439232.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmppw22pjz0/epoch_2.pth\n",
      "[2023-01-10 19:18:22,313 36763:4308452736][runners.py:436 todd.utils.runners.4407439232._run] INFO: Epoch [3/3] ended\n",
      "[2023-01-10 19:18:22,314 36763:4308452736][runners.py:423 todd.utils.runners.4407439232._run] INFO: Epoch [3/3] beginning\n",
      "[2023-01-10 19:18:22,316 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [5/34] Loss 1.710\n",
      "[2023-01-10 19:18:22,317 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [10/34] Loss 7.020\n",
      "[2023-01-10 19:18:22,319 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [15/34] Loss 17.110\n",
      "[2023-01-10 19:18:22,320 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [20/34] Loss 30.020\n",
      "[2023-01-10 19:18:22,321 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [25/34] Loss 48.510\n",
      "[2023-01-10 19:18:22,322 36763:4308452736][runners.py:166 todd.utils.runners.4407439232._run] INFO: Iter [30/34] Loss 69.020\n",
      "[2023-01-10 19:18:22,324 36763:4308452736][runners.py:188 todd.utils.runners.4407439232.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmppw22pjz0/epoch_3.pth\n",
      "[2023-01-10 19:18:22,325 36763:4308452736][runners.py:436 todd.utils.runners.4407439232._run] INFO: Epoch [4/3] ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['epoch_1.pth', 'epoch_2.pth', 'epoch_3.pth', '20230110T191822f.log']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    epoch_based_trainer = trainer.copy()\n",
    "    epoch_based_trainer.update(\n",
    "        type='CustomEpochBasedTrainer',\n",
    "        name=work_dir,\n",
    "        epochs=3,\n",
    "    )\n",
    "    runner = todd.utils.RunnerRegistry.build(epoch_based_trainer, config)\n",
    "    cast(CustomEpochBasedTrainer, runner).run()\n",
    "    print(os.listdir(work_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.utils.BaseRunner.Store.DRY_RUN = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `DRY_RUN` is enabled, the runner will stop upon the first log message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-10 19:18:22,395 36763:4308452736][loggers.py:110 todd.utils.runners.6438374992.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-10 19:18:22,397 36763:4308452736][runners.py:166 todd.utils.runners.6438374992._run] INFO: Iter [5/20] Loss 0.050\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    validator.name = work_dir\n",
    "    runner = todd.utils.RunnerRegistry.build(validator, config)\n",
    "    cast(CustomValidator, runner).run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
