{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install --no-build-isolation --extra-index-url https://pypi.org/simple .. > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-10 18:30:31,203 82437:140704363290240][patches.py:9 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n",
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import todd\n",
    "from todd.runners import Memo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register_()\n",
    "class RunnerModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def _forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        runner: todd.runners.BaseRunner,\n",
    "        batch,\n",
    "        memo: Memo,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> Memo:\n",
    "        log: dict[str, Any] | None = memo.get(\"log\")\n",
    "        y = self._forward(batch[\"x\"])\n",
    "        loss = F.l1_loss(y, batch[\"y\"])\n",
    "        memo[\"loss\"] = loss\n",
    "        if log is not None:\n",
    "            log[\"batch\"] = str(batch)\n",
    "            log[\"weight\"] = f\"{self._weight.item():.3f}\"\n",
    "            log[\"loss\"] = f\"{loss:.3f}\"\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register_()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:35,638 82437:140704363290240][base.py:54 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpxp2c869n\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:36,136 82437:140704363290240][base.py:54 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:36,142 82437:140704363290240][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-10 18:30:36,147 82437:140704363290240][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-10 18:30:36,151 82437:140704363290240][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-10 18:30:36,155 82437:140704363290240][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpbsgc4onq\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[dict(type='LogCallback', interval=5)],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:36,522 82437:140704363290240][base.py:54 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:36,526 82437:140704363290240][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [1/8] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.000 loss=3.000\n",
      "[2024-02-10 18:30:36,529 82437:140704363290240][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [2/8] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.000 loss=13.000\n",
      "[2024-02-10 18:30:36,532 82437:140704363290240][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [3/8] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.000 loss=18.000\n",
      "[2024-02-10 18:30:36,535 82437:140704363290240][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [4/8] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.000 loss=9.000\n",
      "[2024-02-10 18:30:36,537 82437:140704363290240][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [5/8] batch={'x': tensor([3, 9]), 'y': tensor([ 6, 18])} weight=0.000 loss=12.000\n",
      "[2024-02-10 18:30:36,540 82437:140704363290240][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [6/8] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.000 loss=3.000\n",
      "[2024-02-10 18:30:36,542 82437:140704363290240][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [7/8] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.000 loss=13.000\n",
      "[2024-02-10 18:30:36,545 82437:140704363290240][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [8/8] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.000 loss=18.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"iter_based_trainer\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:36,567 82437:140704363290240][base.py:54 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:36,570 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-10 18:30:36,572 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [1/15] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.000 loss=14.000\n",
      "[2024-02-10 18:30:36,575 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [2/15] batch={'x': tensor([8, 1]), 'y': tensor([16,  2])} weight=0.000 loss=9.000\n",
      "[2024-02-10 18:30:36,577 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [3/15] batch={'x': tensor([2, 7]), 'y': tensor([ 4, 14])} weight=0.000 loss=9.000\n",
      "[2024-02-10 18:30:36,580 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [4/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.000 loss=7.000\n",
      "[2024-02-10 18:30:36,582 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [5/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.000 loss=16.000\n",
      "[2024-02-10 18:30:36,584 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-10 18:30:36,587 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [6/15] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.000 loss=14.000\n",
      "[2024-02-10 18:30:36,590 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [7/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.000 loss=17.000\n",
      "[2024-02-10 18:30:36,593 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [8/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.000 loss=7.000\n",
      "[2024-02-10 18:30:36,595 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [9/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.000 loss=12.000\n",
      "[2024-02-10 18:30:36,598 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [10/15] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.000 loss=5.000\n",
      "[2024-02-10 18:30:36,600 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-10 18:30:36,602 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [11/15] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.000 loss=6.000\n",
      "[2024-02-10 18:30:36,605 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [12/15] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.000 loss=11.000\n",
      "[2024-02-10 18:30:36,608 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [13/15] batch={'x': tensor([6, 5]), 'y': tensor([12, 10])} weight=0.000 loss=11.000\n",
      "[2024-02-10 18:30:36,610 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [14/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.000 loss=10.000\n",
      "[2024-02-10 18:30:36,613 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [15/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.000 loss=17.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"epoch_based_trainer\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-10 18:30:36,809 82437:140704363290240][log.py:55 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.7.0\n",
      "todd_version: 0.4.0\n",
      "cuda_home: None\n",
      "git_commit_id: 419d007\n",
      "git_status: \n",
      "MM todd/base/eta.py\n",
      "M  todd/base/stores.py\n",
      "M  todd/runners/base.py\n",
      "M  todd/runners/strategies/cuda.py\n",
      "M  todd/runners/trainer.py\n",
      "M  todd/runners/validator.py\n",
      "MM todd/utils/torch.py\n",
      " M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-02-10 18:30:36,812 82437:140704363290240][base.py:54 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:36,817 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-10 18:30:36,826 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-10 18:30:36,830 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-10 18:30:36,834 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:36,869 82437:140704363290240][base.py:54 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:36,874 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-10 18:30:36,877 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-10 18:30:36,887 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-10 18:30:36,892 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmprtb35w81\u001b[0m\n",
      "└── \u001b[1;36mlog_callback\u001b[0m\n",
      "    └── 2024-02-10T18-30-36_869304-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2024-02-10 18:30:36,869 82437:140704363290240][base.py:54 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2024-02-10 18:30:36,874 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-10 18:30:36,877 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-10 18:30:36,887 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-10 18:30:36,892 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='log_callback',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type='LogCallback',\n",
    "            interval=5,\n",
    "            with_file_handler=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/log_callback/*.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:37,589 82437:140704363290240][base.py:54 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:38,110 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:01 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-10 18:30:38,621 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:01 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-10 18:30:39,137 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-10 18:30:39,658 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:39,673 82437:140704363290240][base.py:54 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:41,189 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:04 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-10 18:30:45,208 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:05 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-10 18:30:50,224 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:03 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-10 18:30:55,239 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"EMA_ETA\", ema=dict(decay=0.2)),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1 * min(10, runner.iter_))\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-10 18:30:55,383 82437:140704363290240][log.py:55 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.7.0\n",
      "todd_version: 0.4.0\n",
      "cuda_home: None\n",
      "git_commit_id: 419d007\n",
      "git_status: \n",
      "MM todd/base/eta.py\n",
      "M  todd/base/stores.py\n",
      "M  todd/runners/base.py\n",
      "M  todd/runners/strategies/cuda.py\n",
      "M  todd/runners/trainer.py\n",
      "M  todd/runners/validator.py\n",
      "MM todd/utils/torch.py\n",
      " M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-02-10 18:30:55,385 82437:140704363290240][base.py:54 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:55,390 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:00 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-10 18:30:55,393 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:00 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-10 18:30:55,396 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-10 18:30:55,399 82437:140704363290240][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "            with_file_handler=True,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-10 18:30:55,474 82437:140704363290240][git.py:41 todd.Validator.git_callback init] INFO: Saving git diff to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpw64zoo15/git_callback/git_diff_2024-02-10T18-30-55_474606-08-00.log\n",
      "\u001b[2m[2024-02-10 18:30:55,477 82437:140704363290240][base.py:54 todd.Validator.git_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff --git a/todd/base/eta.py b/todd/base/eta.py\n",
      "index d4c53ea..e6bed85 100644\n",
      "--- a/todd/base/eta.py\n",
      "+++ b/todd/base/eta.py\n",
      "@@ -9,6 +9,8 @@ import datetime\n",
      " from abc import ABC, abstractmethod\n",
      " from typing import NamedTuple\n",
      " \n",
      "+from ..utils import ExponentialMovingAverage as EMA\n",
      "+from .configs import Config\n",
      " from .registries import ETARegistry\n",
      " \n",
      " \n",
      "@@ -28,19 +30,19 @@ class BaseETA(ABC):\n",
      "         return Datum(x, t)\n",
      " \n",
      "     @abstractmethod\n",
      "-    def _pace(self, datum: Datum) -> float:\n",
      "+    def pace(self, datum: Datum) -> float:\n",
      "         pass\n",
      " \n",
      "     def __call__(self, x: int) -> float:\n",
      "         datum = self._datum(x)\n",
      "-        pace = self._pace(datum)\n",
      "+        pace = self.pace(datum)\n",
      "         return pace * (self._end - x) / 1000\n",
      " \n",
      " \n",
      " @ETARegistry.register_()\n",
      " class AverageETA(BaseETA):\n",
      " \n",
      "-    def _pace(self, datum: Datum) -> float:\n",
      "+    def pace(self, datum: Datum) -> float:\n",
      "         t = datum.t - self._start.t\n",
      "         x = datum.x - self._start.x\n",
      "         return t.total_seconds() * 1000 / x\n",
      "@@ -49,14 +51,13 @@ class AverageETA(BaseETA):\n",
      " @ETARegistry.register_()\n",
      " class EMA_ETA(AverageETA):  # noqa: N801 pylint: disable=invalid-name\n",
      " \n",
      "-    def __init__(self, *args, decay: float, **kwargs) -> None:\n",
      "-        assert 0 <= decay <= 1\n",
      "+    def __init__(self, *args, ema: Config, **kwargs) -> None:\n",
      "         super().__init__(*args, **kwargs)\n",
      "-        self._decay = decay\n",
      "-        self._ema_pace = 0.\n",
      "+        self._ema = EMA(**ema)\n",
      "+        self._pace: float | None = None\n",
      " \n",
      "-    def _pace(self, datum: Datum) -> float:\n",
      "-        pace = super()._pace(datum)\n",
      "-        pace = self._decay * self._ema_pace + (1 - self._decay) * pace\n",
      "-        self._ema_pace = pace\n",
      "+    def pace(self, datum: Datum) -> float:\n",
      "+        pace = super().pace(datum)\n",
      "+        pace = self._ema(self._pace, pace)\n",
      "+        self._pace = pace\n",
      "         return pace\n",
      "diff --git a/todd/base/stores.py b/todd/base/stores.py\n",
      "index d8b5d06..98e3d89 100644\n",
      "--- a/todd/base/stores.py\n",
      "+++ b/todd/base/stores.py\n",
      "@@ -6,7 +6,6 @@ __all__ = [\n",
      " import os\n",
      " \n",
      " import torch\n",
      "-import torch.distributed\n",
      " from packaging.version import parse\n",
      " \n",
      " from ..utils import NonInstantiableMeta\n",
      "diff --git a/todd/runners/base.py b/todd/runners/base.py\n",
      "index 0ee1232..2efd19e 100644\n",
      "--- a/todd/runners/base.py\n",
      "+++ b/todd/runners/base.py\n",
      "@@ -11,7 +11,6 @@ import socket\n",
      " from typing import TYPE_CHECKING, Any, Mapping\n",
      " \n",
      " import torch\n",
      "-import torch.distributed\n",
      " import torch.utils.data\n",
      " \n",
      " from ..base import (\n",
      "diff --git a/todd/runners/strategies/cuda.py b/todd/runners/strategies/cuda.py\n",
      "index 8c62fe7..07114c7 100644\n",
      "--- a/todd/runners/strategies/cuda.py\n",
      "+++ b/todd/runners/strategies/cuda.py\n",
      "@@ -5,7 +5,7 @@ __all__ = [\n",
      " from typing import TypeVar\n",
      " \n",
      " import torch\n",
      "-import torch.distributed\n",
      "+import torch.distributed as dist\n",
      " from torch import nn\n",
      " \n",
      " from ...base import Config, Store, StrategyRegistry\n",
      "@@ -31,12 +31,12 @@ class CUDAStrategy(BaseStrategy[T]):\n",
      "         super().__init__(*args, **kwargs)\n",
      " \n",
      "     def _setup(self, config: Config) -> None:\n",
      "-        if not torch.distributed.is_initialized():\n",
      "+        if not dist.is_initialized():\n",
      "             init_process_group = config.get(\n",
      "                 'init_process_group',\n",
      "                 Config(backend='nccl'),\n",
      "             )\n",
      "-            torch.distributed.init_process_group(**init_process_group)\n",
      "+            dist.init_process_group(**init_process_group)\n",
      "         torch.cuda.set_device(get_local_rank() % torch.cuda.device_count())\n",
      " \n",
      "     def map_model(\n",
      "diff --git a/todd/runners/trainer.py b/todd/runners/trainer.py\n",
      "index 4c42765..5030f4c 100644\n",
      "--- a/todd/runners/trainer.py\n",
      "+++ b/todd/runners/trainer.py\n",
      "@@ -5,7 +5,6 @@ __all__ = [\n",
      " from typing import Any, Mapping\n",
      " \n",
      " import torch\n",
      "-import torch.distributed\n",
      " import torch.utils.data\n",
      " \n",
      " from ..base import Config, RunnerRegistry\n",
      "diff --git a/todd/runners/validator.py b/todd/runners/validator.py\n",
      "index 24ea0e9..820b568 100644\n",
      "--- a/todd/runners/validator.py\n",
      "+++ b/todd/runners/validator.py\n",
      "@@ -3,7 +3,6 @@ __all__ = [\n",
      " ]\n",
      " \n",
      " import torch\n",
      "-import torch.distributed\n",
      " import torch.utils.data\n",
      " \n",
      " from .base import BaseRunner, RunnerRegistry\n",
      "diff --git a/todd/utils/torch.py b/todd/utils/torch.py\n",
      "index 42eda22..05ec37b 100644\n",
      "--- a/todd/utils/torch.py\n",
      "+++ b/todd/utils/torch.py\n",
      "@@ -4,16 +4,18 @@ __all__ = [\n",
      "     'get_world_size',\n",
      "     'all_gather',\n",
      "     'all_gather_',\n",
      "+    'all_sync',\n",
      "     'Shape',\n",
      "     'ModuleList',\n",
      "     'ModuleDict',\n",
      "+    'ExponentialMovingAverage',\n",
      " ]\n",
      " \n",
      " import functools\n",
      " import itertools\n",
      " import operator\n",
      " import os\n",
      "-from typing import Any\n",
      "+from typing import TYPE_CHECKING\n",
      " \n",
      " import torch\n",
      " import torch.distributed as dist\n",
      "@@ -90,6 +92,15 @@ def all_gather_(\n",
      "     return tensors\n",
      " \n",
      " \n",
      "+def all_sync(x: torch.Tensor) -> bool:\n",
      "+    if get_world_size() <= 1:\n",
      "+        return True\n",
      "+    x_prime = x.clone()\n",
      "+    dist.all_reduce(x)\n",
      "+    x /= get_world_size()\n",
      "+    return torch.allclose(x, x_prime)\n",
      "+\n",
      "+\n",
      " class Shape:\n",
      " \n",
      "     @classmethod\n",
      "@@ -134,11 +145,50 @@ class Shape:\n",
      " \n",
      " class ModuleList(nn.ModuleList):\n",
      " \n",
      "-    def forward(self, *args, **kwargs) -> list:\n",
      "+    def forward(self, *args, **kwargs) -> list[nn.Module]:\n",
      "         return [m(*args, **kwargs) for m in self]\n",
      " \n",
      " \n",
      " class ModuleDict(nn.ModuleDict):\n",
      " \n",
      "-    def forward(self, *args, **kwargs) -> dict[str, Any]:\n",
      "+    def forward(self, *args, **kwargs) -> dict[str, nn.Module]:\n",
      "         return {k: m(*args, **kwargs) for k, m in self.items()}\n",
      "+\n",
      "+\n",
      "+class ExponentialMovingAverage(nn.Module):\n",
      "+\n",
      "+    def __init__(\n",
      "+        self,\n",
      "+        *args,\n",
      "+        decay=0.99,\n",
      "+        **kwargs,\n",
      "+    ) -> None:\n",
      "+        self.check_decay(decay)\n",
      "+        super().__init__(*args, **kwargs)\n",
      "+        self._decay = decay\n",
      "+\n",
      "+    @staticmethod\n",
      "+    def check_decay(decay) -> None:\n",
      "+        if isinstance(decay, torch.Tensor):\n",
      "+            assert decay.ge(0).all() and decay.le(1).all()\n",
      "+        else:\n",
      "+            assert 0 <= decay <= 1\n",
      "+\n",
      "+    @property\n",
      "+    def decay(self):\n",
      "+        return self._decay\n",
      "+\n",
      "+    def forward(self, x, y, decay=None):\n",
      "+        assert x is not None or y is not None\n",
      "+        if x is None:\n",
      "+            return y\n",
      "+        if y is None:\n",
      "+            return x\n",
      "+        if decay is None:\n",
      "+            decay = self._decay\n",
      "+        else:\n",
      "+            self.check_decay(decay)\n",
      "+        return x * decay + y * (1 - decay)\n",
      "+\n",
      "+    if TYPE_CHECKING:\n",
      "+        __call__ = forward\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"git_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"GitCallback\", diff='HEAD -- \":(exclude)*.ipynb\"'),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/git_callback/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:55,787 82437:140704363290240][base.py:54 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:55,791 82437:140704363290240][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.000 loss=18.000\n",
      "[2024-02-10 18:30:55,793 82437:140704363290240][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.045 loss=11.730\n",
      "[2024-02-10 18:30:55,795 82437:140704363290240][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.075 loss=12.513\n",
      "[2024-02-10 18:30:55,798 82437:140704363290240][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.107 loss=3.785\n",
      "[2024-02-10 18:30:55,799 82437:140704363290240][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.117 loss=7.530\n",
      "[2024-02-10 18:30:55,801 82437:140704363290240][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.137 loss=16.763\n",
      "[2024-02-10 18:30:55,803 82437:140704363290240][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.182 loss=10.905\n",
      "[2024-02-10 18:30:55,805 82437:140704363290240][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.212 loss=11.619\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"optimize_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:55,819 82437:140704363290240][base.py:54 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:55,822 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.000 loss=13.000 lr=['1.667e-03']\n",
      "[2024-02-10 18:30:55,824 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.011 loss=12.930 lr=['2.333e-03']\n",
      "[2024-02-10 18:30:55,826 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.026 loss=5.922 lr=['3.000e-03']\n",
      "[2024-02-10 18:30:55,828 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.035 loss=18.667 lr=['3.667e-03']\n",
      "[2024-02-10 18:30:55,830 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.070 loss=3.860 lr=['4.333e-03']\n",
      "[2024-02-10 18:30:55,833 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.078 loss=12.490 lr=['5.000e-03']\n",
      "[2024-02-10 18:30:55,835 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.111 loss=12.278 lr=['5.000e-03']\n",
      "[2024-02-10 18:30:55,837 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.144 loss=5.569 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=5),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:55,852 82437:140704363290240][base.py:54 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:55,853 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "[2024-02-10 18:30:55,856 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/10] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.000 loss=5.000 lr=['1.667e-03']\n",
      "[2024-02-10 18:30:55,858 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/10] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.004 loss=4.990 lr=['1.667e-03']\n",
      "[2024-02-10 18:30:55,859 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [2/5]\n",
      "[2024-02-10 18:30:55,861 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/10] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.008 loss=5.975 lr=['2.778e-03']\n",
      "[2024-02-10 18:30:55,864 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/10] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.017 loss=3.967 lr=['2.778e-03']\n",
      "[2024-02-10 18:30:55,865 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [3/5]\n",
      "[2024-02-10 18:30:55,867 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/10] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.022 loss=3.956 lr=['3.889e-03']\n",
      "[2024-02-10 18:30:55,869 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/10] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.030 loss=5.910 lr=['3.889e-03']\n",
      "[2024-02-10 18:30:55,870 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [4/5]\n",
      "[2024-02-10 18:30:55,872 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.042 loss=2.938 lr=['5.000e-03']\n",
      "[2024-02-10 18:30:55,875 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.049 loss=6.828 lr=['5.000e-03']\n",
      "[2024-02-10 18:30:55,876 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [5/5]\n",
      "[2024-02-10 18:30:55,878 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [9/10] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.067 loss=4.833 lr=['5.000e-03']\n",
      "[2024-02-10 18:30:55,880 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [10/10] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.079 loss=4.802 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=4),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=3),\n",
    "            by_epoch=True,\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=5,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-10 18:30:55,894 82437:140704363290240][lr.py:93 todd.IterBasedTrainer.lr_scale_callback _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-02-10 18:30:55,895 82437:140704363290240][base.py:54 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:55,898 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.000 loss=5.000\n",
      "[2024-02-10 18:30:55,900 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.025 loss=10.863\n",
      "[2024-02-10 18:30:55,902 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.080 loss=13.440\n",
      "[2024-02-10 18:30:55,904 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.150 loss=12.950\n",
      "[2024-02-10 18:30:55,907 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.220 loss=9.790\n",
      "[2024-02-10 18:30:55,909 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.275 loss=4.312\n",
      "[2024-02-10 18:30:55,912 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.300 loss=9.350\n",
      "[2024-02-10 18:30:55,915 82437:140704363290240][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.355 loss=11.515\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_scale_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScaleCallback\",\n",
    "            lr_scaler=dict(base_batch_size=1),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:55,939 82437:140704363290240][base.py:54 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:55,942 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.000 loss=9.000\n",
      "[2024-02-10 18:30:55,943 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_1\n",
      "[2024-02-10 18:30:55,950 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.022 loss=4.944\n",
      "[2024-02-10 18:30:55,951 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-02-10 18:30:55,956 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.035 loss=13.755\n",
      "[2024-02-10 18:30:55,957 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_3\n",
      "[2024-02-10 18:30:55,961 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.070 loss=14.475\n",
      "[2024-02-10 18:30:55,962 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-02-10 18:30:55,967 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.107 loss=11.355\n",
      "[2024-02-10 18:30:55,968 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-02-10 18:30:55,973 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.137 loss=8.381\n",
      "[2024-02-10 18:30:55,975 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-10 18:30:55,979 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.160 loss=4.600\n",
      "[2024-02-10 18:30:55,980 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-02-10 18:30:55,986 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.172 loss=12.792\n",
      "[2024-02-10 18:30:55,987 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_5\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_7\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_8\u001b[0m\n",
      "\n",
      "12 directories, 40 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-10 18:30:56,437 82437:140704363290240][checkpoint.py:54 todd.IterBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-02-10 18:30:56,440 82437:140704363290240][base.py:82 todd.IterBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-10 18:30:56,441 82437:140704363290240][base.py:54 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:56,444 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.137 loss=3.725\n",
      "[2024-02-10 18:30:56,445 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-10 18:30:56,450 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.147 loss=11.115\n",
      "[2024-02-10 18:30:56,451 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-02-10 18:30:56,455 82437:140704363290240][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.177 loss=11.846\n",
      "[2024-02-10 18:30:56,456 82437:140704363290240][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_5cambej/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {0: {'momentum_buffer': None}}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 5}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.1375))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_5 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_5'\n",
    "    for f in iter_5.glob('*.pth'):\n",
    "        print(f'{f.name}:')\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()\n",
    "\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_5),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:56,499 82437:140704363290240][base.py:54 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:56,501 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-10 18:30:56,503 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.000 loss=11.000\n",
      "[2024-02-10 18:30:56,505 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.027 loss=7.890\n",
      "[2024-02-10 18:30:56,506 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-02-10 18:30:56,510 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.047 loss=14.644\n",
      "[2024-02-10 18:30:56,513 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.085 loss=13.405\n",
      "[2024-02-10 18:30:56,514 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-02-10 18:30:56,518 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.120 loss=6.580\n",
      "[2024-02-10 18:30:56,518 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-10 18:30:56,521 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.137 loss=16.763\n",
      "[2024-02-10 18:30:56,522 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-10 18:30:56,526 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.182 loss=9.996\n",
      "[2024-02-10 18:30:56,529 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([6, 5]), 'y': tensor([12, 10])} weight=0.210 loss=9.845\n",
      "[2024-02-10 18:30:56,530 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-02-10 18:30:56,535 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.237 loss=6.169\n",
      "[2024-02-10 18:30:56,537 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.255 loss=6.980\n",
      "[2024-02-10 18:30:56,539 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-10 18:30:56,542 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-10 18:30:56,545 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.275 loss=9.488\n",
      "[2024-02-10 18:30:56,547 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.303 loss=7.639\n",
      "[2024-02-10 18:30:56,548 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-10 18:30:56,552 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.325 loss=8.375\n",
      "[2024-02-10 18:30:56,554 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.350 loss=12.375\n",
      "[2024-02-10 18:30:56,555 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-10 18:30:56,559 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([6, 4]), 'y': tensor([12,  8])} weight=0.388 loss=8.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_12\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_14\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_14\u001b[0m\n",
      "\n",
      "11 directories, 35 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-10 18:30:56,993 82437:140704363290240][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-02-10 18:30:56,997 82437:140704363290240][base.py:82 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-10 18:30:56,997 82437:140704363290240][base.py:54 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:56,999 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-10 18:30:57,002 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.237 loss=8.812\n",
      "[2024-02-10 18:30:57,004 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.262 loss=11.294\n",
      "[2024-02-10 18:30:57,005 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-10 18:30:57,009 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-10 18:30:57,011 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.295 loss=11.083\n",
      "[2024-02-10 18:30:57,013 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.327 loss=4.181\n",
      "[2024-02-10 18:30:57,014 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-10 18:30:57,018 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.340 loss=15.770\n",
      "[2024-02-10 18:30:57,020 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.387 loss=10.481\n",
      "[2024-02-10 18:30:57,021 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-10 18:30:57,025 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.420 loss=3.950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-10 18:30:57,446 82437:140704363290240][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-10 18:30:57,450 82437:140704363290240][base.py:82 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-10 18:30:57,451 82437:140704363290240][base.py:54 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:57,452 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-10 18:30:57,456 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.295 loss=10.230\n",
      "[2024-02-10 18:30:57,458 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.325 loss=10.050\n",
      "[2024-02-10 18:30:57,458 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-10 18:30:57,463 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.355 loss=6.580\n",
      "[2024-02-10 18:30:57,465 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.375 loss=12.188\n",
      "[2024-02-10 18:30:57,466 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmphnnfpp67/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-10 18:30:57,471 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([3, 5]), 'y': tensor([ 6, 10])} weight=0.412 loss=6.350\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=2),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_8 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_8'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_8),\n",
    "        )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    iter_10 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_10'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_10),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:57,501 82437:140704363290240][base.py:54 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:57,502 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-10 18:30:57,504 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.000 loss=11.000\n",
      "[2024-02-10 18:30:57,507 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.027 loss=16.766\n",
      "[2024-02-10 18:30:57,509 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.070 loss=3.860\n",
      "[2024-02-10 18:30:57,511 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.080 loss=5.760\n",
      "[2024-02-10 18:30:57,512 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.095 loss=16.193\n",
      "[2024-02-10 18:30:57,514 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpophxxzac/checkpoint_callback/checkpoints/epoch_1\n",
      "[2024-02-10 18:30:57,517 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-10 18:30:57,519 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.138 loss=5.587\n",
      "[2024-02-10 18:30:57,521 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.153 loss=13.856\n",
      "[2024-02-10 18:30:57,523 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.190 loss=15.385\n",
      "[2024-02-10 18:30:57,525 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.233 loss=6.186\n",
      "[2024-02-10 18:30:57,527 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.250 loss=8.750\n",
      "[2024-02-10 18:30:57,528 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpophxxzac/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-02-10 18:30:57,531 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-10 18:30:57,533 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.275 loss=8.625\n",
      "[2024-02-10 18:30:57,535 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.300 loss=12.750\n",
      "[2024-02-10 18:30:57,537 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.338 loss=6.650\n",
      "[2024-02-10 18:30:57,540 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.358 loss=7.391\n",
      "[2024-02-10 18:30:57,541 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.380 loss=10.530\n",
      "[2024-02-10 18:30:57,542 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpophxxzac/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpophxxzac\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36mepoch_3\u001b[0m\n",
      "\n",
      "7 directories, 15 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-10 18:30:58,007 82437:140704363290240][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpophxxzac/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-02-10 18:30:58,011 82437:140704363290240][base.py:82 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-10 18:30:58,012 82437:140704363290240][base.py:54 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:58,013 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-10 18:30:58,017 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.275 loss=7.763\n",
      "[2024-02-10 18:30:58,019 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.298 loss=5.959\n",
      "[2024-02-10 18:30:58,021 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.315 loss=9.267\n",
      "[2024-02-10 18:30:58,023 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.343 loss=13.260\n",
      "[2024-02-10 18:30:58,025 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.383 loss=9.705\n",
      "[2024-02-10 18:30:58,027 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpophxxzac/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'epoch_2'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(epoch_2),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register_()\n",
    "class FaultyValidator(todd.runners.Validator):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError(\"faulty runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:58,070 82437:140704363290240][base.py:54 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-02-10 18:30:58,074 82437:140704363290240][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x111620910>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 222, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_82437/1715875531.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2024-02-10 18:30:58,070 82437:140704363290240][base.py:54 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2024-02-10 18:30:58,074 82437:140704363290240][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x111620910>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 222, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_82437/1715875531.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='FaultyValidator',\n",
    "    name='monitor_callback',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[\n",
    "        dict(type='MonitorCallback'),\n",
    "        dict(type='LogCallback', interval=5, with_file_handler=True),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/monitor_callback/*.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:58,396 82437:140704363290240][base.py:54 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:58,398 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-10 18:30:58,400 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.000 loss=15.000\n",
      "[2024-02-10 18:30:58,402 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.037 loss=11.775\n",
      "[2024-02-10 18:30:58,405 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.067 loss=9.663\n",
      "[2024-02-10 18:30:58,407 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.092 loss=8.584\n",
      "[2024-02-10 18:30:58,409 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.115 loss=8.483\n",
      "[2024-02-10 18:30:58,410 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_kof8sjl/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-02-10 18:30:58,414 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-10 18:30:58,416 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.137 loss=6.519\n",
      "[2024-02-10 18:30:58,418 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.155 loss=9.225\n",
      "[2024-02-10 18:30:58,420 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.180 loss=9.100\n",
      "[2024-02-10 18:30:58,422 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.205 loss=10.770\n",
      "[2024-02-10 18:30:58,424 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.235 loss=14.120\n",
      "[2024-02-10 18:30:58,426 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_kof8sjl/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-02-10 18:30:58,429 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-10 18:30:58,431 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.275 loss=16.388\n",
      "[2024-02-10 18:30:58,434 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.322 loss=8.388\n",
      "[2024-02-10 18:30:58,436 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([8, 1]), 'y': tensor([16,  2])} weight=0.347 loss=7.436\n",
      "[2024-02-10 18:30:58,438 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([7, 3]), 'y': tensor([14,  6])} weight=0.370 loss=8.150\n",
      "[2024-02-10 18:30:58,440 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 5]), 'y': tensor([ 4, 10])} weight=0.395 loss=5.617\n",
      "[2024-02-10 18:30:58,442 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_kof8sjl/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-10 18:30:58,873 82437:140704363290240][base.py:54 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-10 18:30:58,875 82437:140704363290240][base.py:97 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_kof8sjl/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-02-10 18:30:58,877 82437:140704363290240][base.py:82 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "[2024-02-10 18:30:58,878 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-10 18:30:58,882 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.275 loss=9.488\n",
      "[2024-02-10 18:30:58,884 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.302 loss=11.882\n",
      "[2024-02-10 18:30:58,886 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.337 loss=5.819\n",
      "[2024-02-10 18:30:58,888 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([8, 7]), 'y': tensor([16, 14])} weight=0.355 loss=12.337\n",
      "[2024-02-10 18:30:58,890 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.392 loss=6.430\n",
      "[2024-02-10 18:30:58,891 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_kof8sjl/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-02-10 18:30:58,894 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-10 18:30:58,896 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.412 loss=11.112\n",
      "[2024-02-10 18:30:58,898 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.447 loss=7.762\n",
      "[2024-02-10 18:30:58,900 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([10,  3]), 'y': tensor([20,  6])} weight=0.472 loss=9.929\n",
      "[2024-02-10 18:30:58,902 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.505 loss=4.485\n",
      "[2024-02-10 18:30:58,904 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.520 loss=8.880\n",
      "[2024-02-10 18:30:58,905 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_kof8sjl/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-02-10 18:30:58,909 82437:140704363290240][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-10 18:30:58,911 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([8, 6]), 'y': tensor([16, 12])} weight=0.550 loss=10.150\n",
      "[2024-02-10 18:30:58,913 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 7]), 'y': tensor([18, 14])} weight=0.585 loss=11.320\n",
      "[2024-02-10 18:30:58,914 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.625 loss=3.438\n",
      "[2024-02-10 18:30:58,916 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([10,  5]), 'y': tensor([20, 10])} weight=0.637 loss=10.219\n",
      "[2024-02-10 18:30:58,918 82437:140704363290240][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.675 loss=3.312\n",
      "[2024-02-10 18:30:58,919 82437:140704363290240][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_kof8sjl/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"strategy_load_model_from\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = (pathlib.Path(work_dirs) / 'strategy_load_model_from' / 'checkpoints' / 'epoch_2' / 'model.pth')\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.strategy.load_model_from(epoch_2)\n",
    "    runner.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
