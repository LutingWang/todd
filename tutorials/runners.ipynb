{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.3.0\n",
      "Uninstalling todd-ai-0.3.0:\n",
      "  Successfully uninstalled todd-ai-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing /Users/bytedance/Developer/todd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (0.6.1)\n",
      "Requirement already satisfied: lmdb in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (1.4.1)\n",
      "Requirement already satisfied: opencv-python in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (4.7.0.72)\n",
      "Requirement already satisfied: pandas in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (2.0.1)\n",
      "Requirement already satisfied: python-pptx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (0.6.21)\n",
      "Requirement already satisfied: timm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (0.9.2)\n",
      "Requirement already satisfied: toml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (4.5.0)\n",
      "Requirement already satisfied: yapf in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from opencv-python->todd-ai==0.3.0) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.3.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.3.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.3.0) (2023.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.3.0) (4.9.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.3.0) (9.5.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.3.0) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.3.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.3.0) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.3.0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.3.0) (0.14.1)\n",
      "Requirement already satisfied: safetensors in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.3.0) (0.3.1)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from yapf->todd-ai==0.3.0) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->todd-ai==0.3.0) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.3.0) (3.12.0)\n",
      "Requirement already satisfied: sympy in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.3.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.3.0) (2023.5.0)\n",
      "Requirement already satisfied: requests in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.3.0) (2.30.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.3.0) (23.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from jinja2->torch>=1.7->timm->todd-ai==0.3.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.3.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.3.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.3.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.3.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from sympy->torch>=1.7->timm->todd-ai==0.3.0) (1.3.0)\n",
      "Building wheels for collected packages: todd-ai\n",
      "  Building wheel for todd-ai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for todd-ai: filename=todd_ai-0.3.0-py3-none-any.whl size=83430 sha256=42c79faa6bd05058926fbf0fbabd51b91a374adce4aeef76642ad0fa1b2a9f20\n",
      "  Stored in directory: /private/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/pip-ephem-wheel-cache-535qpn9m/wheels/15/ef/5a/9fc12e257ce5cef16b333a2ed6c992ff9cbcc9167f7199e6ac\n",
      "Successfully built todd-ai\n",
      "Installing collected packages: todd-ai\n",
      "Successfully installed todd-ai-0.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-05 20:41:35,709 89557:140704704583232][patches.py:82 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n",
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import Any, TypedDict, cast\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "import todd\n",
    "\n",
    "Memo = dict[str, Any]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models should be built by users.\n",
    "The same model can be used by multiple runners, such as a trainer and a validator, simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(todd.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to models, datasets are built inside runners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x=self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _build_dataloader(\n",
    "        self,\n",
    "        config: todd.Config,\n",
    "    ) -> torch.utils.data.DataLoader:\n",
    "        dataset = Dataset(**config.pop('dataset'))\n",
    "        return torch.utils.data.DataLoader(dataset, **config)\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> None:\n",
    "        y: torch.Tensor = self._model(batch['x'])\n",
    "        loss = F.l1_loss(y, batch['y'])\n",
    "        memo['loss'] = loss\n",
    "        if 'log' in memo:\n",
    "            memo['log']['loss'] = f'{loss.item():.3f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerMixin(RunnerMixin):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> None:\n",
    "        super()._run_iter(batch, memo)\n",
    "        if 'log' in memo:\n",
    "            model = cast(Model, self._model)\n",
    "            memo['log']['weight'] = f'{model.weight.item():.3f}'\n",
    "            memo['log']['batch'] = str(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomValidator(RunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomIterBasedTrainer(TrainerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomEpochBasedTrainer(TrainerMixin, todd.runners.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and register the validator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the validator config. \n",
    "`config` will be reused by trainers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_config = todd.Config(\n",
    "    type='CustomValidator',\n",
    "    name='custom_validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(n=20)),\n",
    "    callbacks=dict(type='LogCallback', interval=5),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and run the validator.\n",
    "Logs will be saved to the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 20:41:37,312 89557:140704704583232][runners.py:59 todd.CustomValidator.custom_validator __init__] DEBUG: Runner custom_validator initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 20:41:37,315 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] loss=10.000\n",
      "[2023-07-05 20:41:37,316 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] loss=20.000\n",
      "[2023-07-05 20:41:37,318 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] loss=30.000\n",
      "[2023-07-05 20:41:37,320 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpg6ja8dcx\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_config.callbacks.with_file_handler = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 20:41:37,622 89557:140704704583232][runners.py:59 todd.CustomValidator.custom_validator __init__] DEBUG: Runner custom_validator initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 20:41:37,627 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] loss=10.000\n",
      "[2023-07-05 20:41:37,631 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] loss=20.000\n",
      "[2023-07-05 20:41:37,635 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] loss=30.000\n",
      "[2023-07-05 20:41:37,637 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfb6iuce5\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "    └── 2023-07-05T20-41-37_626231-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2023-07-05 20:41:37,627 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] loss=10.000\n",
      "[2023-07-05 20:41:37,631 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] loss=20.000\n",
      "[2023-07-05 20:41:37,635 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] loss=30.000\n",
      "[2023-07-05 20:41:37,637 89557:140704704583232][log.py:42 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] loss=40.000\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs_root}\n",
    "    !echo\n",
    "    !cat $(find {work_dirs_root}/custom_validator -name '*.log' -print -quit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = validator_config.copy()\n",
    "trainer_config.pop('type')\n",
    "trainer_config.dataloader = todd.Config(batch_size=2, shuffle=True, dataset=dict(n=67))\n",
    "trainer_config.optimizer = todd.Config(type='SGD', lr=0.005) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_based_trainer_config = trainer_config.copy()\n",
    "iter_based_trainer_config.type = 'CustomIterBasedTrainer'\n",
    "iter_based_trainer_config.name = 'custom_iter_based_trainer'\n",
    "iter_based_trainer_config.iters = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 20:41:38,258 89557:140704704583232][runners.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Runner custom_iter_based_trainer initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 20:41:38,263 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=67.000 weight=0.000 batch={'x': tensor([33, 34]), 'y': tensor([66, 68])}\n",
      "[2023-07-05 20:41:38,265 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=85.000 weight=0.000 batch={'x': tensor([19, 66]), 'y': tensor([ 38, 132])}\n",
      "[2023-07-05 20:41:38,267 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=26.000 weight=0.000 batch={'x': tensor([ 2, 24]), 'y': tensor([ 4, 48])}\n",
      "[2023-07-05 20:41:38,269 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=41.000 weight=0.000 batch={'x': tensor([31, 10]), 'y': tensor([62, 20])}\n",
      "[2023-07-05 20:41:38,272 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=101.000 weight=0.000 batch={'x': tensor([48, 53]), 'y': tensor([ 96, 106])}\n",
      "[2023-07-05 20:41:38,274 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=56.000 weight=0.000 batch={'x': tensor([ 6, 50]), 'y': tensor([ 12, 100])}\n",
      "[2023-07-05 20:41:38,276 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=124.000 weight=0.000 batch={'x': tensor([61, 63]), 'y': tensor([122, 126])}\n",
      "[2023-07-05 20:41:38,277 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=97.000 weight=0.000 batch={'x': tensor([51, 46]), 'y': tensor([102,  92])}\n",
      "[2023-07-05 20:41:38,279 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=72.000 weight=0.000 batch={'x': tensor([54, 18]), 'y': tensor([108,  36])}\n",
      "[2023-07-05 20:41:38,280 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=50.000 weight=0.000 batch={'x': tensor([30, 20]), 'y': tensor([60, 40])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        iter_based_trainer_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainers increment `todd.Store.ITER` to keep track of the training progress.\n",
    "If multiple trainers are to be run, `todd.Store.ITER` must be manually reset to zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_based_trainer_config = trainer_config.copy()\n",
    "epoch_based_trainer_config.type = 'CustomEpochBasedTrainer'\n",
    "epoch_based_trainer_config.name = 'custom_epoch_based_trainer'\n",
    "epoch_based_trainer_config.epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 20:41:38,296 89557:140704704583232][runners.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Runner custom_epoch_based_trainer initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 20:41:38,297 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-05 20:41:38,299 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=101.000 weight=0.000 batch={'x': tensor([62, 39]), 'y': tensor([124,  78])}\n",
      "[2023-07-05 20:41:38,301 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=84.000 weight=0.000 batch={'x': tensor([58, 26]), 'y': tensor([116,  52])}\n",
      "[2023-07-05 20:41:38,303 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=83.000 weight=0.000 batch={'x': tensor([67, 16]), 'y': tensor([134,  32])}\n",
      "[2023-07-05 20:41:38,306 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=105.000 weight=0.000 batch={'x': tensor([40, 65]), 'y': tensor([ 80, 130])}\n",
      "[2023-07-05 20:41:38,308 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=43.000 weight=0.000 batch={'x': tensor([37,  6]), 'y': tensor([74, 12])}\n",
      "[2023-07-05 20:41:38,310 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=109.000 weight=0.000 batch={'x': tensor([53, 56]), 'y': tensor([106, 112])}\n",
      "[2023-07-05 20:41:38,312 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-05 20:41:38,313 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=72.000 weight=0.000 batch={'x': tensor([ 5, 67]), 'y': tensor([ 10, 134])}\n",
      "[2023-07-05 20:41:38,315 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=45.000 weight=0.000 batch={'x': tensor([ 8, 37]), 'y': tensor([16, 74])}\n",
      "[2023-07-05 20:41:38,316 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=58.000 weight=0.000 batch={'x': tensor([26, 32]), 'y': tensor([52, 64])}\n",
      "[2023-07-05 20:41:38,318 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=49.000 weight=0.000 batch={'x': tensor([29, 20]), 'y': tensor([58, 40])}\n",
      "[2023-07-05 20:41:38,320 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=54.000 weight=0.000 batch={'x': tensor([18, 36]), 'y': tensor([36, 72])}\n",
      "[2023-07-05 20:41:38,323 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=108.000 weight=0.000 batch={'x': tensor([49, 59]), 'y': tensor([ 98, 118])}\n",
      "[2023-07-05 20:41:38,325 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=63.000 weight=0.000 batch={'x': tensor([24, 39]), 'y': tensor([48, 78])}\n",
      "[2023-07-05 20:41:38,326 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-05 20:41:38,328 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=91.000 weight=0.000 batch={'x': tensor([50, 41]), 'y': tensor([100,  82])}\n",
      "[2023-07-05 20:41:38,330 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=83.000 weight=0.000 batch={'x': tensor([18, 65]), 'y': tensor([ 36, 130])}\n",
      "[2023-07-05 20:41:38,331 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=76.000 weight=0.000 batch={'x': tensor([54, 22]), 'y': tensor([108,  44])}\n",
      "[2023-07-05 20:41:38,333 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=92.000 weight=0.000 batch={'x': tensor([47, 45]), 'y': tensor([94, 90])}\n",
      "[2023-07-05 20:41:38,335 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=71.000 weight=0.000 batch={'x': tensor([19, 52]), 'y': tensor([ 38, 104])}\n",
      "[2023-07-05 20:41:38,337 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=28.000 weight=0.000 batch={'x': tensor([11, 17]), 'y': tensor([22, 34])}\n",
      "[2023-07-05 20:41:38,339 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=52.000 weight=0.000 batch={'x': tensor([13, 39]), 'y': tensor([26, 78])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        epoch_based_trainer_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_callback_config = iter_based_trainer_config.copy()\n",
    "log_callback = optimize_callback_config.callbacks\n",
    "optimize_callback = todd.Config(type='OptimizeCallback')\n",
    "composed_callback = todd.Config(type='ComposedCallback', callbacks=[optimize_callback, log_callback])\n",
    "optimize_callback_config.callbacks = composed_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 20:41:38,356 89557:140704704583232][runners.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Runner custom_iter_based_trainer initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 20:41:38,360 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=5.140 weight=0.715 batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])}\n",
      "[2023-07-05 20:41:38,363 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=35.213 weight=1.217 batch={'x': tensor([30, 60]), 'y': tensor([ 60, 120])}\n",
      "[2023-07-05 20:41:38,366 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=4.356 weight=2.102 batch={'x': tensor([44, 41]), 'y': tensor([88, 82])}\n",
      "[2023-07-05 20:41:38,369 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=0.930 weight=1.980 batch={'x': tensor([57, 36]), 'y': tensor([114,  72])}\n",
      "[2023-07-05 20:41:38,372 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=2.161 weight=2.047 batch={'x': tensor([52, 39]), 'y': tensor([104,  78])}\n",
      "[2023-07-05 20:41:38,375 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=0.713 weight=2.038 batch={'x': tensor([29,  9]), 'y': tensor([58, 18])}\n",
      "[2023-07-05 20:41:38,378 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=3.108 weight=1.945 batch={'x': tensor([50, 63]), 'y': tensor([100, 126])}\n",
      "[2023-07-05 20:41:38,380 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=1.320 weight=2.030 batch={'x': tensor([35, 53]), 'y': tensor([ 70, 106])}\n",
      "[2023-07-05 20:41:38,382 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=9.240 weight=1.808 batch={'x': tensor([34, 62]), 'y': tensor([ 68, 124])}\n",
      "[2023-07-05 20:41:38,385 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=8.700 weight=2.145 batch={'x': tensor([54, 66]), 'y': tensor([108, 132])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        optimize_callback_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_callback = todd.Config(type='LrScheduleCallback')\n",
    "lr_schedule_callback_config = optimize_callback_config.copy()\n",
    "lr_schedule_callback_config.lr_scheduler = todd.Config(type='LinearLR', total_iters=10)\n",
    "lr_schedule_callback_config.callbacks.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 20:41:38,402 89557:140704704583232][runners.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Runner custom_iter_based_trainer initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 20:41:38,406 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=53.416 weight=0.405 batch={'x': tensor([32, 35]), 'y': tensor([64, 70])} lr=['3.333e-03']\n",
      "[2023-07-05 20:41:38,410 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=41.335 weight=0.992 batch={'x': tensor([25, 57]), 'y': tensor([ 50, 114])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,413 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=3.472 weight=1.893 batch={'x': tensor([ 7, 58]), 'y': tensor([ 14, 116])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,416 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=0.764 weight=2.023 batch={'x': tensor([55, 11]), 'y': tensor([110,  22])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,419 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=1.365 weight=2.031 batch={'x': tensor([36, 53]), 'y': tensor([ 72, 106])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,423 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=10.244 weight=1.753 batch={'x': tensor([24, 59]), 'y': tensor([ 48, 118])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,426 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=3.295 weight=1.876 batch={'x': tensor([ 2, 51]), 'y': tensor([  4, 102])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,429 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=4.004 weight=2.061 batch={'x': tensor([67, 65]), 'y': tensor([134, 130])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,431 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=4.180 weight=1.861 batch={'x': tensor([22, 38]), 'y': tensor([44, 76])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,433 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=0.939 weight=1.963 batch={'x': tensor([39, 12]), 'y': tensor([78, 24])} lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_callback_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_by_epoch_callback = lr_schedule_callback.copy()\n",
    "lr_schedule_by_epoch_callback_config = epoch_based_trainer_config.copy()\n",
    "lr_schedule_by_epoch_callback_config.lr_scheduler = lr_schedule_callback_config.lr_scheduler\n",
    "lr_schedule_by_epoch_callback_config.callbacks = dict(\n",
    "    type='ComposedCallback',\n",
    "    callbacks=[optimize_callback, lr_schedule_by_epoch_callback, log_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 20:41:38,450 89557:140704704583232][runners.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Runner custom_epoch_based_trainer initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 20:41:38,452 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-05 20:41:38,456 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=84.326 weight=0.167 batch={'x': tensor([35, 57]), 'y': tensor([ 70, 114])} lr=['3.333e-03']\n",
      "[2023-07-05 20:41:38,459 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=34.273 weight=0.818 batch={'x': tensor([43, 15]), 'y': tensor([86, 30])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,462 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=14.904 weight=1.586 batch={'x': tensor([21, 51]), 'y': tensor([ 42, 102])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,465 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=2.553 weight=2.111 batch={'x': tensor([24, 22]), 'y': tensor([48, 44])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,468 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=3.749 weight=1.919 batch={'x': tensor([52, 40]), 'y': tensor([104,  80])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,471 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=5.925 weight=2.118 batch={'x': tensor([59, 41]), 'y': tensor([118,  82])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,474 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-05 20:41:38,476 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=0.147 weight=2.004 batch={'x': tensor([33, 51]), 'y': tensor([ 66, 102])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,479 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=2.360 weight=1.941 batch={'x': tensor([35, 45]), 'y': tensor([70, 90])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,482 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=5.609 weight=2.139 batch={'x': tensor([57, 24]), 'y': tensor([114,  48])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,485 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=1.917 weight=1.946 batch={'x': tensor([10, 61]), 'y': tensor([ 20, 122])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,488 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=3.110 weight=1.929 batch={'x': tensor([37, 50]), 'y': tensor([ 74, 100])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,491 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=8.892 weight=1.766 batch={'x': tensor([12, 64]), 'y': tensor([ 24, 128])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,494 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=5.936 weight=2.106 batch={'x': tensor([46, 66]), 'y': tensor([ 92, 132])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,496 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-05 20:41:38,498 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=2.132 weight=2.044 batch={'x': tensor([57, 41]), 'y': tensor([114,  82])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,501 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=4.986 weight=2.139 batch={'x': tensor([47, 25]), 'y': tensor([94, 50])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,504 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=0.152 weight=1.996 batch={'x': tensor([54, 22]), 'y': tensor([108,  44])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,507 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=8.241 weight=2.201 batch={'x': tensor([21, 61]), 'y': tensor([ 42, 122])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,510 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=2.618 weight=2.094 batch={'x': tensor([ 3, 53]), 'y': tensor([  6, 106])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,513 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=5.556 weight=2.271 batch={'x': tensor([14, 27]), 'y': tensor([28, 54])} lr=['5.000e-03']\n",
      "[2023-07-05 20:41:38,516 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=1.145 weight=1.971 batch={'x': tensor([63, 16]), 'y': tensor([126,  32])} lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_by_epoch_callback_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_by_epoch_callback_config.callbacks.callbacks[1].by_epoch = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 20:41:38,534 89557:140704704583232][runners.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Runner custom_epoch_based_trainer initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 20:41:38,536 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-05 20:41:38,540 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=67.093 weight=0.187 batch={'x': tensor([12, 62]), 'y': tensor([ 24, 124])} lr=['1.667e-03']\n",
      "[2023-07-05 20:41:38,543 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=55.777 weight=0.493 batch={'x': tensor([59, 15]), 'y': tensor([118,  30])} lr=['1.667e-03']\n",
      "[2023-07-05 20:41:38,546 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=45.321 weight=0.758 batch={'x': tensor([27, 46]), 'y': tensor([54, 92])} lr=['1.667e-03']\n",
      "[2023-07-05 20:41:38,550 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=53.960 weight=1.053 batch={'x': tensor([58, 56]), 'y': tensor([116, 112])} lr=['1.667e-03']\n",
      "[2023-07-05 20:41:38,553 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=30.280 weight=1.369 batch={'x': tensor([67, 29]), 'y': tensor([134,  58])} lr=['1.667e-03']\n",
      "[2023-07-05 20:41:38,556 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=10.676 weight=1.707 batch={'x': tensor([34, 39]), 'y': tensor([68, 78])} lr=['1.667e-03']\n",
      "[2023-07-05 20:41:38,559 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-05 20:41:38,561 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=4.492 weight=1.918 batch={'x': tensor([63, 47]), 'y': tensor([126,  94])} lr=['2.000e-03']\n",
      "[2023-07-05 20:41:38,564 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=0.864 weight=2.024 batch={'x': tensor([55, 16]), 'y': tensor([110,  32])} lr=['2.000e-03']\n",
      "[2023-07-05 20:41:38,567 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=0.339 weight=2.012 batch={'x': tensor([26, 29]), 'y': tensor([52, 58])} lr=['2.000e-03']\n",
      "[2023-07-05 20:41:38,570 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=0.401 weight=2.009 batch={'x': tensor([28, 58]), 'y': tensor([ 56, 116])} lr=['2.000e-03']\n",
      "[2023-07-05 20:41:38,573 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=4.583 weight=1.914 batch={'x': tensor([41, 66]), 'y': tensor([ 82, 132])} lr=['2.000e-03']\n",
      "[2023-07-05 20:41:38,576 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=2.146 weight=2.034 batch={'x': tensor([64, 61]), 'y': tensor([128, 122])} lr=['2.000e-03']\n",
      "[2023-07-05 20:41:38,579 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=3.120 weight=1.936 batch={'x': tensor([59, 39]), 'y': tensor([118,  78])} lr=['2.000e-03']\n",
      "[2023-07-05 20:41:38,581 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-05 20:41:38,583 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=0.184 weight=1.996 batch={'x': tensor([56, 49]), 'y': tensor([112,  98])} lr=['2.333e-03']\n",
      "[2023-07-05 20:41:38,586 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=1.748 weight=2.033 batch={'x': tensor([57, 50]), 'y': tensor([114, 100])} lr=['2.333e-03']\n",
      "[2023-07-05 20:41:38,589 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=1.785 weight=1.960 batch={'x': tensor([47, 43]), 'y': tensor([94, 86])} lr=['2.333e-03']\n",
      "[2023-07-05 20:41:38,593 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=0.225 weight=2.006 batch={'x': tensor([15, 62]), 'y': tensor([ 30, 124])} lr=['2.333e-03']\n",
      "[2023-07-05 20:41:38,595 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=0.882 weight=2.042 batch={'x': tensor([14, 28]), 'y': tensor([28, 56])} lr=['2.333e-03']\n",
      "[2023-07-05 20:41:38,598 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=0.672 weight=2.021 batch={'x': tensor([30, 34]), 'y': tensor([60, 68])} lr=['2.333e-03']\n",
      "[2023-07-05 20:41:38,601 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=1.913 weight=1.953 batch={'x': tensor([58, 24]), 'y': tensor([116,  48])} lr=['2.333e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_by_epoch_callback_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = todd.Config(type='CheckpointCallback', every_n_iters=10)\n",
    "checkpoint_callback_config = iter_based_trainer_config.copy()\n",
    "checkpoint_callback_config.callbacks = dict(\n",
    "    type='ComposedCallback',\n",
    "    callbacks=[checkpoint_callback, log_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 20:41:38,624 89557:140704704583232][runners.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Runner custom_iter_based_trainer initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 20:41:38,627 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=85.000 weight=0.000 batch={'x': tensor([19, 66]), 'y': tensor([ 38, 132])}\n",
      "[2023-07-05 20:41:38,629 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/iter_10.pth\n",
      "[2023-07-05 20:41:38,631 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=112.000 weight=0.000 batch={'x': tensor([67, 45]), 'y': tensor([134,  90])}\n",
      "[2023-07-05 20:41:38,633 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=108.000 weight=0.000 batch={'x': tensor([65, 43]), 'y': tensor([130,  86])}\n",
      "[2023-07-05 20:41:38,635 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/iter_20.pth\n",
      "[2023-07-05 20:41:38,637 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=91.000 weight=0.000 batch={'x': tensor([40, 51]), 'y': tensor([ 80, 102])}\n",
      "[2023-07-05 20:41:38,639 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=123.000 weight=0.000 batch={'x': tensor([60, 63]), 'y': tensor([120, 126])}\n",
      "[2023-07-05 20:41:38,641 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/iter_30.pth\n",
      "[2023-07-05 20:41:38,642 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=39.000 weight=0.000 batch={'x': tensor([ 1, 38]), 'y': tensor([ 2, 76])}\n",
      "[2023-07-05 20:41:38,644 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=47.000 weight=0.000 batch={'x': tensor([14, 33]), 'y': tensor([28, 66])}\n",
      "[2023-07-05 20:41:38,646 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/iter_40.pth\n",
      "[2023-07-05 20:41:38,647 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=40.000 weight=0.000 batch={'x': tensor([25, 15]), 'y': tensor([50, 30])}\n",
      "[2023-07-05 20:41:38,649 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=86.000 weight=0.000 batch={'x': tensor([64, 22]), 'y': tensor([128,  44])}\n",
      "[2023-07-05 20:41:38,651 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/iter_50.pth\n",
      "[2023-07-05 20:41:38,652 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=41.000 weight=0.000 batch={'x': tensor([28, 13]), 'y': tensor([56, 26])}\n",
      "[2023-07-05 20:41:38,654 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/latest.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf\u001b[0m\n",
      "└── \u001b[1;36mcustom_iter_based_trainer\u001b[0m\n",
      "    ├── 2023-07-05T20-41-38_626008-08-00.log\n",
      "    ├── iter_10.pth\n",
      "    ├── iter_20.pth\n",
      "    ├── iter_30.pth\n",
      "    ├── iter_40.pth\n",
      "    ├── iter_50.pth\n",
      "    └── latest.pth\n",
      "\n",
      "2 directories, 7 files\n",
      "\n",
      "[2023-07-05 20:41:38,627 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=85.000 weight=0.000 batch={'x': tensor([19, 66]), 'y': tensor([ 38, 132])}\n",
      "[2023-07-05 20:41:38,629 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/iter_10.pth\n",
      "[2023-07-05 20:41:38,631 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=112.000 weight=0.000 batch={'x': tensor([67, 45]), 'y': tensor([134,  90])}\n",
      "[2023-07-05 20:41:38,633 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=108.000 weight=0.000 batch={'x': tensor([65, 43]), 'y': tensor([130,  86])}\n",
      "[2023-07-05 20:41:38,635 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/iter_20.pth\n",
      "[2023-07-05 20:41:38,637 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=91.000 weight=0.000 batch={'x': tensor([40, 51]), 'y': tensor([ 80, 102])}\n",
      "[2023-07-05 20:41:38,639 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=123.000 weight=0.000 batch={'x': tensor([60, 63]), 'y': tensor([120, 126])}\n",
      "[2023-07-05 20:41:38,641 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/iter_30.pth\n",
      "[2023-07-05 20:41:38,642 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=39.000 weight=0.000 batch={'x': tensor([ 1, 38]), 'y': tensor([ 2, 76])}\n",
      "[2023-07-05 20:41:38,644 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=47.000 weight=0.000 batch={'x': tensor([14, 33]), 'y': tensor([28, 66])}\n",
      "[2023-07-05 20:41:38,646 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/iter_40.pth\n",
      "[2023-07-05 20:41:38,647 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=40.000 weight=0.000 batch={'x': tensor([25, 15]), 'y': tensor([50, 30])}\n",
      "[2023-07-05 20:41:38,649 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=86.000 weight=0.000 batch={'x': tensor([64, 22]), 'y': tensor([128,  44])}\n",
      "[2023-07-05 20:41:38,651 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/iter_50.pth\n",
      "[2023-07-05 20:41:38,652 89557:140704704583232][log.py:42 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=41.000 weight=0.000 batch={'x': tensor([28, 13]), 'y': tensor([56, 26])}\n",
      "[2023-07-05 20:41:38,654 89557:140704704583232][checkpoint.py:29 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ubutelf/custom_iter_based_trainer/latest.pth\n",
      "\n",
      "dict_keys(['meta', 'model', 'optimizer'])\n",
      "{'iter_': 50}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_callback_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs_root}\n",
    "    !echo\n",
    "    !cat $(find {work_dirs_root}/custom_iter_based_trainer -name '*.log' -print -quit)\n",
    "    !echo\n",
    "\n",
    "    checkpoint_path = os.path.join(work_dirs_root, 'custom_iter_based_trainer', 'iter_50.pth')\n",
    "    checkpoint: dict[str, Any] = torch.load(checkpoint_path, 'cpu')\n",
    "    print(checkpoint.keys())\n",
    "    print(checkpoint['meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_by_epoch_callback_config = epoch_based_trainer_config.copy()\n",
    "checkpoint_by_epoch_callback_config.callbacks = dict(\n",
    "    type='ComposedCallback',\n",
    "    callbacks=[checkpoint_callback, log_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 20:41:39,353 89557:140704704583232][runners.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Runner custom_epoch_based_trainer initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 20:41:39,356 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-05 20:41:39,364 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=63.000 weight=0.000 batch={'x': tensor([48, 15]), 'y': tensor([96, 30])}\n",
      "[2023-07-05 20:41:39,367 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/iter_10.pth\n",
      "[2023-07-05 20:41:39,369 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=83.000 weight=0.000 batch={'x': tensor([44, 39]), 'y': tensor([88, 78])}\n",
      "[2023-07-05 20:41:39,374 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=33.000 weight=0.000 batch={'x': tensor([17, 16]), 'y': tensor([34, 32])}\n",
      "[2023-07-05 20:41:39,382 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/iter_20.pth\n",
      "[2023-07-05 20:41:39,385 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=12.000 weight=0.000 batch={'x': tensor([11,  1]), 'y': tensor([22,  2])}\n",
      "[2023-07-05 20:41:39,391 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=86.000 weight=0.000 batch={'x': tensor([60, 26]), 'y': tensor([120,  52])}\n",
      "[2023-07-05 20:41:39,401 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/iter_30.pth\n",
      "[2023-07-05 20:41:39,407 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=37.000 weight=0.000 batch={'x': tensor([10, 27]), 'y': tensor([20, 54])}\n",
      "[2023-07-05 20:41:39,419 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-05 20:41:39,421 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=49.000 weight=0.000 batch={'x': tensor([12, 37]), 'y': tensor([24, 74])}\n",
      "[2023-07-05 20:41:39,424 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/iter_40.pth\n",
      "[2023-07-05 20:41:39,426 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=93.000 weight=0.000 batch={'x': tensor([49, 44]), 'y': tensor([98, 88])}\n",
      "[2023-07-05 20:41:39,429 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=81.000 weight=0.000 batch={'x': tensor([14, 67]), 'y': tensor([ 28, 134])}\n",
      "[2023-07-05 20:41:39,432 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/iter_50.pth\n",
      "[2023-07-05 20:41:39,433 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=22.000 weight=0.000 batch={'x': tensor([ 7, 15]), 'y': tensor([14, 30])}\n",
      "[2023-07-05 20:41:39,437 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=105.000 weight=0.000 batch={'x': tensor([39, 66]), 'y': tensor([ 78, 132])}\n",
      "[2023-07-05 20:41:39,440 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/iter_60.pth\n",
      "[2023-07-05 20:41:39,441 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=79.000 weight=0.000 batch={'x': tensor([21, 58]), 'y': tensor([ 42, 116])}\n",
      "[2023-07-05 20:41:39,444 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=32.000 weight=0.000 batch={'x': tensor([ 8, 24]), 'y': tensor([16, 48])}\n",
      "[2023-07-05 20:41:39,446 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-05 20:41:39,448 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/iter_70.pth\n",
      "[2023-07-05 20:41:39,450 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=81.000 weight=0.000 batch={'x': tensor([28, 53]), 'y': tensor([ 56, 106])}\n",
      "[2023-07-05 20:41:39,453 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=37.000 weight=0.000 batch={'x': tensor([12, 25]), 'y': tensor([24, 50])}\n",
      "[2023-07-05 20:41:39,456 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/iter_80.pth\n",
      "[2023-07-05 20:41:39,458 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=90.000 weight=0.000 batch={'x': tensor([46, 44]), 'y': tensor([92, 88])}\n",
      "[2023-07-05 20:41:39,460 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=120.000 weight=0.000 batch={'x': tensor([54, 66]), 'y': tensor([108, 132])}\n",
      "[2023-07-05 20:41:39,463 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/iter_90.pth\n",
      "[2023-07-05 20:41:39,464 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=64.000 weight=0.000 batch={'x': tensor([48, 16]), 'y': tensor([96, 32])}\n",
      "[2023-07-05 20:41:39,467 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=58.000 weight=0.000 batch={'x': tensor([ 9, 49]), 'y': tensor([18, 98])}\n",
      "[2023-07-05 20:41:39,470 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/iter_100.pth\n",
      "[2023-07-05 20:41:39,472 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=94.000 weight=0.000 batch={'x': tensor([57, 37]), 'y': tensor([114,  74])}\n",
      "[2023-07-05 20:41:39,474 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi/custom_epoch_based_trainer/latest.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpoe6pvopi\u001b[0m\n",
      "└── \u001b[1;36mcustom_epoch_based_trainer\u001b[0m\n",
      "    ├── 2023-07-05T20-41-39_356076-08-00.log\n",
      "    ├── iter_10.pth\n",
      "    ├── iter_100.pth\n",
      "    ├── iter_20.pth\n",
      "    ├── iter_30.pth\n",
      "    ├── iter_40.pth\n",
      "    ├── iter_50.pth\n",
      "    ├── iter_60.pth\n",
      "    ├── iter_70.pth\n",
      "    ├── iter_80.pth\n",
      "    ├── iter_90.pth\n",
      "    └── latest.pth\n",
      "\n",
      "2 directories, 12 files\n",
      "\n",
      "dict_keys(['meta', 'model', 'optimizer'])\n",
      "{'iter_': 90, 'epoch': 3}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_by_epoch_callback_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs_root}\n",
    "    !echo\n",
    "\n",
    "    checkpoint_path = os.path.join(work_dirs_root, 'custom_epoch_based_trainer', 'iter_90.pth')\n",
    "    checkpoint: dict[str, Any] = torch.load(checkpoint_path, 'cpu')\n",
    "    print(checkpoint.keys())\n",
    "    print(checkpoint['meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_by_epoch_callback_config.callbacks.callbacks[0].update(\n",
    "    every_n_iters=0,\n",
    "    every_n_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-05 21:24:46,684 89557:140704704583232][runners.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Runner custom_epoch_based_trainer initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-05 21:24:46,686 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-05 21:24:46,689 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=88.000 weight=0.000 batch={'x': tensor([39, 49]), 'y': tensor([78, 98])}\n",
      "[2023-07-05 21:24:46,691 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=33.000 weight=0.000 batch={'x': tensor([28,  5]), 'y': tensor([56, 10])}\n",
      "[2023-07-05 21:24:46,695 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=69.000 weight=0.000 batch={'x': tensor([21, 48]), 'y': tensor([42, 96])}\n",
      "[2023-07-05 21:24:46,704 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=109.000 weight=0.000 batch={'x': tensor([63, 46]), 'y': tensor([126,  92])}\n",
      "[2023-07-05 21:24:46,707 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=97.000 weight=0.000 batch={'x': tensor([66, 31]), 'y': tensor([132,  62])}\n",
      "[2023-07-05 21:24:46,711 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=58.000 weight=0.000 batch={'x': tensor([40, 18]), 'y': tensor([80, 36])}\n",
      "[2023-07-05 21:24:46,714 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqaqm9v2n/custom_epoch_based_trainer/epoch_1.pth\n",
      "[2023-07-05 21:24:46,716 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-05 21:24:46,719 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=92.000 weight=0.000 batch={'x': tensor([50, 42]), 'y': tensor([100,  84])}\n",
      "[2023-07-05 21:24:46,722 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=85.000 weight=0.000 batch={'x': tensor([21, 64]), 'y': tensor([ 42, 128])}\n",
      "[2023-07-05 21:24:46,724 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=82.000 weight=0.000 batch={'x': tensor([62, 20]), 'y': tensor([124,  40])}\n",
      "[2023-07-05 21:24:46,727 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=27.000 weight=0.000 batch={'x': tensor([22,  5]), 'y': tensor([44, 10])}\n",
      "[2023-07-05 21:24:46,730 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=17.000 weight=0.000 batch={'x': tensor([14,  3]), 'y': tensor([28,  6])}\n",
      "[2023-07-05 21:24:46,732 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=77.000 weight=0.000 batch={'x': tensor([28, 49]), 'y': tensor([56, 98])}\n",
      "[2023-07-05 21:24:46,735 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=85.000 weight=0.000 batch={'x': tensor([26, 59]), 'y': tensor([ 52, 118])}\n",
      "[2023-07-05 21:24:46,737 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqaqm9v2n/custom_epoch_based_trainer/epoch_2.pth\n",
      "[2023-07-05 21:24:46,738 89557:140704704583232][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-05 21:24:46,740 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=38.000 weight=0.000 batch={'x': tensor([33,  5]), 'y': tensor([66, 10])}\n",
      "[2023-07-05 21:24:46,743 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=69.000 weight=0.000 batch={'x': tensor([66,  3]), 'y': tensor([132,   6])}\n",
      "[2023-07-05 21:24:46,745 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=37.000 weight=0.000 batch={'x': tensor([11, 26]), 'y': tensor([22, 52])}\n",
      "[2023-07-05 21:24:46,747 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=25.000 weight=0.000 batch={'x': tensor([ 8, 17]), 'y': tensor([16, 34])}\n",
      "[2023-07-05 21:24:46,750 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=35.000 weight=0.000 batch={'x': tensor([29,  6]), 'y': tensor([58, 12])}\n",
      "[2023-07-05 21:24:46,752 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=36.000 weight=0.000 batch={'x': tensor([ 2, 34]), 'y': tensor([ 4, 68])}\n",
      "[2023-07-05 21:24:46,754 89557:140704704583232][log.py:42 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=89.000 weight=0.000 batch={'x': tensor([39, 50]), 'y': tensor([ 78, 100])}\n",
      "[2023-07-05 21:24:46,756 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqaqm9v2n/custom_epoch_based_trainer/epoch_3.pth\n",
      "[2023-07-05 21:24:46,757 89557:140704704583232][checkpoint.py:29 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqaqm9v2n/custom_epoch_based_trainer/latest.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqaqm9v2n\u001b[0m\n",
      "└── \u001b[1;36mcustom_epoch_based_trainer\u001b[0m\n",
      "    ├── 2023-07-05T21-24-46_685671-08-00.log\n",
      "    ├── epoch_1.pth\n",
      "    ├── epoch_2.pth\n",
      "    ├── epoch_3.pth\n",
      "    └── latest.pth\n",
      "\n",
      "2 directories, 5 files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs_root:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_by_epoch_callback_config, \n",
    "        todd.Config(work_dirs_root=work_dirs_root, model=Model()),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs_root}\n",
    "    !echo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `DRY_RUN` is enabled, the runner will stop upon the first log message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
