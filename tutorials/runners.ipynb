{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd_ai 0.5.1\n",
      "Uninstalling todd_ai-0.5.1:\n",
      "  Successfully uninstalled todd_ai-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install --extra-index-url https://pypi.org/simple .. > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import todd\n",
    "from todd.runners import Memo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.registries.ModelRegistry.register_()\n",
    "class RunnerModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def _forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        runner: todd.runners.BaseRunner,\n",
    "        batch,\n",
    "        memo: Memo,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> Memo:\n",
    "        log: dict[str, Any] | None = memo.get(\"log\")\n",
    "        y = self._forward(batch[\"x\"])\n",
    "        loss = F.l1_loss(y, batch[\"y\"])\n",
    "        memo[\"loss\"] = loss\n",
    "        if log is not None:\n",
    "            log[\"batch\"] = str(batch)\n",
    "            log[\"weight\"] = f\"{self._weight.item():.3f}\"\n",
    "            log[\"loss\"] = f\"{loss:.3f}\"\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.registries.DatasetRegistry.register_()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:13,752 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:13,755 25854:140704458489472][base.py:67 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zsh:1: command not found: tree\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    logger=dict(),\n",
    "    callbacks=[],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:15,523 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:15,528 25854:140704458489472][base.py:67 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:15,536 25854:140704458489472][log.py:87 todd.Validator.validator after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:15,540 25854:140704458489472][log.py:87 todd.Validator.validator after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-26 16:57:15,544 25854:140704458489472][log.py:87 todd.Validator.validator after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-26 16:57:15,549 25854:140704458489472][log.py:87 todd.Validator.validator after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zsh:1: command not found: tree\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[dict(type='LogCallback', interval=5)],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:17,365 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:17,370 25854:140704458489472][base.py:67 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-06-26 16:57:17,370 25854:140704458489472][base.py:67 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:17,379 25854:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [1/8] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.000 loss=12.000\n",
      "[2024-06-26 16:57:17,382 25854:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [2/8] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.000 loss=11.000\n",
      "[2024-06-26 16:57:17,385 25854:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [3/8] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:17,389 25854:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [4/8] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.000 loss=9.000\n",
      "[2024-06-26 16:57:17,392 25854:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [5/8] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.000 loss=13.000\n",
      "[2024-06-26 16:57:17,397 25854:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [6/8] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.000 loss=7.000\n",
      "[2024-06-26 16:57:17,399 25854:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [7/8] batch={'x': tensor([2, 5]), 'y': tensor([ 4, 10])} weight=0.000 loss=7.000\n",
      "[2024-06-26 16:57:17,403 25854:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [8/8] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.000 loss=18.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"iter_based_trainer\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:17,424 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:17,428 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:17,431 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "\u001b[2m[2024-06-26 16:57:17,428 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:17,431 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-26 16:57:17,436 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [1/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.000 loss=15.000\n",
      "[2024-06-26 16:57:17,439 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [2/15] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.000 loss=8.000\n",
      "[2024-06-26 16:57:17,442 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [3/15] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.000 loss=12.000\n",
      "[2024-06-26 16:57:17,447 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [4/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.000 loss=15.000\n",
      "[2024-06-26 16:57:17,450 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [5/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.000 loss=5.000\n",
      "[2024-06-26 16:57:17,452 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-26 16:57:17,456 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [6/15] batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])} weight=0.000 loss=17.000\n",
      "[2024-06-26 16:57:17,459 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [7/15] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.000 loss=8.000\n",
      "[2024-06-26 16:57:17,461 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [8/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.000 loss=8.000\n",
      "[2024-06-26 16:57:17,463 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [9/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.000 loss=12.000\n",
      "[2024-06-26 16:57:17,467 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [10/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:17,468 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-26 16:57:17,471 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [11/15] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.000 loss=6.000\n",
      "[2024-06-26 16:57:17,474 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.000 loss=19.000\n",
      "[2024-06-26 16:57:17,476 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [13/15] batch={'x': tensor([6, 5]), 'y': tensor([12, 10])} weight=0.000 loss=11.000\n",
      "[2024-06-26 16:57:17,479 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [14/15] batch={'x': tensor([7, 3]), 'y': tensor([14,  6])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:17,482 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [15/15] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.000 loss=9.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"epoch_based_trainer\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:17,498 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-26 16:57:18,102 25854:140704458489472][log.py:49 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.10.0\n",
      "todd_version: 0.5.1\n",
      "cuda_home: None\n",
      "git_commit_id: a530070\n",
      "git_status: M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-06-26 16:57:18,105 25854:140704458489472][base.py:67 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:18,112 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:18,116 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-26 16:57:18,120 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-26 16:57:18,124 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:18,157 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:18,161 25854:140704458489472][base.py:67 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:18,169 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:18,174 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-26 16:57:18,178 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-26 16:57:18,183 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zsh:1: command not found: tree\n",
      "\n",
      "\u001b[2m[2024-06-26 16:57:18,161 25854:140704458489472][base.py:67 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:18,169 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:18,174 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-26 16:57:18,178 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-26 16:57:18,183 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='log_callback',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type='LogCallback',\n",
    "            interval=5,\n",
    "            with_file_handler=True,\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/log_callback/*.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:21,568 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:21,587 25854:140704458489472][base.py:67 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-06-26 16:57:21,587 25854:140704458489472][base.py:67 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:22,110 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:01 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:22,630 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:01 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-26 16:57:23,155 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-26 16:57:23,681 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:23,701 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:23,703 25854:140704458489472][base.py:67 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-06-26 16:57:23,703 25854:140704458489472][base.py:67 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:25,229 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:04 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:29,253 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:05 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-26 16:57:34,277 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:03 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-26 16:57:39,299 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"EMA_ETA\", ema=dict(decay=0.2)),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1 * min(10, runner.iter_))\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:39,324 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:39,569 25854:140704458489472][log.py:49 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.10.0\n",
      "todd_version: 0.5.1\n",
      "cuda_home: None\n",
      "git_commit_id: a530070\n",
      "git_status: M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-06-26 16:57:39,570 25854:140704458489472][base.py:67 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:39,577 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:00 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:39,580 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:00 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "\u001b[2m[2024-06-26 16:57:39,570 25854:140704458489472][base.py:67 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:39,577 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:00 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-26 16:57:39,580 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:00 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-26 16:57:39,583 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-26 16:57:39,586 25854:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "            with_file_handler=True,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:39,606 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-26 16:57:39,735 25854:140704458489472][git.py:43 todd.Validator.git_callback init] INFO: Saving git diff to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpk7571pfo/git_callback/git_diff_2024-06-26T16-57-39_734738-08-00.log\n",
      "\u001b[2m[2024-06-26 16:57:39,738 25854:140704458489472][base.py:67 todd.Validator.git_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"git_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(type=\"GitCallback\", diff='HEAD -- \":(exclude)*.ipynb\"'),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/git_callback/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:41,497 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:41,501 25854:140704458489472][base.py:67 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:41,506 25854:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.000 loss=15.000\n",
      "\u001b[2m[2024-06-26 16:57:41,501 25854:140704458489472][base.py:67 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:41,506 25854:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.000 loss=15.000\n",
      "[2024-06-26 16:57:41,508 25854:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.037 loss=10.794\n",
      "[2024-06-26 16:57:41,511 25854:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.065 loss=11.610\n",
      "[2024-06-26 16:57:41,514 25854:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.095 loss=10.477\n",
      "[2024-06-26 16:57:41,517 25854:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.123 loss=5.632\n",
      "[2024-06-26 16:57:41,519 25854:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.138 loss=11.175\n",
      "[2024-06-26 16:57:41,521 25854:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([6, 5]), 'y': tensor([12, 10])} weight=0.168 loss=10.079\n",
      "[2024-06-26 16:57:41,524 25854:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.195 loss=10.830\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"optimize_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:41,539 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:41,542 25854:140704458489472][base.py:67 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:41,548 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.000 loss=10.000 lr=['1.667e-03']\n",
      "\u001b[2m[2024-06-26 16:57:41,542 25854:140704458489472][base.py:67 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:41,548 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.000 loss=10.000 lr=['1.667e-03']\n",
      "[2024-06-26 16:57:41,550 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.008 loss=8.962 lr=['2.333e-03']\n",
      "[2024-06-26 16:57:41,553 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.019 loss=16.840 lr=['3.000e-03']\n",
      "[2024-06-26 16:57:41,556 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.044 loss=2.934 lr=['3.667e-03']\n",
      "[2024-06-26 16:57:41,558 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([10,  6]), 'y': tensor([20, 12])} weight=0.050 loss=15.601 lr=['4.333e-03']\n",
      "[2024-06-26 16:57:41,561 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([10,  6]), 'y': tensor([20, 12])} weight=0.084 loss=15.324 lr=['5.000e-03']\n",
      "[2024-06-26 16:57:41,564 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([8, 3]), 'y': tensor([16,  6])} weight=0.124 loss=10.315 lr=['5.000e-03']\n",
      "[2024-06-26 16:57:41,566 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.152 loss=2.772 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=5),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:41,582 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:41,585 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:41,587 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "\u001b[2m[2024-06-26 16:57:41,585 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:41,587 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "[2024-06-26 16:57:41,592 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/10] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.000 loss=6.000 lr=['1.667e-03']\n",
      "[2024-06-26 16:57:41,595 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/10] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.005 loss=3.990 lr=['1.667e-03']\n",
      "[2024-06-26 16:57:41,596 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [2/5]\n",
      "[2024-06-26 16:57:41,599 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.008 loss=6.971 lr=['2.778e-03']\n",
      "[2024-06-26 16:57:41,601 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/10] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.018 loss=2.973 lr=['2.778e-03']\n",
      "[2024-06-26 16:57:41,602 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [3/5]\n",
      "[2024-06-26 16:57:41,605 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/10] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.022 loss=4.944 lr=['3.889e-03']\n",
      "[2024-06-26 16:57:41,608 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/10] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.032 loss=4.920 lr=['3.889e-03']\n",
      "[2024-06-26 16:57:41,610 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [4/5]\n",
      "[2024-06-26 16:57:41,612 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.042 loss=6.854 lr=['5.000e-03']\n",
      "[2024-06-26 16:57:41,615 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.059 loss=2.911 lr=['5.000e-03']\n",
      "[2024-06-26 16:57:41,616 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [5/5]\n",
      "[2024-06-26 16:57:41,619 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [9/10] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.067 loss=5.800 lr=['5.000e-03']\n",
      "[2024-06-26 16:57:41,622 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [10/10] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.082 loss=3.837 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=4),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=3),\n",
    "            by_epoch=True,\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=5,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:41,637 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-26 16:57:41,639 25854:140704458489472][lr.py:95 todd.IterBasedTrainer.lr_scale_callback _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-06-26 16:57:41,640 25854:140704458489472][base.py:67 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:41,645 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.000 loss=12.000\n",
      "[2024-06-26 16:57:41,639 25854:140704458489472][lr.py:95 todd.IterBasedTrainer.lr_scale_callback _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-06-26 16:57:41,640 25854:140704458489472][base.py:67 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:41,645 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.000 loss=12.000\n",
      "[2024-06-26 16:57:41,648 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.060 loss=12.610\n",
      "[2024-06-26 16:57:41,651 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.125 loss=8.438\n",
      "[2024-06-26 16:57:41,653 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.170 loss=7.320\n",
      "[2024-06-26 16:57:41,655 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.210 loss=11.635\n",
      "[2024-06-26 16:57:41,658 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([8, 3]), 'y': tensor([16,  6])} weight=0.275 loss=9.488\n",
      "[2024-06-26 16:57:41,660 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.330 loss=6.680\n",
      "[2024-06-26 16:57:41,663 25854:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.370 loss=5.705\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_scale_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScaleCallback\",\n",
    "            lr_scaler=dict(base_batch_size=1),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:41,686 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:41,689 25854:140704458489472][base.py:67 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:41,694 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.000 loss=13.000\n",
      "[2024-06-26 16:57:41,695 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_1\n",
      "[2024-06-26 16:57:41,701 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([7, 9]), 'y': tensor([14, 18])} weight=0.032 loss=15.740\n",
      "[2024-06-26 16:57:41,703 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-06-26 16:57:41,767 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.072 loss=7.710\n",
      "[2024-06-26 16:57:41,768 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_3\n",
      "[2024-06-26 16:57:41,787 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.093 loss=8.584\n",
      "[2024-06-26 16:57:41,789 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-06-26 16:57:41,794 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([8, 1]), 'y': tensor([16,  2])} weight=0.115 loss=8.483\n",
      "[2024-06-26 16:57:41,795 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-06-26 16:57:41,801 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.138 loss=8.381\n",
      "[2024-06-26 16:57:41,802 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-06-26 16:57:41,807 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.160 loss=15.640\n",
      "[2024-06-26 16:57:41,808 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-06-26 16:57:41,814 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.203 loss=8.988\n",
      "[2024-06-26 16:57:41,815 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zsh:1: command not found: tree\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:44,448 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-26 16:57:44,451 25854:140704458489472][checkpoint.py:55 todd.IterBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-06-26 16:57:44,457 25854:140704458489472][base.py:65 todd.IterBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-26 16:57:44,458 25854:140704458489472][base.py:67 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:44,462 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.138 loss=9.312\n",
      "[2024-06-26 16:57:44,463 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-06-26 16:57:44,469 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])} weight=0.162 loss=15.619\n",
      "[2024-06-26 16:57:44,470 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-06-26 16:57:44,476 25854:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([2, 5]), 'y': tensor([ 4, 10])} weight=0.205 loss=6.283\n",
      "[2024-06-26 16:57:44,477 25854:140704458489472][checkpoint.py:81 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy2gxoy40/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {0: {'momentum_buffer': None}}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 5}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.1375))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_5 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_5'\n",
    "    for f in iter_5.glob('*.pth'):\n",
    "        print(f'{f.name}:')\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()\n",
    "\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_5),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:44,524 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:44,526 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:44,528 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-26 16:57:44,532 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.000 loss=8.000\n",
      "[2024-06-26 16:57:44,534 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.020 loss=11.880\n",
      "[2024-06-26 16:57:44,536 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-06-26 16:57:44,541 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.050 loss=17.550\n",
      "[2024-06-26 16:57:44,543 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.095 loss=3.810\n",
      "[2024-06-26 16:57:44,544 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-06-26 16:57:44,549 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.105 loss=12.318\n",
      "[2024-06-26 16:57:44,551 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-26 16:57:44,553 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.137 loss=10.244\n",
      "[2024-06-26 16:57:44,554 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-06-26 16:57:44,559 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.165 loss=5.505\n",
      "[2024-06-26 16:57:44,561 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.180 loss=10.010\n",
      "[2024-06-26 16:57:44,562 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-06-26 16:57:44,567 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.207 loss=14.340\n",
      "[2024-06-26 16:57:44,569 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.248 loss=9.639\n",
      "[2024-06-26 16:57:44,570 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-06-26 16:57:44,575 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-26 16:57:44,577 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.275 loss=15.525\n",
      "[2024-06-26 16:57:44,579 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.320 loss=6.720\n",
      "[2024-06-26 16:57:44,580 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-06-26 16:57:44,585 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.340 loss=9.130\n",
      "[2024-06-26 16:57:44,587 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.368 loss=3.265\n",
      "[2024-06-26 16:57:44,588 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-06-26 16:57:44,593 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.377 loss=11.358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zsh:1: command not found: tree\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:47,069 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-26 16:57:47,072 25854:140704458489472][checkpoint.py:55 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-06-26 16:57:47,077 25854:140704458489472][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-26 16:57:47,077 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:47,078 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-26 16:57:47,083 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.207 loss=10.755\n",
      "[2024-06-26 16:57:47,086 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.237 loss=13.219\n",
      "[2024-06-26 16:57:47,087 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-06-26 16:57:47,092 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-26 16:57:47,094 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.275 loss=9.488\n",
      "[2024-06-26 16:57:47,097 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.303 loss=9.336\n",
      "[2024-06-26 16:57:47,098 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-06-26 16:57:47,104 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.330 loss=11.690\n",
      "[2024-06-26 16:57:47,106 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.365 loss=4.905\n",
      "[2024-06-26 16:57:47,107 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-06-26 16:57:47,112 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([10,  3]), 'y': tensor([20,  6])} weight=0.380 loss=10.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:49,690 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-26 16:57:49,692 25854:140704458489472][checkpoint.py:55 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-06-26 16:57:49,697 25854:140704458489472][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-26 16:57:49,698 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:49,699 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-26 16:57:49,692 25854:140704458489472][checkpoint.py:55 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-06-26 16:57:49,697 25854:140704458489472][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-26 16:57:49,698 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:49,699 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-26 16:57:49,703 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.275 loss=12.938\n",
      "[2024-06-26 16:57:49,705 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([10,  4]), 'y': tensor([20,  8])} weight=0.312 loss=11.812\n",
      "[2024-06-26 16:57:49,706 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-06-26 16:57:49,712 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.347 loss=4.957\n",
      "[2024-06-26 16:57:49,715 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.362 loss=4.094\n",
      "[2024-06-26 16:57:49,716 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpphcic27h/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-06-26 16:57:49,721 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([8, 7]), 'y': tensor([16, 14])} weight=0.375 loss=12.188\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=2),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_8 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_8'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_8),\n",
    "        )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    iter_10 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_10'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_10),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:49,753 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:49,755 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:49,756 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-26 16:57:49,760 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.000 loss=11.000\n",
      "[2024-06-26 16:57:49,762 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.027 loss=11.835\n",
      "[2024-06-26 16:57:49,764 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.057 loss=11.655\n",
      "[2024-06-26 16:57:49,767 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.087 loss=11.475\n",
      "[2024-06-26 16:57:49,769 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.117 loss=7.530\n",
      "[2024-06-26 16:57:49,771 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp6zmfeuog/checkpoint_callback/checkpoints/epoch_1\n",
      "[2024-06-26 16:57:49,774 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-26 16:57:49,777 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.138 loss=12.106\n",
      "[2024-06-26 16:57:49,779 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.170 loss=17.385\n",
      "[2024-06-26 16:57:49,782 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.218 loss=7.130\n",
      "[2024-06-26 16:57:49,785 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.237 loss=8.812\n",
      "[2024-06-26 16:57:49,789 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.262 loss=4.344\n",
      "[2024-06-26 16:57:49,791 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp6zmfeuog/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-06-26 16:57:49,796 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-26 16:57:49,799 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([3, 5]), 'y': tensor([ 6, 10])} weight=0.275 loss=6.900\n",
      "[2024-06-26 16:57:49,802 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.295 loss=10.230\n",
      "[2024-06-26 16:57:49,804 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([8, 1]), 'y': tensor([16,  2])} weight=0.325 loss=7.537\n",
      "[2024-06-26 16:57:49,807 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.347 loss=9.089\n",
      "[2024-06-26 16:57:49,809 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.375 loss=12.188\n",
      "[2024-06-26 16:57:49,811 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp6zmfeuog/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zsh:1: command not found: tree\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:52,298 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-26 16:57:52,301 25854:140704458489472][checkpoint.py:55 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp6zmfeuog/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-06-26 16:57:52,305 25854:140704458489472][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-26 16:57:52,307 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:52,308 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-26 16:57:52,301 25854:140704458489472][checkpoint.py:55 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp6zmfeuog/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-06-26 16:57:52,305 25854:140704458489472][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-26 16:57:52,307 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:52,308 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-26 16:57:52,311 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.275 loss=8.625\n",
      "[2024-06-26 16:57:52,314 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.300 loss=15.300\n",
      "[2024-06-26 16:57:52,316 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([7, 9]), 'y': tensor([14, 18])} weight=0.345 loss=13.240\n",
      "[2024-06-26 16:57:52,319 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 5]), 'y': tensor([ 6, 10])} weight=0.385 loss=6.460\n",
      "[2024-06-26 16:57:52,321 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.405 loss=2.392\n",
      "[2024-06-26 16:57:52,322 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp6zmfeuog/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'epoch_2'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(epoch_2),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.registries.RunnerRegistry.register_()\n",
    "class FaultyValidator(todd.runners.Validator):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError(\"faulty runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:52,443 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:52,445 25854:140704458489472][base.py:67 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-06-26 16:57:52,448 25854:140704458489472][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x148d04890>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 232, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_25854/454645826.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m[2024-06-26 16:57:52,445 25854:140704458489472][base.py:67 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-06-26 16:57:52,448 25854:140704458489472][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x148d04890>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 232, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_25854/454645826.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='FaultyValidator',\n",
    "    name='monitor_callback',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(type='MonitorCallback'),\n",
    "        dict(type='LogCallback', interval=5, with_file_handler=True),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/monitor_callback/*.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:54,136 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:54,139 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:54,140 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-26 16:57:54,143 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.000 loss=17.000\n",
      "[2024-06-26 16:57:54,146 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.043 loss=10.766\n",
      "[2024-06-26 16:57:54,149 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.070 loss=12.545\n",
      "[2024-06-26 16:57:54,152 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.102 loss=10.436\n",
      "[2024-06-26 16:57:54,156 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.130 loss=2.805\n",
      "[2024-06-26 16:57:54,160 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpeuzo5_w0/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-06-26 16:57:54,165 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-26 16:57:54,168 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([10,  4]), 'y': tensor([20,  8])} weight=0.137 loss=13.038\n",
      "[2024-06-26 16:57:54,170 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.172 loss=13.706\n",
      "[2024-06-26 16:57:54,173 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.210 loss=10.740\n",
      "[2024-06-26 16:57:54,176 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.240 loss=7.920\n",
      "[2024-06-26 16:57:54,178 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.262 loss=4.344\n",
      "[2024-06-26 16:57:54,180 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpeuzo5_w0/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-06-26 16:57:54,184 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-26 16:57:54,187 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.275 loss=12.075\n",
      "[2024-06-26 16:57:54,190 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.310 loss=6.760\n",
      "[2024-06-26 16:57:54,192 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.330 loss=15.865\n",
      "[2024-06-26 16:57:54,195 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.377 loss=4.867\n",
      "[2024-06-26 16:57:54,197 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.392 loss=6.430\n",
      "[2024-06-26 16:57:54,199 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpeuzo5_w0/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 16:57:56,721 25854:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-26 16:57:56,724 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:56,725 25854:140704458489472][base.py:80 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpeuzo5_w0/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-06-26 16:57:56,727 25854:140704458489472][base.py:65 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-26 16:57:56,724 25854:140704458489472][base.py:67 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-26 16:57:56,725 25854:140704458489472][base.py:80 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpeuzo5_w0/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-06-26 16:57:56,727 25854:140704458489472][base.py:65 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "[2024-06-26 16:57:56,734 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-26 16:57:56,747 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.275 loss=11.212\n",
      "[2024-06-26 16:57:56,750 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.307 loss=15.233\n",
      "[2024-06-26 16:57:56,752 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([1, 5]), 'y': tensor([ 2, 10])} weight=0.352 loss=4.943\n",
      "[2024-06-26 16:57:56,755 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.367 loss=4.898\n",
      "[2024-06-26 16:57:56,758 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.382 loss=9.705\n",
      "[2024-06-26 16:57:56,767 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpeuzo5_w0/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-06-26 16:57:56,775 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-26 16:57:56,779 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.412 loss=6.350\n",
      "[2024-06-26 16:57:56,782 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.432 loss=4.703\n",
      "[2024-06-26 16:57:56,785 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.447 loss=13.972\n",
      "[2024-06-26 16:57:56,788 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.492 loss=6.030\n",
      "[2024-06-26 16:57:56,792 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.512 loss=11.156\n",
      "[2024-06-26 16:57:56,795 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpeuzo5_w0/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-06-26 16:57:56,799 25854:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-26 16:57:56,802 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([10,  3]), 'y': tensor([20,  6])} weight=0.550 loss=9.425\n",
      "[2024-06-26 16:57:56,805 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.582 loss=7.796\n",
      "[2024-06-26 16:57:56,808 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([6, 5]), 'y': tensor([12, 10])} weight=0.610 loss=7.645\n",
      "[2024-06-26 16:57:56,811 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.637 loss=3.406\n",
      "[2024-06-26 16:57:56,813 25854:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([7, 8]), 'y': tensor([14, 16])} weight=0.650 loss=10.125\n",
      "[2024-06-26 16:57:56,815 25854:140704458489472][checkpoint.py:81 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpeuzo5_w0/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"strategy_load_model_from\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = (pathlib.Path(work_dirs) / 'strategy_load_model_from' / 'checkpoints' / 'epoch_2' / 'model.pth')\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.strategy.load_model_from(epoch_2)\n",
    "    runner.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
