{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing /Users/bytedance/Developer/todd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.6.1)\n",
      "Requirement already satisfied: lmdb in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (1.4.1)\n",
      "Requirement already satisfied: opencv-python in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.7.0.72)\n",
      "Requirement already satisfied: pandas in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: python-pptx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.6.21)\n",
      "Requirement already satisfied: timm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.9.2)\n",
      "Requirement already satisfied: toml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.5.0)\n",
      "Requirement already satisfied: yapf in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from opencv-python->todd-ai==0.4.0) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2023.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (4.9.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (9.5.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.14.1)\n",
      "Requirement already satisfied: safetensors in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.3.1)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from yapf->todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->todd-ai==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.12.0)\n",
      "Requirement already satisfied: sympy in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (2023.5.0)\n",
      "Requirement already satisfied: requests in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (2.30.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (23.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from jinja2->torch>=1.7->timm->todd-ai==0.4.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from sympy->torch>=1.7->timm->todd-ai==0.4.0) (1.3.0)\n",
      "Building wheels for collected packages: todd-ai\n",
      "  Building wheel for todd-ai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for todd-ai: filename=todd_ai-0.4.0-py3-none-any.whl size=104120 sha256=4a79610704328db1d032fd3043602dc99a2cac6fb4be42d0574fb362f6fb1142\n",
      "  Stored in directory: /private/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/pip-ephem-wheel-cache-x01z44ue/wheels/15/ef/5a/9fc12e257ce5cef16b333a2ed6c992ff9cbcc9167f7199e6ac\n",
      "Successfully built todd-ai\n",
      "Installing collected packages: todd-ai\n",
      "Successfully installed todd-ai-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "[2023-07-26 19:05:05,868 86878:140704676111936][patches.py:72 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tempfile\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict, cast\n",
    "\n",
    "import todd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "Memo = dict[str, Any]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register()\n",
    "class RunnerModel(todd.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> Memo:\n",
    "        y: torch.Tensor = self._strategy.model(batch['x'])\n",
    "        loss = F.l1_loss(y, batch['y'])\n",
    "        memo['loss'] = loss\n",
    "        if 'log' in memo:\n",
    "            memo['log']['loss'] = f'{loss.item():.3f}'\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerMixin(RunnerMixin):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> Memo:\n",
    "        memo = super()._run_iter(batch, memo)\n",
    "        if 'log' in memo:\n",
    "            model = cast(RunnerModel, self._strategy.module)\n",
    "            memo['log']['weight'] = f'{model.weight.item():.3f}'\n",
    "            memo['log']['batch'] = str(batch)\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomValidator(RunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomIterBasedTrainer(TrainerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomEpochBasedTrainer(TrainerMixin, todd.runners.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_demo = todd.Config(\n",
    "    type='CustomValidator',\n",
    "    name='custom_validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=dict(type='LogCallback', interval=5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:06,543 86878:140704676111936][base.py:52 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:06,545 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] loss=10.000\n",
      "[2023-07-26 19:05:06,547 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] loss=20.000\n",
      "[2023-07-26 19:05:06,549 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] loss=30.000\n",
      "[2023-07-26 19:05:06,550 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp4yhsm6_1\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_demo = validator_demo.copy()\n",
    "trainer_demo.pop('type')\n",
    "trainer_demo.dataloader = todd.Config(\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    dataset=dict(type='RunnerDataset', n=67),\n",
    ")\n",
    "trainer_demo.optimizer = todd.Config(type='SGD', lr=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_based_trainer_demo = trainer_demo.copy()\n",
    "iter_based_trainer_demo.type = 'CustomIterBasedTrainer'\n",
    "iter_based_trainer_demo.name = 'custom_iter_based_trainer'\n",
    "iter_based_trainer_demo.iters = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:06,863 86878:140704676111936][base.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:06,867 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=87.000 weight=0.000 batch={'x': tensor([59, 28]), 'y': tensor([118,  56])}\n",
      "[2023-07-26 19:05:06,870 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=114.000 weight=0.000 batch={'x': tensor([49, 65]), 'y': tensor([ 98, 130])}\n",
      "[2023-07-26 19:05:06,872 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=70.000 weight=0.000 batch={'x': tensor([20, 50]), 'y': tensor([ 40, 100])}\n",
      "[2023-07-26 19:05:06,873 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=13.000 weight=0.000 batch={'x': tensor([ 2, 11]), 'y': tensor([ 4, 22])}\n",
      "[2023-07-26 19:05:06,875 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=82.000 weight=0.000 batch={'x': tensor([58, 24]), 'y': tensor([116,  48])}\n",
      "[2023-07-26 19:05:06,877 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=93.000 weight=0.000 batch={'x': tensor([45, 48]), 'y': tensor([90, 96])}\n",
      "[2023-07-26 19:05:06,879 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=63.000 weight=0.000 batch={'x': tensor([36, 27]), 'y': tensor([72, 54])}\n",
      "[2023-07-26 19:05:06,881 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=77.000 weight=0.000 batch={'x': tensor([63, 14]), 'y': tensor([126,  28])}\n",
      "[2023-07-26 19:05:06,883 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=108.000 weight=0.000 batch={'x': tensor([51, 57]), 'y': tensor([102, 114])}\n",
      "[2023-07-26 19:05:06,884 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=61.000 weight=0.000 batch={'x': tensor([39, 22]), 'y': tensor([78, 44])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        iter_based_trainer_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_based_trainer_demo = trainer_demo.copy()\n",
    "epoch_based_trainer_demo.type = 'CustomEpochBasedTrainer'\n",
    "epoch_based_trainer_demo.name = 'custom_epoch_based_trainer'\n",
    "epoch_based_trainer_demo.epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:06,947 86878:140704676111936][base.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:06,951 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-26 19:05:06,954 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=75.000 weight=0.000 batch={'x': tensor([18, 57]), 'y': tensor([ 36, 114])}\n",
      "[2023-07-26 19:05:06,956 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=45.000 weight=0.000 batch={'x': tensor([37,  8]), 'y': tensor([74, 16])}\n",
      "[2023-07-26 19:05:06,958 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=108.000 weight=0.000 batch={'x': tensor([49, 59]), 'y': tensor([ 98, 118])}\n",
      "[2023-07-26 19:05:06,960 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=39.000 weight=0.000 batch={'x': tensor([13, 26]), 'y': tensor([26, 52])}\n",
      "[2023-07-26 19:05:06,964 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=37.000 weight=0.000 batch={'x': tensor([ 9, 28]), 'y': tensor([18, 56])}\n",
      "[2023-07-26 19:05:06,969 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=80.000 weight=0.000 batch={'x': tensor([51, 29]), 'y': tensor([102,  58])}\n",
      "[2023-07-26 19:05:06,971 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-26 19:05:06,973 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=105.000 weight=0.000 batch={'x': tensor([61, 44]), 'y': tensor([122,  88])}\n",
      "[2023-07-26 19:05:06,975 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=81.000 weight=0.000 batch={'x': tensor([26, 55]), 'y': tensor([ 52, 110])}\n",
      "[2023-07-26 19:05:06,978 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=87.000 weight=0.000 batch={'x': tensor([60, 27]), 'y': tensor([120,  54])}\n",
      "[2023-07-26 19:05:06,981 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=81.000 weight=0.000 batch={'x': tensor([19, 62]), 'y': tensor([ 38, 124])}\n",
      "[2023-07-26 19:05:06,985 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=18.000 weight=0.000 batch={'x': tensor([17,  1]), 'y': tensor([34,  2])}\n",
      "[2023-07-26 19:05:06,987 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=71.000 weight=0.000 batch={'x': tensor([33, 38]), 'y': tensor([66, 76])}\n",
      "[2023-07-26 19:05:06,989 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=68.000 weight=0.000 batch={'x': tensor([ 9, 59]), 'y': tensor([ 18, 118])}\n",
      "[2023-07-26 19:05:06,991 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-26 19:05:06,992 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=50.000 weight=0.000 batch={'x': tensor([14, 36]), 'y': tensor([28, 72])}\n",
      "[2023-07-26 19:05:06,994 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=88.000 weight=0.000 batch={'x': tensor([27, 61]), 'y': tensor([ 54, 122])}\n",
      "[2023-07-26 19:05:06,996 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=86.000 weight=0.000 batch={'x': tensor([66, 20]), 'y': tensor([132,  40])}\n",
      "[2023-07-26 19:05:06,998 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=75.000 weight=0.000 batch={'x': tensor([67,  8]), 'y': tensor([134,  16])}\n",
      "[2023-07-26 19:05:07,000 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=60.000 weight=0.000 batch={'x': tensor([38, 22]), 'y': tensor([76, 44])}\n",
      "[2023-07-26 19:05:07,002 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=74.000 weight=0.000 batch={'x': tensor([40, 34]), 'y': tensor([80, 68])}\n",
      "[2023-07-26 19:05:07,003 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=37.000 weight=0.000 batch={'x': tensor([21, 16]), 'y': tensor([42, 32])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        epoch_based_trainer_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_callback_demo = validator_demo.copy()\n",
    "log_callback = log_callback_demo.callbacks\n",
    "log_callback.with_file_handler = True\n",
    "log_callback_demo.callbacks = [log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:07,022 86878:140704676111936][base.py:52 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:07,025 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] loss=10.000\n",
      "[2023-07-26 19:05:07,026 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] loss=20.000\n",
      "[2023-07-26 19:05:07,028 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] loss=30.000\n",
      "[2023-07-26 19:05:07,030 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1r6nblpz\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "    └── 2023-07-26T19-05-07_022077-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2023-07-26 19:05:07,022 86878:140704676111936][base.py:52 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-07-26 19:05:07,025 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] loss=10.000\n",
      "[2023-07-26 19:05:07,026 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] loss=20.000\n",
      "[2023-07-26 19:05:07,028 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] loss=30.000\n",
      "[2023-07-26 19:05:07,030 86878:140704676111936][log.py:48 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] loss=40.000\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        log_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_callback_demo = iter_based_trainer_demo.copy()\n",
    "optimize_callback = todd.Config(type='OptimizeCallback')\n",
    "optimize_callback_demo.callbacks = [optimize_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:07,590 86878:140704676111936][base.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:07,594 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=44.240 weight=0.618 batch={'x': tensor([14, 50]), 'y': tensor([ 28, 100])}\n",
      "[2023-07-26 19:05:07,597 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=5.445 weight=1.395 batch={'x': tensor([15,  3]), 'y': tensor([30,  6])}\n",
      "[2023-07-26 19:05:07,600 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=0.963 weight=1.975 batch={'x': tensor([40, 37]), 'y': tensor([80, 74])}\n",
      "[2023-07-26 19:05:07,603 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=4.204 weight=2.143 batch={'x': tensor([52,  7]), 'y': tensor([104,  14])}\n",
      "[2023-07-26 19:05:07,606 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=10.375 weight=1.793 batch={'x': tensor([67, 33]), 'y': tensor([134,  66])}\n",
      "[2023-07-26 19:05:07,609 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=6.757 weight=1.873 batch={'x': tensor([42, 64]), 'y': tensor([ 84, 128])}\n",
      "[2023-07-26 19:05:07,612 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=2.791 weight=2.072 batch={'x': tensor([22, 55]), 'y': tensor([ 44, 110])}\n",
      "[2023-07-26 19:05:07,615 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=3.570 weight=2.105 batch={'x': tensor([48, 20]), 'y': tensor([96, 40])}\n",
      "[2023-07-26 19:05:07,618 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=1.073 weight=1.903 batch={'x': tensor([21,  1]), 'y': tensor([42,  2])}\n",
      "[2023-07-26 19:05:07,620 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=5.740 weight=2.140 batch={'x': tensor([57, 25]), 'y': tensor([114,  50])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        optimize_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_schedule_callback = todd.Config(\n",
    "    type='LRScheduleCallback',\n",
    "    lr_scheduler=dict(type='LinearLR', total_iters=10),\n",
    ")\n",
    "lr_schedule_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_callback,\n",
    "    log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:07,637 86878:140704676111936][base.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:07,640 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=58.149 weight=0.362 batch={'x': tensor([56, 15]), 'y': tensor([112,  30])} lr=['3.333e-03']\n",
      "[2023-07-26 19:05:07,643 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=64.715 weight=0.930 batch={'x': tensor([54, 67]), 'y': tensor([108, 134])} lr=['5.000e-03']\n",
      "[2023-07-26 19:05:07,646 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=13.367 weight=1.733 batch={'x': tensor([59, 41]), 'y': tensor([118,  82])} lr=['5.000e-03']\n",
      "[2023-07-26 19:05:07,650 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=3.706 weight=2.100 batch={'x': tensor([30, 44]), 'y': tensor([60, 88])} lr=['5.000e-03']\n",
      "[2023-07-26 19:05:07,653 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=0.642 weight=2.143 batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} lr=['5.000e-03']\n",
      "[2023-07-26 19:05:07,656 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=7.094 weight=2.130 batch={'x': tensor([51, 58]), 'y': tensor([102, 116])} lr=['5.000e-03']\n",
      "[2023-07-26 19:05:07,659 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=2.220 weight=2.058 batch={'x': tensor([60, 17]), 'y': tensor([120,  34])} lr=['5.000e-03']\n",
      "[2023-07-26 19:05:07,662 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=4.046 weight=1.850 batch={'x': tensor([33, 21]), 'y': tensor([66, 42])} lr=['5.000e-03']\n",
      "[2023-07-26 19:05:07,665 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=2.762 weight=2.230 batch={'x': tensor([16,  8]), 'y': tensor([32, 16])} lr=['5.000e-03']\n",
      "[2023-07-26 19:05:07,668 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=6.248 weight=2.130 batch={'x': tensor([61, 35]), 'y': tensor([122,  70])} lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "lr_schedule_by_epoch_callback = lr_schedule_callback.copy()\n",
    "lr_schedule_by_epoch_callback.by_epoch = True\n",
    "lr_schedule_by_epoch_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_by_epoch_callback,\n",
    "    log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:07,685 86878:140704676111936][base.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:07,686 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-26 19:05:07,690 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=19.021 weight=0.271 batch={'x': tensor([ 2, 20]), 'y': tensor([ 4, 40])} lr=['1.667e-03']\n",
      "[2023-07-26 19:05:07,693 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=29.605 weight=0.556 batch={'x': tensor([37,  4]), 'y': tensor([74,  8])} lr=['1.667e-03']\n",
      "[2023-07-26 19:05:07,696 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=23.924 weight=0.741 batch={'x': tensor([10, 28]), 'y': tensor([20, 56])} lr=['1.667e-03']\n",
      "[2023-07-26 19:05:07,700 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=43.091 weight=0.962 batch={'x': tensor([66, 17]), 'y': tensor([132,  34])} lr=['1.667e-03']\n",
      "[2023-07-26 19:05:07,703 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=33.183 weight=1.246 batch={'x': tensor([59, 29]), 'y': tensor([118,  58])} lr=['1.667e-03']\n",
      "[2023-07-26 19:05:07,706 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=12.722 weight=1.626 batch={'x': tensor([44, 24]), 'y': tensor([88, 48])} lr=['1.667e-03']\n",
      "[2023-07-26 19:05:07,708 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-26 19:05:07,710 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=1.920 weight=1.940 batch={'x': tensor([61,  3]), 'y': tensor([122,   6])} lr=['2.000e-03']\n",
      "[2023-07-26 19:05:07,713 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=0.250 weight=2.010 batch={'x': tensor([35, 15]), 'y': tensor([70, 30])} lr=['2.000e-03']\n",
      "[2023-07-26 19:05:07,717 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=0.450 weight=1.982 batch={'x': tensor([24, 26]), 'y': tensor([48, 52])} lr=['2.000e-03']\n",
      "[2023-07-26 19:05:07,720 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=1.386 weight=2.063 batch={'x': tensor([ 6, 38]), 'y': tensor([12, 76])} lr=['2.000e-03']\n",
      "[2023-07-26 19:05:07,723 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=2.105 weight=2.061 batch={'x': tensor([ 4, 65]), 'y': tensor([  8, 130])} lr=['2.000e-03']\n",
      "[2023-07-26 19:05:07,726 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=1.763 weight=2.075 batch={'x': tensor([ 2, 45]), 'y': tensor([ 4, 90])} lr=['2.000e-03']\n",
      "[2023-07-26 19:05:07,729 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=1.833 weight=2.047 batch={'x': tensor([20, 58]), 'y': tensor([ 40, 116])} lr=['2.000e-03']\n",
      "[2023-07-26 19:05:07,732 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-26 19:05:07,735 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=1.045 weight=2.043 batch={'x': tensor([18, 31]), 'y': tensor([36, 62])} lr=['2.333e-03']\n",
      "[2023-07-26 19:05:07,740 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=3.623 weight=2.082 batch={'x': tensor([41, 47]), 'y': tensor([82, 94])} lr=['2.333e-03']\n",
      "[2023-07-26 19:05:07,743 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=0.823 weight=1.960 batch={'x': tensor([11, 30]), 'y': tensor([22, 60])} lr=['2.333e-03']\n",
      "[2023-07-26 19:05:07,746 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=4.012 weight=2.088 batch={'x': tensor([32, 59]), 'y': tensor([ 64, 118])} lr=['2.333e-03']\n",
      "[2023-07-26 19:05:07,750 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=0.360 weight=1.984 batch={'x': tensor([21, 25]), 'y': tensor([42, 50])} lr=['2.333e-03']\n",
      "[2023-07-26 19:05:07,755 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=0.490 weight=2.010 batch={'x': tensor([42, 56]), 'y': tensor([ 84, 112])} lr=['2.333e-03']\n",
      "[2023-07-26 19:05:07,759 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=0.023 weight=2.001 batch={'x': tensor([ 8, 61]), 'y': tensor([ 16, 122])} lr=['2.333e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_by_epoch_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scaler_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_scaler_callback = todd.Config(\n",
    "    type='LRScaleCallback',\n",
    "    lr_scaler=dict(base_batch_size=1),\n",
    ")\n",
    "lr_scaler_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_scaler_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-26 19:05:07,780 86878:140704676111936][lr.py:92 todd.CustomIterBasedTrainer.custom_iter_based_trainer _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2023-07-26 19:05:07,782 86878:140704676111936][base.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:07,786 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=31.785 weight=1.185 batch={'x': tensor([43, 35]), 'y': tensor([86, 70])}\n",
      "[2023-07-26 19:05:07,789 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=8.140 weight=2.185 batch={'x': tensor([61, 27]), 'y': tensor([122,  54])}\n",
      "[2023-07-26 19:05:07,792 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=2.450 weight=2.140 batch={'x': tensor([33,  2]), 'y': tensor([66,  4])}\n",
      "[2023-07-26 19:05:07,794 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=16.000 weight=1.600 batch={'x': tensor([18, 62]), 'y': tensor([ 36, 124])}\n",
      "[2023-07-26 19:05:07,797 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=8.293 weight=1.845 batch={'x': tensor([53, 54]), 'y': tensor([106, 108])}\n",
      "[2023-07-26 19:05:07,800 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=5.885 weight=2.110 batch={'x': tensor([56, 51]), 'y': tensor([112, 102])}\n",
      "[2023-07-26 19:05:07,803 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=6.758 weight=1.735 batch={'x': tensor([38, 13]), 'y': tensor([76, 26])}\n",
      "[2023-07-26 19:05:07,806 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=2.890 weight=1.660 batch={'x': tensor([9, 8]), 'y': tensor([18, 16])}\n",
      "[2023-07-26 19:05:07,808 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=7.140 weight=1.830 batch={'x': tensor([58, 26]), 'y': tensor([116,  52])}\n",
      "[2023-07-26 19:05:07,811 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=1.850 weight=2.050 batch={'x': tensor([19, 55]), 'y': tensor([ 38, 110])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_scaler_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback_demo = iter_based_trainer_demo.copy()\n",
    "checkpoint_callback = todd.Config(type='CheckpointCallback', interval=10)\n",
    "checkpoint_callback_demo.callbacks = [checkpoint_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:07,832 86878:140704676111936][base.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:07,834 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=90.000 weight=0.000 batch={'x': tensor([44, 46]), 'y': tensor([88, 92])}\n",
      "[2023-07-26 19:05:07,837 86878:140704676111936][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp0f8g7ela/custom_iter_based_trainer/checkpoints/iter_10\n",
      "[2023-07-26 19:05:07,839 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=75.000 weight=0.000 batch={'x': tensor([26, 49]), 'y': tensor([52, 98])}\n",
      "[2023-07-26 19:05:07,842 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=64.000 weight=0.000 batch={'x': tensor([ 3, 61]), 'y': tensor([  6, 122])}\n",
      "[2023-07-26 19:05:07,845 86878:140704676111936][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp0f8g7ela/custom_iter_based_trainer/checkpoints/iter_20\n",
      "[2023-07-26 19:05:07,847 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=54.000 weight=0.000 batch={'x': tensor([52,  2]), 'y': tensor([104,   4])}\n",
      "[2023-07-26 19:05:07,850 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=81.000 weight=0.000 batch={'x': tensor([33, 48]), 'y': tensor([66, 96])}\n",
      "[2023-07-26 19:05:07,852 86878:140704676111936][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp0f8g7ela/custom_iter_based_trainer/checkpoints/iter_30\n",
      "[2023-07-26 19:05:07,855 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=42.000 weight=0.000 batch={'x': tensor([22, 20]), 'y': tensor([44, 40])}\n",
      "[2023-07-26 19:05:07,857 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=124.000 weight=0.000 batch={'x': tensor([64, 60]), 'y': tensor([128, 120])}\n",
      "[2023-07-26 19:05:07,859 86878:140704676111936][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp0f8g7ela/custom_iter_based_trainer/checkpoints/iter_40\n",
      "[2023-07-26 19:05:07,862 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=62.000 weight=0.000 batch={'x': tensor([15, 47]), 'y': tensor([30, 94])}\n",
      "[2023-07-26 19:05:07,864 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=61.000 weight=0.000 batch={'x': tensor([ 5, 56]), 'y': tensor([ 10, 112])}\n",
      "[2023-07-26 19:05:07,866 86878:140704676111936][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp0f8g7ela/custom_iter_based_trainer/checkpoints/iter_50\n",
      "[2023-07-26 19:05:07,868 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=88.000 weight=0.000 batch={'x': tensor([31, 57]), 'y': tensor([ 62, 114])}\n",
      "[2023-07-26 19:05:07,870 86878:140704676111936][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp0f8g7ela/custom_iter_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp0f8g7ela\u001b[0m\n",
      "└── \u001b[1;36mcustom_iter_based_trainer\u001b[0m\n",
      "    ├── 2023-07-26T19-05-07_831637-08-00.log\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_20\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_30\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_40\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_50\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[1;36mlatest\u001b[0m\n",
      "            ├── callbacks.pth\n",
      "            ├── meta.pth\n",
      "            ├── model.pth\n",
      "            ├── optim.pth\n",
      "            └── strategy.pth\n",
      "\n",
      "9 directories, 31 files\n",
      "\n",
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 50}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_50 = pathlib.Path(work_dirs) / 'custom_iter_based_trainer' / 'checkpoints' / 'iter_50'\n",
    "    for f in iter_50.glob('*.pth'):\n",
    "        print(f\"{f.name}:\")\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "checkpoint_by_epoch_callback = checkpoint_callback.copy()\n",
    "checkpoint_by_epoch_callback.update(interval=1, by_epoch=True)\n",
    "checkpoint_by_epoch_callback_demo.callbacks = [\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:08,355 86878:140704676111936][base.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:08,356 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-26 19:05:08,360 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=67.000 weight=0.000 batch={'x': tensor([45, 22]), 'y': tensor([90, 44])}\n",
      "[2023-07-26 19:05:08,363 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=45.000 weight=0.000 batch={'x': tensor([ 9, 36]), 'y': tensor([18, 72])}\n",
      "[2023-07-26 19:05:08,367 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=83.000 weight=0.000 batch={'x': tensor([55, 28]), 'y': tensor([110,  56])}\n",
      "[2023-07-26 19:05:08,371 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=39.000 weight=0.000 batch={'x': tensor([18, 21]), 'y': tensor([36, 42])}\n",
      "[2023-07-26 19:05:08,375 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=30.000 weight=0.000 batch={'x': tensor([ 5, 25]), 'y': tensor([10, 50])}\n",
      "[2023-07-26 19:05:08,378 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=58.000 weight=0.000 batch={'x': tensor([48, 10]), 'y': tensor([96, 20])}\n",
      "[2023-07-26 19:05:08,382 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpxfqj6q1q/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-07-26 19:05:08,386 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-26 19:05:08,389 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=22.000 weight=0.000 batch={'x': tensor([17,  5]), 'y': tensor([34, 10])}\n",
      "[2023-07-26 19:05:08,392 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=100.000 weight=0.000 batch={'x': tensor([67, 33]), 'y': tensor([134,  66])}\n",
      "[2023-07-26 19:05:08,394 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=75.000 weight=0.000 batch={'x': tensor([46, 29]), 'y': tensor([92, 58])}\n",
      "[2023-07-26 19:05:08,398 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=63.000 weight=0.000 batch={'x': tensor([25, 38]), 'y': tensor([50, 76])}\n",
      "[2023-07-26 19:05:08,401 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=53.000 weight=0.000 batch={'x': tensor([37, 16]), 'y': tensor([74, 32])}\n",
      "[2023-07-26 19:05:08,405 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=99.000 weight=0.000 batch={'x': tensor([35, 64]), 'y': tensor([ 70, 128])}\n",
      "[2023-07-26 19:05:08,407 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=34.000 weight=0.000 batch={'x': tensor([13, 21]), 'y': tensor([26, 42])}\n",
      "[2023-07-26 19:05:08,410 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpxfqj6q1q/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-07-26 19:05:08,414 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-26 19:05:08,416 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=95.000 weight=0.000 batch={'x': tensor([63, 32]), 'y': tensor([126,  64])}\n",
      "[2023-07-26 19:05:08,419 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=45.000 weight=0.000 batch={'x': tensor([39,  6]), 'y': tensor([78, 12])}\n",
      "[2023-07-26 19:05:08,422 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=39.000 weight=0.000 batch={'x': tensor([23, 16]), 'y': tensor([46, 32])}\n",
      "[2023-07-26 19:05:08,424 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=71.000 weight=0.000 batch={'x': tensor([18, 53]), 'y': tensor([ 36, 106])}\n",
      "[2023-07-26 19:05:08,426 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=74.000 weight=0.000 batch={'x': tensor([36, 38]), 'y': tensor([72, 76])}\n",
      "[2023-07-26 19:05:08,429 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=54.000 weight=0.000 batch={'x': tensor([12, 42]), 'y': tensor([24, 84])}\n",
      "[2023-07-26 19:05:08,431 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=52.000 weight=0.000 batch={'x': tensor([27, 25]), 'y': tensor([54, 50])}\n",
      "[2023-07-26 19:05:08,433 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpxfqj6q1q/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-07-26 19:05:08,435 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpxfqj6q1q/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpxfqj6q1q\u001b[0m\n",
      "└── \u001b[1;36mcustom_epoch_based_trainer\u001b[0m\n",
      "    ├── 2023-07-26T19-05-08_355022-08-00.log\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[1;36mlatest\u001b[0m\n",
      "            ├── callbacks.pth\n",
      "            ├── meta.pth\n",
      "            ├── model.pth\n",
      "            ├── optim.pth\n",
      "            └── strategy.pth\n",
      "\n",
      "7 directories, 21 files\n",
      "\n",
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {}}\n",
      "\n",
      "meta.pth:\n",
      "{'epoch': 2, 'iter_': 68}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_by_epoch_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'custom_epoch_based_trainer' / 'checkpoints' / 'epoch_2'\n",
    "    for f in epoch_2.glob('*.pth'):\n",
    "        print(f\"{f.name}:\")\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_load_from_callback_demo = checkpoint_by_epoch_callback_demo.copy()\n",
    "checkpoint_load_from_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:09,046 86878:140704676111936][base.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:09,048 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-26 19:05:09,053 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=35.100 weight=0.700 batch={'x': tensor([ 1, 53]), 'y': tensor([  2, 106])}\n",
      "[2023-07-26 19:05:09,057 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=19.035 weight=1.595 batch={'x': tensor([59, 35]), 'y': tensor([118,  70])}\n",
      "[2023-07-26 19:05:09,061 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=0.569 weight=1.982 batch={'x': tensor([19, 46]), 'y': tensor([38, 92])}\n",
      "[2023-07-26 19:05:09,066 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=2.708 weight=2.095 batch={'x': tensor([10, 47]), 'y': tensor([20, 94])}\n",
      "[2023-07-26 19:05:09,071 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=3.230 weight=2.095 batch={'x': tensor([ 7, 61]), 'y': tensor([ 14, 122])}\n",
      "[2023-07-26 19:05:09,076 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=0.145 weight=1.998 batch={'x': tensor([64, 52]), 'y': tensor([128, 104])}\n",
      "[2023-07-26 19:05:09,079 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp4r7tm9jv/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-07-26 19:05:09,084 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-26 19:05:09,086 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=4.388 weight=2.163 batch={'x': tensor([ 4, 50]), 'y': tensor([  8, 100])}\n",
      "[2023-07-26 19:05:09,091 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=1.523 weight=2.053 batch={'x': tensor([ 3, 55]), 'y': tensor([  6, 110])}\n",
      "[2023-07-26 19:05:09,096 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=5.569 weight=1.863 batch={'x': tensor([57, 24]), 'y': tensor([114,  48])}\n",
      "[2023-07-26 19:05:09,101 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=7.787 weight=1.825 batch={'x': tensor([51, 38]), 'y': tensor([102,  76])}\n",
      "[2023-07-26 19:05:09,106 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=1.402 weight=1.958 batch={'x': tensor([48, 18]), 'y': tensor([96, 36])}\n",
      "[2023-07-26 19:05:09,109 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=3.720 weight=1.845 batch={'x': tensor([ 5, 43]), 'y': tensor([10, 86])}\n",
      "[2023-07-26 19:05:09,113 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=6.338 weight=2.163 batch={'x': tensor([45, 33]), 'y': tensor([90, 66])}\n",
      "[2023-07-26 19:05:09,117 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp4r7tm9jv/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-07-26 19:05:09,120 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-26 19:05:09,123 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=0.090 weight=2.010 batch={'x': tensor([15,  3]), 'y': tensor([30,  6])}\n",
      "[2023-07-26 19:05:09,126 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=2.589 weight=1.953 batch={'x': tensor([53, 56]), 'y': tensor([106, 112])}\n",
      "[2023-07-26 19:05:09,130 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=8.113 weight=2.148 batch={'x': tensor([58, 52]), 'y': tensor([116, 104])}\n",
      "[2023-07-26 19:05:09,135 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=4.594 weight=2.088 batch={'x': tensor([54, 51]), 'y': tensor([108, 102])}\n",
      "[2023-07-26 19:05:09,139 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=1.251 weight=1.773 batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])}\n",
      "[2023-07-26 19:05:09,142 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=4.604 weight=1.928 batch={'x': tensor([63, 64]), 'y': tensor([126, 128])}\n",
      "[2023-07-26 19:05:09,145 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=0.720 weight=2.015 batch={'x': tensor([39, 57]), 'y': tensor([ 78, 114])}\n",
      "[2023-07-26 19:05:09,148 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp4r7tm9jv/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-07-26 19:05:09,152 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp4r7tm9jv/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-26 19:05:09,565 86878:140704676111936][checkpoint.py:44 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer connect] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp4r7tm9jv/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "\u001b[2m[2023-07-26 19:05:09,571 86878:140704676111936][base.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:09,572 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-26 19:05:09,576 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=3.847 weight=1.865 batch={'x': tensor([15, 42]), 'y': tensor([30, 84])}\n",
      "[2023-07-26 19:05:09,579 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=5.580 weight=2.155 batch={'x': tensor([63,  9]), 'y': tensor([126,  18])}\n",
      "[2023-07-26 19:05:09,586 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=0.543 weight=2.018 batch={'x': tensor([38, 24]), 'y': tensor([76, 48])}\n",
      "[2023-07-26 19:05:09,589 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=4.306 weight=1.838 batch={'x': tensor([34, 19]), 'y': tensor([68, 38])}\n",
      "[2023-07-26 19:05:09,594 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=7.995 weight=2.195 batch={'x': tensor([45, 37]), 'y': tensor([90, 74])}\n",
      "[2023-07-26 19:05:09,600 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=3.420 weight=1.940 batch={'x': tensor([64, 50]), 'y': tensor([128, 100])}\n",
      "[2023-07-26 19:05:09,604 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=8.508 weight=2.208 batch={'x': tensor([39, 43]), 'y': tensor([78, 86])}\n",
      "[2023-07-26 19:05:09,606 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp4r7tm9jv/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-07-26 19:05:09,609 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp4r7tm9jv/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_load_from_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_load_from_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "        load_from=os.path.join(work_dirs, 'custom_epoch_based_trainer', 'checkpoints', 'epoch_2')\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaultyRunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError('faulty runner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyValidator(FaultyRunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyIterBasedTrainer(FaultyRunnerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyEpochBasedTrainer(\n",
    "    FaultyRunnerMixin,\n",
    "    todd.runners.EpochBasedTrainer,\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_callback_demo = validator_demo.copy()\n",
    "monitor_callback_demo.type = 'FaultyValidator'\n",
    "monitor_callback = todd.Config(type='MonitorCallback')\n",
    "monitor_callback_demo.callbacks = [monitor_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:09,677 86878:140704676111936][base.py:52 todd.FaultyValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2023-07-26 19:05:09,679 86878:140704676111936][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x154bc7390>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 192, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_86878/2137902126.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2023-07-26 19:05:09,677 86878:140704676111936][base.py:52 todd.FaultyValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-07-26 19:05:09,679 86878:140704676111936][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x154bc7390>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 192, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_86878/2137902126.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        monitor_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_callback_demo = iter_based_trainer_demo.copy()\n",
    "eta_callback = todd.Config(type='ETACallback')\n",
    "eta_callback_demo.callbacks = [eta_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:09,975 86878:140704676111936][base.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:09,979 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=59.000 weight=0.000 batch={'x': tensor([ 7, 52]), 'y': tensor([ 14, 104])} eta=0:00:00.025862\n",
      "[2023-07-26 19:05:09,983 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=92.000 weight=0.000 batch={'x': tensor([30, 62]), 'y': tensor([ 60, 124])} eta=0:00:00.026458\n",
      "[2023-07-26 19:05:09,985 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=59.000 weight=0.000 batch={'x': tensor([ 8, 51]), 'y': tensor([ 16, 102])} eta=0:00:00.021797\n",
      "[2023-07-26 19:05:09,988 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=30.000 weight=0.000 batch={'x': tensor([29,  1]), 'y': tensor([58,  2])} eta=0:00:00.018030\n",
      "[2023-07-26 19:05:09,990 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=107.000 weight=0.000 batch={'x': tensor([64, 43]), 'y': tensor([128,  86])} eta=0:00:00.015402\n",
      "[2023-07-26 19:05:09,993 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=74.000 weight=0.000 batch={'x': tensor([14, 60]), 'y': tensor([ 28, 120])} eta=0:00:00.012209\n",
      "[2023-07-26 19:05:09,995 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=94.000 weight=0.000 batch={'x': tensor([39, 55]), 'y': tensor([ 78, 110])} eta=0:00:00.009283\n",
      "[2023-07-26 19:05:09,997 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=24.000 weight=0.000 batch={'x': tensor([11, 13]), 'y': tensor([22, 26])} eta=0:00:00.006504\n",
      "[2023-07-26 19:05:09,999 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=92.000 weight=0.000 batch={'x': tensor([50, 42]), 'y': tensor([100,  84])} eta=0:00:00.004012\n",
      "[2023-07-26 19:05:10,001 86878:140704676111936][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=59.000 weight=0.000 batch={'x': tensor([33, 26]), 'y': tensor([66, 52])} eta=0:00:00.001462\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        eta_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_load_model_from_demo = checkpoint_load_from_callback_demo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:10,021 86878:140704676111936][base.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:10,022 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-26 19:05:10,025 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=74.160 weight=0.560 batch={'x': tensor([54, 49]), 'y': tensor([108,  98])}\n",
      "[2023-07-26 19:05:10,028 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=13.413 weight=1.537 batch={'x': tensor([36, 22]), 'y': tensor([72, 44])}\n",
      "[2023-07-26 19:05:10,033 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=5.606 weight=1.837 batch={'x': tensor([45, 24]), 'y': tensor([90, 48])}\n",
      "[2023-07-26 19:05:10,036 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=4.380 weight=1.880 batch={'x': tensor([59, 14]), 'y': tensor([118,  28])}\n",
      "[2023-07-26 19:05:10,038 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=1.937 weight=2.125 batch={'x': tensor([ 5, 26]), 'y': tensor([10, 52])}\n",
      "[2023-07-26 19:05:10,041 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=1.100 weight=2.040 batch={'x': tensor([ 4, 51]), 'y': tensor([  8, 102])}\n",
      "[2023-07-26 19:05:10,044 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpl4yrcj4_/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-07-26 19:05:10,046 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-26 19:05:10,049 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=7.624 weight=1.732 batch={'x': tensor([ 9, 48]), 'y': tensor([18, 96])}\n",
      "[2023-07-26 19:05:10,052 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=10.686 weight=2.257 batch={'x': tensor([23, 60]), 'y': tensor([ 46, 120])}\n",
      "[2023-07-26 19:05:10,055 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=1.600 weight=1.950 batch={'x': tensor([24, 40]), 'y': tensor([48, 80])}\n",
      "[2023-07-26 19:05:10,058 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=2.700 weight=2.112 batch={'x': tensor([32, 16]), 'y': tensor([64, 32])}\n",
      "[2023-07-26 19:05:10,061 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=1.472 weight=2.047 batch={'x': tensor([51, 11]), 'y': tensor([102,  22])}\n",
      "[2023-07-26 19:05:10,065 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=1.938 weight=2.078 batch={'x': tensor([44,  6]), 'y': tensor([88, 12])}\n",
      "[2023-07-26 19:05:10,075 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=3.150 weight=1.900 batch={'x': tensor([ 4, 59]), 'y': tensor([  8, 118])}\n",
      "[2023-07-26 19:05:10,080 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpl4yrcj4_/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-07-26 19:05:10,085 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-26 19:05:10,090 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=1.350 weight=1.970 batch={'x': tensor([65, 25]), 'y': tensor([130,  50])}\n",
      "[2023-07-26 19:05:10,097 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=0.908 weight=2.083 batch={'x': tensor([17,  5]), 'y': tensor([34, 10])}\n",
      "[2023-07-26 19:05:10,101 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=5.950 weight=1.788 batch={'x': tensor([32, 24]), 'y': tensor([64, 48])}\n",
      "[2023-07-26 19:05:10,105 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=3.870 weight=1.910 batch={'x': tensor([46, 40]), 'y': tensor([92, 80])}\n",
      "[2023-07-26 19:05:10,109 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=6.640 weight=1.840 batch={'x': tensor([61, 22]), 'y': tensor([122,  44])}\n",
      "[2023-07-26 19:05:10,112 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=0.788 weight=2.088 batch={'x': tensor([10,  8]), 'y': tensor([20, 16])}\n",
      "[2023-07-26 19:05:10,118 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=0.551 weight=2.018 batch={'x': tensor([21, 42]), 'y': tensor([42, 84])}\n",
      "[2023-07-26 19:05:10,121 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpl4yrcj4_/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-07-26 19:05:10,124 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpl4yrcj4_/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-26 19:05:10,559 86878:140704676111936][base.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-26 19:05:10,571 86878:140704676111936][base.py:60 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpl4yrcj4_/custom_epoch_based_trainer/checkpoints/epoch_2/model.pth\n",
      "[2023-07-26 19:05:10,577 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-26 19:05:10,586 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=1.508 weight=2.045 batch={'x': tensor([22, 45]), 'y': tensor([44, 90])}\n",
      "[2023-07-26 19:05:10,599 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=4.046 weight=1.903 batch={'x': tensor([52, 31]), 'y': tensor([104,  62])}\n",
      "[2023-07-26 19:05:10,616 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=6.050 weight=2.138 batch={'x': tensor([60, 28]), 'y': tensor([120,  56])}\n",
      "[2023-07-26 19:05:10,624 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=2.286 weight=2.148 batch={'x': tensor([21, 10]), 'y': tensor([42, 20])}\n",
      "[2023-07-26 19:05:10,630 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=8.215 weight=1.845 batch={'x': tensor([43, 63]), 'y': tensor([ 86, 126])}\n",
      "[2023-07-26 19:05:10,646 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=0.337 weight=1.985 batch={'x': tensor([41,  4]), 'y': tensor([82,  8])}\n",
      "[2023-07-26 19:05:10,657 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpl4yrcj4_/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-07-26 19:05:10,663 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-26 19:05:10,670 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=2.020 weight=2.040 batch={'x': tensor([50, 51]), 'y': tensor([100, 102])}\n",
      "[2023-07-26 19:05:10,674 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=4.970 weight=1.860 batch={'x': tensor([ 5, 66]), 'y': tensor([ 10, 132])}\n",
      "[2023-07-26 19:05:10,678 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=9.157 weight=1.835 batch={'x': tensor([53, 58]), 'y': tensor([106, 116])}\n",
      "[2023-07-26 19:05:10,682 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=9.488 weight=2.173 batch={'x': tensor([43, 67]), 'y': tensor([ 86, 134])}\n",
      "[2023-07-26 19:05:10,686 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=5.460 weight=2.120 batch={'x': tensor([46, 45]), 'y': tensor([92, 90])}\n",
      "[2023-07-26 19:05:10,690 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=1.383 weight=2.198 batch={'x': tensor([8, 6]), 'y': tensor([16, 12])}\n",
      "[2023-07-26 19:05:10,693 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=4.675 weight=1.890 batch={'x': tensor([41, 44]), 'y': tensor([82, 88])}\n",
      "[2023-07-26 19:05:10,696 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpl4yrcj4_/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-07-26 19:05:10,700 86878:140704676111936][log.py:54 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-26 19:05:10,703 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=6.960 weight=2.145 batch={'x': tensor([32, 64]), 'y': tensor([ 64, 128])}\n",
      "[2023-07-26 19:05:10,707 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=4.054 weight=2.173 batch={'x': tensor([45,  2]), 'y': tensor([90,  4])}\n",
      "[2023-07-26 19:05:10,710 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=2.400 weight=2.050 batch={'x': tensor([33, 63]), 'y': tensor([ 66, 126])}\n",
      "[2023-07-26 19:05:10,714 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=3.770 weight=2.130 batch={'x': tensor([42, 16]), 'y': tensor([84, 32])}\n",
      "[2023-07-26 19:05:10,719 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=3.060 weight=1.940 batch={'x': tensor([62, 40]), 'y': tensor([124,  80])}\n",
      "[2023-07-26 19:05:10,722 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=3.145 weight=2.085 batch={'x': tensor([61, 13]), 'y': tensor([122,  26])}\n",
      "[2023-07-26 19:05:10,726 86878:140704676111936][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=1.567 weight=1.905 batch={'x': tensor([24,  9]), 'y': tensor([48, 18])}\n",
      "[2023-07-26 19:05:10,728 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpl4yrcj4_/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-07-26 19:05:10,732 86878:140704676111936][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpl4yrcj4_/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        strategy_load_model_from_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        strategy_load_model_from_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.load_model_from(os.path.join(work_dirs, 'custom_epoch_based_trainer', 'checkpoints', 'epoch_2', 'model.pth'))\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
