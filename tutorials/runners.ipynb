{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing /Users/bytedance/Developer/todd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.6.1)\n",
      "Requirement already satisfied: lmdb in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (1.4.1)\n",
      "Requirement already satisfied: opencv-python in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.7.0.72)\n",
      "Requirement already satisfied: pandas in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: python-pptx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.6.21)\n",
      "Requirement already satisfied: timm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.9.2)\n",
      "Requirement already satisfied: toml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.5.0)\n",
      "Requirement already satisfied: yapf in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from opencv-python->todd-ai==0.4.0) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2023.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (4.9.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (9.5.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.14.1)\n",
      "Requirement already satisfied: safetensors in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.3.1)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from yapf->todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->todd-ai==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.12.0)\n",
      "Requirement already satisfied: sympy in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (2023.5.0)\n",
      "Requirement already satisfied: requests in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (2.30.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (23.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from jinja2->torch>=1.7->timm->todd-ai==0.4.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from sympy->torch>=1.7->timm->todd-ai==0.4.0) (1.3.0)\n",
      "Building wheels for collected packages: todd-ai\n",
      "  Building wheel for todd-ai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for todd-ai: filename=todd_ai-0.4.0-py3-none-any.whl size=106529 sha256=36400bf999963bc8d235c5aac6befc5818d4cc6a096686754d6cd8b11016a0b7\n",
      "  Stored in directory: /private/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/pip-ephem-wheel-cache-a__q8cpm/wheels/15/ef/5a/9fc12e257ce5cef16b333a2ed6c992ff9cbcc9167f7199e6ac\n",
      "Successfully built todd-ai\n",
      "Installing collected packages: todd-ai\n",
      "Successfully installed todd-ai-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "[2023-08-02 23:27:34,023 74460:140704362395200][patches.py:14 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tempfile\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict, cast\n",
    "\n",
    "import todd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "Memo = dict[str, Any]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register()\n",
    "class RunnerModel(todd.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> Memo:\n",
    "        y: torch.Tensor = self._strategy.model(batch['x'])\n",
    "        loss = F.l1_loss(y, batch['y'])\n",
    "        memo['loss'] = loss\n",
    "        if 'log' in memo:\n",
    "            memo['log']['loss'] = f'{loss.item():.3f}'\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerMixin(RunnerMixin):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> Memo:\n",
    "        memo = super()._run_iter(batch, memo)\n",
    "        if 'log' in memo:\n",
    "            model = cast(RunnerModel, self._strategy.module)\n",
    "            memo['log']['weight'] = f'{model.weight.item():.3f}'\n",
    "            memo['log']['batch'] = str(batch)\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomValidator(RunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomIterBasedTrainer(TrainerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomEpochBasedTrainer(TrainerMixin, todd.runners.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_demo = todd.Config(\n",
    "    type='CustomValidator',\n",
    "    name='custom_validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 23:27:34,856 74460:140704362395200][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpu456jlu7\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_demo.callbacks=dict(type='LogCallback', interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 23:27:35,219 74460:140704362395200][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:35,227 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.010833 loss=10.000\n",
      "[2023-08-02 23:27:35,231 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.007368 loss=20.000\n",
      "[2023-08-02 23:27:35,233 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.003283 loss=30.000\n",
      "[2023-08-02 23:27:35,235 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpj5tb9eze\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_demo = validator_demo.copy()\n",
    "trainer_demo.pop('type')\n",
    "trainer_demo.dataloader = todd.Config(\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    dataset=dict(type='RunnerDataset', n=67),\n",
    ")\n",
    "trainer_demo.optimizer = todd.Config(type='SGD', lr=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_based_trainer_demo = trainer_demo.copy()\n",
    "iter_based_trainer_demo.type = 'CustomIterBasedTrainer'\n",
    "iter_based_trainer_demo.name = 'custom_iter_based_trainer'\n",
    "iter_based_trainer_demo.iters = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 23:27:35,545 74460:140704362395200][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:35,550 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.030058 loss=50.000 weight=0.000 batch={'x': tensor([24, 26]), 'y': tensor([48, 52])}\n",
      "[2023-08-02 23:27:35,553 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.024691 loss=70.000 weight=0.000 batch={'x': tensor([64,  6]), 'y': tensor([128,  12])}\n",
      "[2023-08-02 23:27:35,556 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.021822 loss=29.000 weight=0.000 batch={'x': tensor([15, 14]), 'y': tensor([30, 28])}\n",
      "[2023-08-02 23:27:35,558 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.018261 loss=104.000 weight=0.000 batch={'x': tensor([37, 67]), 'y': tensor([ 74, 134])}\n",
      "[2023-08-02 23:27:35,561 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.015941 loss=51.000 weight=0.000 batch={'x': tensor([ 8, 43]), 'y': tensor([16, 86])}\n",
      "[2023-08-02 23:27:35,564 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.012889 loss=67.000 weight=0.000 batch={'x': tensor([12, 55]), 'y': tensor([ 24, 110])}\n",
      "[2023-08-02 23:27:35,566 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.009940 loss=9.000 weight=0.000 batch={'x': tensor([5, 4]), 'y': tensor([10,  8])}\n",
      "[2023-08-02 23:27:35,568 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.006928 loss=44.000 weight=0.000 batch={'x': tensor([16, 28]), 'y': tensor([32, 56])}\n",
      "[2023-08-02 23:27:35,570 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.004172 loss=20.000 weight=0.000 batch={'x': tensor([ 9, 11]), 'y': tensor([18, 22])}\n",
      "[2023-08-02 23:27:35,573 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.001538 loss=97.000 weight=0.000 batch={'x': tensor([63, 34]), 'y': tensor([126,  68])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        iter_based_trainer_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_based_trainer_demo = trainer_demo.copy()\n",
    "epoch_based_trainer_demo.type = 'CustomEpochBasedTrainer'\n",
    "epoch_based_trainer_demo.name = 'custom_epoch_based_trainer'\n",
    "epoch_based_trainer_demo.epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 23:27:35,594 74460:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:35,596 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 23:27:35,599 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.067628 loss=68.000 weight=0.000 batch={'x': tensor([58, 10]), 'y': tensor([116,  20])}\n",
      "[2023-08-02 23:27:35,602 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.056801 loss=62.000 weight=0.000 batch={'x': tensor([41, 21]), 'y': tensor([82, 42])}\n",
      "[2023-08-02 23:27:35,604 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.049097 loss=69.000 weight=0.000 batch={'x': tensor([39, 30]), 'y': tensor([78, 60])}\n",
      "[2023-08-02 23:27:35,607 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.045293 loss=88.000 weight=0.000 batch={'x': tensor([38, 50]), 'y': tensor([ 76, 100])}\n",
      "[2023-08-02 23:27:35,610 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.043215 loss=71.000 weight=0.000 batch={'x': tensor([48, 23]), 'y': tensor([96, 46])}\n",
      "[2023-08-02 23:27:35,613 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.040318 loss=57.000 weight=0.000 batch={'x': tensor([ 4, 53]), 'y': tensor([  8, 106])}\n",
      "[2023-08-02 23:27:35,616 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 23:27:35,618 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.042107 loss=37.000 weight=0.000 batch={'x': tensor([ 5, 32]), 'y': tensor([10, 64])}\n",
      "[2023-08-02 23:27:35,620 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.037972 loss=52.000 weight=0.000 batch={'x': tensor([ 4, 48]), 'y': tensor([ 8, 96])}\n",
      "[2023-08-02 23:27:35,622 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.033807 loss=110.000 weight=0.000 batch={'x': tensor([52, 58]), 'y': tensor([104, 116])}\n",
      "[2023-08-02 23:27:35,625 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.030293 loss=41.000 weight=0.000 batch={'x': tensor([20, 21]), 'y': tensor([40, 42])}\n",
      "[2023-08-02 23:27:35,627 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.027055 loss=76.000 weight=0.000 batch={'x': tensor([37, 39]), 'y': tensor([74, 78])}\n",
      "[2023-08-02 23:27:35,630 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.023758 loss=71.000 weight=0.000 batch={'x': tensor([36, 35]), 'y': tensor([72, 70])}\n",
      "[2023-08-02 23:27:35,632 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.020793 loss=99.000 weight=0.000 batch={'x': tensor([53, 46]), 'y': tensor([106,  92])}\n",
      "[2023-08-02 23:27:35,634 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 23:27:35,636 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.018421 loss=61.000 weight=0.000 batch={'x': tensor([55,  6]), 'y': tensor([110,  12])}\n",
      "[2023-08-02 23:27:35,638 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.015296 loss=81.000 weight=0.000 batch={'x': tensor([67, 14]), 'y': tensor([134,  28])}\n",
      "[2023-08-02 23:27:35,641 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.012306 loss=36.000 weight=0.000 batch={'x': tensor([12, 24]), 'y': tensor([24, 48])}\n",
      "[2023-08-02 23:27:35,643 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.009460 loss=114.000 weight=0.000 batch={'x': tensor([50, 64]), 'y': tensor([100, 128])}\n",
      "[2023-08-02 23:27:35,645 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.006609 loss=108.000 weight=0.000 batch={'x': tensor([49, 59]), 'y': tensor([ 98, 118])}\n",
      "[2023-08-02 23:27:35,648 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.003843 loss=73.000 weight=0.000 batch={'x': tensor([20, 53]), 'y': tensor([ 40, 106])}\n",
      "[2023-08-02 23:27:35,650 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001088 loss=49.000 weight=0.000 batch={'x': tensor([23, 26]), 'y': tensor([46, 52])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        epoch_based_trainer_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_callback_demo = validator_demo.copy()\n",
    "log_callback = log_callback_demo.callbacks\n",
    "log_callback.collect_env = todd.Config(verbose=False)\n",
    "log_callback.with_file_handler = True\n",
    "log_callback_demo.callbacks = [log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:35,739 74460:140704362395200][log.py:48 todd.CustomValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:35,742 74460:140704362395200][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:35,746 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.006705 loss=10.000\n",
      "[2023-08-02 23:27:35,748 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.004505 loss=20.000\n",
      "[2023-08-02 23:27:35,751 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.002364 loss=30.000\n",
      "[2023-08-02 23:27:35,753 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9l81by23\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "    └── 2023-08-02T23-27-35_674616-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2023-08-02 23:27:35,739 74460:140704362395200][log.py:48 todd.CustomValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "[2023-08-02 23:27:35,742 74460:140704362395200][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-08-02 23:27:35,746 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.006705 loss=10.000\n",
      "[2023-08-02 23:27:35,748 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.004505 loss=20.000\n",
      "[2023-08-02 23:27:35,751 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.002364 loss=30.000\n",
      "[2023-08-02 23:27:35,753 74460:140704362395200][log.py:81 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        log_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_callback_demo = iter_based_trainer_demo.copy()\n",
    "optimize_callback = todd.Config(type='OptimizeCallback')\n",
    "optimize_callback_demo.callbacks = [optimize_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:36,434 74460:140704362395200][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:36,437 74460:140704362395200][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:36,446 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.068026 loss=23.760 weight=0.515 batch={'x': tensor([11, 21]), 'y': tensor([22, 42])}\n",
      "[2023-08-02 23:27:36,452 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.053625 loss=40.327 weight=1.293 batch={'x': tensor([49, 65]), 'y': tensor([ 98, 130])}\n",
      "[2023-08-02 23:27:36,456 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.042438 loss=1.046 weight=1.767 batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])}\n",
      "[2023-08-02 23:27:36,460 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.034530 loss=6.743 weight=1.845 batch={'x': tensor([23, 64]), 'y': tensor([ 46, 128])}\n",
      "[2023-08-02 23:27:36,466 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.030071 loss=0.000 weight=2.000 batch={'x': tensor([42, 45]), 'y': tensor([84, 90])}\n",
      "[2023-08-02 23:27:36,471 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.024286 loss=0.000 weight=2.000 batch={'x': tensor([67, 38]), 'y': tensor([134,  76])}\n",
      "[2023-08-02 23:27:36,475 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.018474 loss=0.000 weight=2.000 batch={'x': tensor([43, 26]), 'y': tensor([86, 52])}\n",
      "[2023-08-02 23:27:36,479 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.012988 loss=0.000 weight=2.000 batch={'x': tensor([30, 37]), 'y': tensor([60, 74])}\n",
      "[2023-08-02 23:27:36,484 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.007877 loss=0.000 weight=2.000 batch={'x': tensor([35, 25]), 'y': tensor([70, 50])}\n",
      "[2023-08-02 23:27:36,487 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.002859 loss=0.000 weight=2.000 batch={'x': tensor([53, 28]), 'y': tensor([106,  56])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        optimize_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_schedule_callback = todd.Config(\n",
    "    type='LRScheduleCallback',\n",
    "    lr_scheduler=dict(type='LinearLR', total_iters=10),\n",
    ")\n",
    "lr_schedule_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_callback,\n",
    "    log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:36,587 74460:140704362395200][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:36,591 74460:140704362395200][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:36,602 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.082128 loss=40.269 weight=0.249 batch={'x': tensor([45,  1]), 'y': tensor([90,  2])} lr=['3.333e-03']\n",
      "[2023-08-02 23:27:36,612 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.077095 loss=61.354 weight=0.969 batch={'x': tensor([52, 67]), 'y': tensor([104, 134])} lr=['5.000e-03']\n",
      "[2023-08-02 23:27:36,620 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.066586 loss=3.929 weight=1.746 batch={'x': tensor([ 2, 29]), 'y': tensor([ 4, 58])} lr=['5.000e-03']\n",
      "[2023-08-02 23:27:36,624 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.049975 loss=0.561 weight=2.016 batch={'x': tensor([64,  4]), 'y': tensor([128,   8])} lr=['5.000e-03']\n",
      "[2023-08-02 23:27:36,629 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.039717 loss=1.892 weight=1.951 batch={'x': tensor([34, 44]), 'y': tensor([68, 88])} lr=['5.000e-03']\n",
      "[2023-08-02 23:27:36,634 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.031185 loss=7.355 weight=2.201 batch={'x': tensor([48, 25]), 'y': tensor([96, 50])} lr=['5.000e-03']\n",
      "[2023-08-02 23:27:36,639 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.023316 loss=0.315 weight=1.979 batch={'x': tensor([22,  8]), 'y': tensor([44, 16])} lr=['5.000e-03']\n",
      "[2023-08-02 23:27:36,643 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.016114 loss=3.168 weight=2.099 batch={'x': tensor([49, 15]), 'y': tensor([98, 30])} lr=['5.000e-03']\n",
      "[2023-08-02 23:27:36,648 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.009580 loss=1.241 weight=2.037 batch={'x': tensor([57, 11]), 'y': tensor([114,  22])} lr=['5.000e-03']\n",
      "[2023-08-02 23:27:36,653 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.003564 loss=0.174 weight=1.994 batch={'x': tensor([ 3, 55]), 'y': tensor([  6, 110])} lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "lr_schedule_by_epoch_callback = lr_schedule_callback.copy()\n",
    "lr_schedule_by_epoch_callback.by_epoch = True\n",
    "lr_schedule_by_epoch_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_by_epoch_callback,\n",
    "    log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:36,750 74460:140704362395200][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:36,753 74460:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:36,755 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 23:27:36,762 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.141368 loss=60.777 weight=0.238 batch={'x': tensor([60,  9]), 'y': tensor([120,  18])} lr=['1.667e-03']\n",
      "[2023-08-02 23:27:36,768 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.122388 loss=61.965 weight=0.470 batch={'x': tensor([52, 29]), 'y': tensor([104,  58])} lr=['1.667e-03']\n",
      "[2023-08-02 23:27:36,775 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.115258 loss=32.840 weight=0.806 batch={'x': tensor([19, 36]), 'y': tensor([38, 72])} lr=['1.667e-03']\n",
      "[2023-08-02 23:27:36,782 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.110913 loss=9.301 weight=1.114 batch={'x': tensor([17,  4]), 'y': tensor([34,  8])} lr=['1.667e-03']\n",
      "[2023-08-02 23:27:36,789 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.104320 loss=30.112 weight=1.323 batch={'x': tensor([55, 34]), 'y': tensor([110,  68])} lr=['1.667e-03']\n",
      "[2023-08-02 23:27:36,793 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.091651 loss=17.442 weight=1.617 batch={'x': tensor([53, 38]), 'y': tensor([106,  76])} lr=['1.667e-03']\n",
      "[2023-08-02 23:27:36,799 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 23:27:36,805 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.096536 loss=2.707 weight=1.939 batch={'x': tensor([25, 64]), 'y': tensor([ 50, 128])} lr=['2.000e-03']\n",
      "[2023-08-02 23:27:36,812 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.089433 loss=2.173 weight=1.951 batch={'x': tensor([50, 39]), 'y': tensor([100,  78])} lr=['2.000e-03']\n",
      "[2023-08-02 23:27:36,820 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.082739 loss=2.062 weight=2.098 batch={'x': tensor([22, 20]), 'y': tensor([44, 40])} lr=['2.000e-03']\n",
      "[2023-08-02 23:27:36,827 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.074988 loss=0.255 weight=1.996 batch={'x': tensor([66, 67]), 'y': tensor([132, 134])} lr=['2.000e-03']\n",
      "[2023-08-02 23:27:36,833 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.067312 loss=0.374 weight=2.019 batch={'x': tensor([18, 21]), 'y': tensor([36, 42])} lr=['2.000e-03']\n",
      "[2023-08-02 23:27:36,841 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.060130 loss=0.182 weight=1.990 batch={'x': tensor([24, 13]), 'y': tensor([48, 26])} lr=['2.000e-03']\n",
      "[2023-08-02 23:27:36,847 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.052751 loss=1.329 weight=2.068 batch={'x': tensor([ 4, 35]), 'y': tensor([ 8, 70])} lr=['2.000e-03']\n",
      "[2023-08-02 23:27:36,856 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 23:27:36,860 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.047906 loss=0.168 weight=1.994 batch={'x': tensor([41, 12]), 'y': tensor([82, 24])} lr=['2.333e-03']\n",
      "[2023-08-02 23:27:36,866 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.040199 loss=3.262 weight=2.078 batch={'x': tensor([20, 64]), 'y': tensor([ 40, 128])} lr=['2.333e-03']\n",
      "[2023-08-02 23:27:36,871 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.031879 loss=1.021 weight=1.962 batch={'x': tensor([25, 29]), 'y': tensor([50, 58])} lr=['2.333e-03']\n",
      "[2023-08-02 23:27:36,877 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.024461 loss=3.210 weight=2.058 batch={'x': tensor([46, 65]), 'y': tensor([ 92, 130])} lr=['2.333e-03']\n",
      "[2023-08-02 23:27:36,882 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.017023 loss=1.921 weight=1.956 batch={'x': tensor([66, 22]), 'y': tensor([132,  44])} lr=['2.333e-03']\n",
      "[2023-08-02 23:27:36,886 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.009702 loss=0.481 weight=1.949 batch={'x': tensor([17,  2]), 'y': tensor([34,  4])} lr=['2.333e-03']\n",
      "[2023-08-02 23:27:36,891 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.002720 loss=2.818 weight=2.074 batch={'x': tensor([16, 60]), 'y': tensor([ 32, 120])} lr=['2.333e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_by_epoch_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scaler_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_scaler_callback = todd.Config(\n",
    "    type='LRScaleCallback',\n",
    "    lr_scaler=dict(base_batch_size=1),\n",
    ")\n",
    "lr_scaler_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_scaler_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:36,954 74460:140704362395200][lr.py:92 todd.CustomIterBasedTrainer.custom_iter_based_trainer _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "[2023-08-02 23:27:37,076 74460:140704362395200][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:37,079 74460:140704362395200][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:37,090 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.052666 loss=27.405 weight=1.370 batch={'x': tensor([59, 28]), 'y': tensor([118,  56])}\n",
      "[2023-08-02 23:27:37,098 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.057091 loss=12.000 weight=1.760 batch={'x': tensor([61, 39]), 'y': tensor([122,  78])}\n",
      "[2023-08-02 23:27:37,105 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.052313 loss=4.655 weight=1.510 batch={'x': tensor([ 2, 17]), 'y': tensor([ 4, 34])}\n",
      "[2023-08-02 23:27:37,112 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.045403 loss=17.927 weight=2.505 batch={'x': tensor([66,  5]), 'y': tensor([132,  10])}\n",
      "[2023-08-02 23:27:37,118 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.037617 loss=12.350 weight=2.325 batch={'x': tensor([34, 42]), 'y': tensor([68, 84])}\n",
      "[2023-08-02 23:27:37,125 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.031130 loss=2.962 weight=2.075 batch={'x': tensor([12, 67]), 'y': tensor([ 24, 134])}\n",
      "[2023-08-02 23:27:37,132 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.024571 loss=17.395 weight=2.490 batch={'x': tensor([21, 50]), 'y': tensor([ 42, 100])}\n",
      "[2023-08-02 23:27:37,138 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.017489 loss=4.590 weight=2.135 batch={'x': tensor([36, 32]), 'y': tensor([72, 64])}\n",
      "[2023-08-02 23:27:37,145 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.010635 loss=6.560 weight=1.680 batch={'x': tensor([ 4, 37]), 'y': tensor([ 8, 74])}\n",
      "[2023-08-02 23:27:37,152 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.004055 loss=8.040 weight=2.335 batch={'x': tensor([ 3, 45]), 'y': tensor([ 6, 90])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_scaler_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback_demo = iter_based_trainer_demo.copy()\n",
    "checkpoint_callback = todd.Config(type='CheckpointCallback', interval=10)\n",
    "checkpoint_callback_demo.callbacks = [checkpoint_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:37,255 74460:140704362395200][log.py:48 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:37,259 74460:140704362395200][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:37,267 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.054778 loss=65.000 weight=0.000 batch={'x': tensor([45, 20]), 'y': tensor([90, 40])}\n",
      "[2023-08-02 23:27:37,272 74460:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpzmzmo5z7/custom_iter_based_trainer/checkpoints/iter_10\n",
      "[2023-08-02 23:27:37,277 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.066598 loss=74.000 weight=0.000 batch={'x': tensor([17, 57]), 'y': tensor([ 34, 114])}\n",
      "[2023-08-02 23:27:37,282 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.050537 loss=65.000 weight=0.000 batch={'x': tensor([55, 10]), 'y': tensor([110,  20])}\n",
      "[2023-08-02 23:27:37,285 74460:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpzmzmo5z7/custom_iter_based_trainer/checkpoints/iter_20\n",
      "[2023-08-02 23:27:37,288 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.043520 loss=59.000 weight=0.000 batch={'x': tensor([27, 32]), 'y': tensor([54, 64])}\n",
      "[2023-08-02 23:27:37,293 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.034915 loss=66.000 weight=0.000 batch={'x': tensor([24, 42]), 'y': tensor([48, 84])}\n",
      "[2023-08-02 23:27:37,297 74460:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpzmzmo5z7/custom_iter_based_trainer/checkpoints/iter_30\n",
      "[2023-08-02 23:27:37,301 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.030297 loss=71.000 weight=0.000 batch={'x': tensor([58, 13]), 'y': tensor([116,  26])}\n",
      "[2023-08-02 23:27:37,306 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.022735 loss=71.000 weight=0.000 batch={'x': tensor([56, 15]), 'y': tensor([112,  30])}\n",
      "[2023-08-02 23:27:37,309 74460:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpzmzmo5z7/custom_iter_based_trainer/checkpoints/iter_40\n",
      "[2023-08-02 23:27:37,313 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.016562 loss=32.000 weight=0.000 batch={'x': tensor([21, 11]), 'y': tensor([42, 22])}\n",
      "[2023-08-02 23:27:37,318 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.009970 loss=90.000 weight=0.000 batch={'x': tensor([26, 64]), 'y': tensor([ 52, 128])}\n",
      "[2023-08-02 23:27:37,320 74460:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpzmzmo5z7/custom_iter_based_trainer/checkpoints/iter_50\n",
      "[2023-08-02 23:27:37,326 74460:140704362395200][log.py:81 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.003869 loss=44.000 weight=0.000 batch={'x': tensor([14, 30]), 'y': tensor([28, 60])}\n",
      "[2023-08-02 23:27:37,332 74460:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpzmzmo5z7/custom_iter_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpzmzmo5z7\u001b[0m\n",
      "└── \u001b[1;36mcustom_iter_based_trainer\u001b[0m\n",
      "    ├── 2023-08-02T23-27-37_195868-08-00.log\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_20\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_30\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_40\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_50\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[1;36mlatest\u001b[0m\n",
      "            ├── callbacks.pth\n",
      "            ├── meta.pth\n",
      "            ├── model.pth\n",
      "            ├── optim.pth\n",
      "            └── strategy.pth\n",
      "\n",
      "9 directories, 31 files\n",
      "\n",
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 50}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_50 = pathlib.Path(work_dirs) / 'custom_iter_based_trainer' / 'checkpoints' / 'iter_50'\n",
    "    for f in iter_50.glob('*.pth'):\n",
    "        print(f\"{f.name}:\")\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "checkpoint_by_epoch_callback = checkpoint_callback.copy()\n",
    "checkpoint_by_epoch_callback.update(interval=1, by_epoch=True)\n",
    "checkpoint_by_epoch_callback_demo.callbacks = [\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:37,907 74460:140704362395200][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:37,909 74460:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:37,912 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 23:27:37,918 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.114014 loss=35.000 weight=0.000 batch={'x': tensor([ 5, 30]), 'y': tensor([10, 60])}\n",
      "[2023-08-02 23:27:37,922 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.091485 loss=96.000 weight=0.000 batch={'x': tensor([49, 47]), 'y': tensor([98, 94])}\n",
      "[2023-08-02 23:27:37,926 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.083288 loss=108.000 weight=0.000 batch={'x': tensor([44, 64]), 'y': tensor([ 88, 128])}\n",
      "[2023-08-02 23:27:37,931 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.077096 loss=17.000 weight=0.000 batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])}\n",
      "[2023-08-02 23:27:37,935 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.071604 loss=35.000 weight=0.000 batch={'x': tensor([ 9, 26]), 'y': tensor([18, 52])}\n",
      "[2023-08-02 23:27:37,939 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.066089 loss=62.000 weight=0.000 batch={'x': tensor([ 8, 54]), 'y': tensor([ 16, 108])}\n",
      "[2023-08-02 23:27:37,944 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpq5ob6p67/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-02 23:27:37,948 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 23:27:37,953 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.078222 loss=65.000 weight=0.000 batch={'x': tensor([43, 22]), 'y': tensor([86, 44])}\n",
      "[2023-08-02 23:27:37,957 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.069510 loss=37.000 weight=0.000 batch={'x': tensor([26, 11]), 'y': tensor([52, 22])}\n",
      "[2023-08-02 23:27:37,961 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.062719 loss=58.000 weight=0.000 batch={'x': tensor([18, 40]), 'y': tensor([36, 80])}\n",
      "[2023-08-02 23:27:37,978 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.069319 loss=64.000 weight=0.000 batch={'x': tensor([45, 19]), 'y': tensor([90, 38])}\n",
      "[2023-08-02 23:27:37,986 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.063604 loss=55.000 weight=0.000 batch={'x': tensor([ 1, 54]), 'y': tensor([  2, 108])}\n",
      "[2023-08-02 23:27:37,995 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.058241 loss=83.000 weight=0.000 batch={'x': tensor([23, 60]), 'y': tensor([ 46, 120])}\n",
      "[2023-08-02 23:27:38,007 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.054006 loss=66.000 weight=0.000 batch={'x': tensor([ 4, 62]), 'y': tensor([  8, 124])}\n",
      "[2023-08-02 23:27:38,017 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpq5ob6p67/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-02 23:27:38,026 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 23:27:38,040 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.058604 loss=102.000 weight=0.000 batch={'x': tensor([39, 63]), 'y': tensor([ 78, 126])}\n",
      "[2023-08-02 23:27:38,045 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.047951 loss=50.000 weight=0.000 batch={'x': tensor([ 4, 46]), 'y': tensor([ 8, 92])}\n",
      "[2023-08-02 23:27:38,050 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.038061 loss=44.000 weight=0.000 batch={'x': tensor([19, 25]), 'y': tensor([38, 50])}\n",
      "[2023-08-02 23:27:38,055 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.028746 loss=108.000 weight=0.000 batch={'x': tensor([51, 57]), 'y': tensor([102, 114])}\n",
      "[2023-08-02 23:27:38,060 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.019784 loss=13.000 weight=0.000 batch={'x': tensor([ 2, 11]), 'y': tensor([ 4, 22])}\n",
      "[2023-08-02 23:27:38,064 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.011214 loss=19.000 weight=0.000 batch={'x': tensor([10,  9]), 'y': tensor([20, 18])}\n",
      "[2023-08-02 23:27:38,071 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.003181 loss=30.000 weight=0.000 batch={'x': tensor([18, 12]), 'y': tensor([36, 24])}\n",
      "[2023-08-02 23:27:38,074 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpq5ob6p67/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-02 23:27:38,079 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpq5ob6p67/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpq5ob6p67\u001b[0m\n",
      "└── \u001b[1;36mcustom_epoch_based_trainer\u001b[0m\n",
      "    ├── 2023-08-02T23-27-37_824195-08-00.log\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[1;36mlatest\u001b[0m\n",
      "            ├── callbacks.pth\n",
      "            ├── meta.pth\n",
      "            ├── model.pth\n",
      "            ├── optim.pth\n",
      "            └── strategy.pth\n",
      "\n",
      "7 directories, 21 files\n",
      "\n",
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {}}\n",
      "\n",
      "meta.pth:\n",
      "{'epoch': 2, 'iter_': 68}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_by_epoch_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'custom_epoch_based_trainer' / 'checkpoints' / 'epoch_2'\n",
    "    for f in epoch_2.glob('*.pth'):\n",
    "        print(f\"{f.name}:\")\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_load_from_callback_demo = checkpoint_by_epoch_callback_demo.copy()\n",
    "checkpoint_load_from_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:38,613 74460:140704362395200][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:38,616 74460:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:38,618 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 23:27:38,626 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.157916 loss=14.035 weight=0.997 batch={'x': tensor([ 5, 23]), 'y': tensor([10, 46])}\n",
      "[2023-08-02 23:27:38,636 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.164809 loss=5.130 weight=1.810 batch={'x': tensor([25, 29]), 'y': tensor([50, 58])}\n",
      "[2023-08-02 23:27:38,642 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.139664 loss=0.563 weight=1.937 batch={'x': tensor([10,  8]), 'y': tensor([20, 16])}\n",
      "[2023-08-02 23:27:38,647 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.120614 loss=0.668 weight=1.985 batch={'x': tensor([33, 56]), 'y': tensor([ 66, 112])}\n",
      "[2023-08-02 23:27:38,653 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.107437 loss=0.402 weight=2.017 batch={'x': tensor([39,  7]), 'y': tensor([78, 14])}\n",
      "[2023-08-02 23:27:38,658 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.096216 loss=4.204 weight=1.852 batch={'x': tensor([27, 30]), 'y': tensor([54, 60])}\n",
      "[2023-08-02 23:27:38,662 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpounurhw4/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-02 23:27:38,668 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 23:27:38,674 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.107573 loss=4.165 weight=2.070 batch={'x': tensor([59, 60]), 'y': tensor([118, 120])}\n",
      "[2023-08-02 23:27:38,680 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.096184 loss=2.090 weight=2.095 batch={'x': tensor([12, 32]), 'y': tensor([24, 64])}\n",
      "[2023-08-02 23:27:38,689 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.089752 loss=1.170 weight=2.032 batch={'x': tensor([22, 50]), 'y': tensor([ 44, 100])}\n",
      "[2023-08-02 23:27:38,693 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.078371 loss=0.461 weight=1.992 batch={'x': tensor([65, 58]), 'y': tensor([130, 116])}\n",
      "[2023-08-02 23:27:38,701 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.071006 loss=1.733 weight=1.917 batch={'x': tensor([ 8, 34]), 'y': tensor([16, 68])}\n",
      "[2023-08-02 23:27:38,706 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.061913 loss=4.025 weight=1.912 batch={'x': tensor([31, 61]), 'y': tensor([ 62, 122])}\n",
      "[2023-08-02 23:27:38,714 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.054603 loss=3.094 weight=2.082 batch={'x': tensor([56, 19]), 'y': tensor([112,  38])}\n",
      "[2023-08-02 23:27:38,726 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpounurhw4/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-02 23:27:38,732 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 23:27:38,739 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.055381 loss=5.750 weight=2.100 batch={'x': tensor([51, 64]), 'y': tensor([102, 128])}\n",
      "[2023-08-02 23:27:38,748 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.046766 loss=4.165 weight=2.085 batch={'x': tensor([37, 61]), 'y': tensor([ 74, 122])}\n",
      "[2023-08-02 23:27:38,760 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.039205 loss=1.943 weight=1.907 batch={'x': tensor([27, 15]), 'y': tensor([54, 30])}\n",
      "[2023-08-02 23:27:38,767 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.029752 loss=3.960 weight=1.890 batch={'x': tensor([23, 49]), 'y': tensor([46, 98])}\n",
      "[2023-08-02 23:27:38,773 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.020657 loss=2.906 weight=1.812 batch={'x': tensor([ 1, 30]), 'y': tensor([ 2, 60])}\n",
      "[2023-08-02 23:27:38,780 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.011971 loss=5.692 weight=2.115 batch={'x': tensor([33, 66]), 'y': tensor([ 66, 132])}\n",
      "[2023-08-02 23:27:38,787 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.003376 loss=7.175 weight=1.795 batch={'x': tensor([62,  8]), 'y': tensor([124,  16])}\n",
      "[2023-08-02 23:27:38,792 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpounurhw4/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-02 23:27:38,799 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpounurhw4/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:39,268 74460:140704362395200][checkpoint.py:44 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpounurhw4/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-02 23:27:39,333 74460:140704362395200][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:39,335 74460:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:39,338 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 23:27:39,345 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.123424 loss=0.863 weight=2.025 batch={'x': tensor([11, 58]), 'y': tensor([ 22, 116])}\n",
      "[2023-08-02 23:27:39,351 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.052947 loss=3.290 weight=2.118 batch={'x': tensor([ 5, 51]), 'y': tensor([ 10, 102])}\n",
      "[2023-08-02 23:27:39,356 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.034239 loss=9.900 weight=2.200 batch={'x': tensor([55, 44]), 'y': tensor([110,  88])}\n",
      "[2023-08-02 23:27:39,366 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.028684 loss=3.547 weight=2.107 batch={'x': tensor([10, 56]), 'y': tensor([ 20, 112])}\n",
      "[2023-08-02 23:27:39,375 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.020691 loss=3.548 weight=1.892 batch={'x': tensor([29, 37]), 'y': tensor([58, 74])}\n",
      "[2023-08-02 23:27:39,384 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.012014 loss=3.372 weight=2.095 batch={'x': tensor([48, 23]), 'y': tensor([96, 46])}\n",
      "[2023-08-02 23:27:39,390 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.003251 loss=3.960 weight=2.165 batch={'x': tensor([21, 27]), 'y': tensor([42, 54])}\n",
      "[2023-08-02 23:27:39,395 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpounurhw4/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-02 23:27:39,399 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpounurhw4/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_load_from_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_load_from_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "        load_from=os.path.join(work_dirs, 'custom_epoch_based_trainer', 'checkpoints', 'epoch_2')\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaultyRunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError('faulty runner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyValidator(FaultyRunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyIterBasedTrainer(FaultyRunnerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyEpochBasedTrainer(\n",
    "    FaultyRunnerMixin,\n",
    "    todd.runners.EpochBasedTrainer,\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_callback_demo = validator_demo.copy()\n",
    "monitor_callback_demo.type = 'FaultyValidator'\n",
    "monitor_callback = todd.Config(type='MonitorCallback')\n",
    "monitor_callback_demo.callbacks = [monitor_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:39,564 74460:140704362395200][log.py:48 todd.FaultyValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:39,566 74460:140704362395200][base.py:53 todd.FaultyValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2023-08-02 23:27:39,571 74460:140704362395200][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x157feba10>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 193, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_74460/2137902126.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:39,564 74460:140704362395200][log.py:48 todd.FaultyValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "[2023-08-02 23:27:39,566 74460:140704362395200][base.py:53 todd.FaultyValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-08-02 23:27:39,571 74460:140704362395200][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x157feba10>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 193, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_74460/2137902126.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        monitor_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_load_model_from_demo = checkpoint_load_from_callback_demo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:40,018 74460:140704362395200][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:40,021 74460:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:40,023 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 23:27:40,030 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.133763 loss=26.048 weight=0.867 batch={'x': tensor([41,  5]), 'y': tensor([82, 10])}\n",
      "[2023-08-02 23:27:40,036 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.118974 loss=6.413 weight=1.775 batch={'x': tensor([19, 38]), 'y': tensor([38, 76])}\n",
      "[2023-08-02 23:27:40,041 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.101413 loss=3.713 weight=1.918 batch={'x': tensor([40, 50]), 'y': tensor([ 80, 100])}\n",
      "[2023-08-02 23:27:40,047 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.096534 loss=8.156 weight=2.188 batch={'x': tensor([53, 34]), 'y': tensor([106,  68])}\n",
      "[2023-08-02 23:27:40,052 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.088898 loss=0.143 weight=1.995 batch={'x': tensor([45, 12]), 'y': tensor([90, 24])}\n",
      "[2023-08-02 23:27:40,056 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.079769 loss=1.581 weight=1.942 batch={'x': tensor([37, 18]), 'y': tensor([74, 36])}\n",
      "[2023-08-02 23:27:40,063 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpx1fho5w5/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-02 23:27:40,069 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 23:27:40,072 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.093718 loss=1.045 weight=2.047 batch={'x': tensor([ 6, 38]), 'y': tensor([12, 76])}\n",
      "[2023-08-02 23:27:40,076 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.082429 loss=7.354 weight=2.132 batch={'x': tensor([67, 44]), 'y': tensor([134,  88])}\n",
      "[2023-08-02 23:27:40,080 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.072330 loss=2.888 weight=1.925 batch={'x': tensor([15, 62]), 'y': tensor([ 30, 124])}\n",
      "[2023-08-02 23:27:40,086 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.064893 loss=6.475 weight=1.825 batch={'x': tensor([28, 46]), 'y': tensor([56, 92])}\n",
      "[2023-08-02 23:27:40,090 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.056706 loss=1.500 weight=1.970 batch={'x': tensor([64, 36]), 'y': tensor([128,  72])}\n",
      "[2023-08-02 23:27:40,095 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.050351 loss=2.745 weight=2.090 batch={'x': tensor([43, 18]), 'y': tensor([86, 36])}\n",
      "[2023-08-02 23:27:40,100 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.043955 loss=0.594 weight=1.987 batch={'x': tensor([34, 61]), 'y': tensor([ 68, 122])}\n",
      "[2023-08-02 23:27:40,104 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpx1fho5w5/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-02 23:27:40,108 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 23:27:40,111 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.040283 loss=6.235 weight=1.855 batch={'x': tensor([27, 59]), 'y': tensor([ 54, 118])}\n",
      "[2023-08-02 23:27:40,117 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.033722 loss=0.586 weight=2.017 batch={'x': tensor([34, 33]), 'y': tensor([68, 66])}\n",
      "[2023-08-02 23:27:40,122 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.027131 loss=4.781 weight=1.887 batch={'x': tensor([42, 43]), 'y': tensor([84, 86])}\n",
      "[2023-08-02 23:27:40,126 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.020508 loss=0.112 weight=2.015 batch={'x': tensor([ 1, 14]), 'y': tensor([ 2, 28])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:40,131 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.014355 loss=8.092 weight=2.195 batch={'x': tensor([64, 19]), 'y': tensor([128,  38])}\n",
      "[2023-08-02 23:27:40,136 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.008334 loss=0.975 weight=2.162 batch={'x': tensor([8, 4]), 'y': tensor([16,  8])}\n",
      "[2023-08-02 23:27:40,141 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.002348 loss=11.155 weight=2.230 batch={'x': tensor([48, 49]), 'y': tensor([96, 98])}\n",
      "[2023-08-02 23:27:40,143 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpx1fho5w5/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-02 23:27:40,149 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpx1fho5w5/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 23:27:40,622 74460:140704362395200][log.py:48 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1-x86_64-i386-64bit\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 093521c\n",
      "\u001b[2m[2023-08-02 23:27:40,625 74460:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 23:27:40,628 74460:140704362395200][base.py:64 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpx1fho5w5/custom_epoch_based_trainer/checkpoints/epoch_2/model.pth\n",
      "[2023-08-02 23:27:40,632 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 23:27:40,639 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.121211 loss=1.953 weight=1.945 batch={'x': tensor([37, 34]), 'y': tensor([74, 68])}\n",
      "[2023-08-02 23:27:40,644 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.107686 loss=0.356 weight=1.987 batch={'x': tensor([32, 25]), 'y': tensor([64, 50])}\n",
      "[2023-08-02 23:27:40,650 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.103513 loss=2.520 weight=1.880 batch={'x': tensor([20, 22]), 'y': tensor([40, 44])}\n",
      "[2023-08-02 23:27:40,655 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.091196 loss=0.473 weight=1.985 batch={'x': tensor([53, 10]), 'y': tensor([106,  20])}\n",
      "[2023-08-02 23:27:40,659 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.081555 loss=3.052 weight=2.092 batch={'x': tensor([21, 45]), 'y': tensor([42, 90])}\n",
      "[2023-08-02 23:27:40,667 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.083196 loss=3.331 weight=1.897 batch={'x': tensor([46, 19]), 'y': tensor([92, 38])}\n",
      "[2023-08-02 23:27:40,673 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpx1fho5w5/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-02 23:27:40,676 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 23:27:40,679 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.089120 loss=6.300 weight=1.880 batch={'x': tensor([63, 42]), 'y': tensor([126,  84])}\n",
      "[2023-08-02 23:27:40,685 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.081195 loss=8.194 weight=2.172 batch={'x': tensor([33, 62]), 'y': tensor([ 66, 124])}\n",
      "[2023-08-02 23:27:40,689 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.072044 loss=5.760 weight=1.840 batch={'x': tensor([35, 37]), 'y': tensor([70, 74])}\n",
      "[2023-08-02 23:27:40,694 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.064442 loss=5.460 weight=1.860 batch={'x': tensor([47, 31]), 'y': tensor([94, 62])}\n",
      "[2023-08-02 23:27:40,699 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.057061 loss=0.712 weight=2.037 batch={'x': tensor([10, 28]), 'y': tensor([20, 56])}\n",
      "[2023-08-02 23:27:40,703 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.049841 loss=6.885 weight=2.202 batch={'x': tensor([36, 32]), 'y': tensor([72, 64])}\n",
      "[2023-08-02 23:27:40,709 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.043569 loss=1.519 weight=2.037 batch={'x': tensor([51, 30]), 'y': tensor([102,  60])}\n",
      "[2023-08-02 23:27:40,714 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpx1fho5w5/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-02 23:27:40,719 74460:140704362395200][log.py:87 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 23:27:40,723 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.041500 loss=4.623 weight=1.892 batch={'x': tensor([25, 61]), 'y': tensor([ 50, 122])}\n",
      "[2023-08-02 23:27:40,729 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.034841 loss=5.704 weight=1.902 batch={'x': tensor([60, 57]), 'y': tensor([120, 114])}\n",
      "[2023-08-02 23:27:40,737 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.028672 loss=1.300 weight=1.960 batch={'x': tensor([14, 51]), 'y': tensor([ 28, 102])}\n",
      "[2023-08-02 23:27:40,790 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.031604 loss=2.400 weight=2.080 batch={'x': tensor([33, 27]), 'y': tensor([66, 54])}\n",
      "[2023-08-02 23:27:40,799 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.022192 loss=2.475 weight=2.045 batch={'x': tensor([45, 65]), 'y': tensor([ 90, 130])}\n",
      "[2023-08-02 23:27:40,811 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.013134 loss=1.757 weight=2.047 batch={'x': tensor([55, 19]), 'y': tensor([110,  38])}\n",
      "[2023-08-02 23:27:40,820 74460:140704362395200][log.py:81 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.003747 loss=3.800 weight=1.920 batch={'x': tensor([54, 41]), 'y': tensor([108,  82])}\n",
      "[2023-08-02 23:27:40,826 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpx1fho5w5/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-02 23:27:40,830 74460:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpx1fho5w5/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        strategy_load_model_from_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        strategy_load_model_from_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.load_model_from(os.path.join(work_dirs, 'custom_epoch_based_trainer', 'checkpoints', 'epoch_2', 'model.pth'))\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
