{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Models and Datasets as Usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-10 01:14:03,913 51456:4344219008][loggers.py:110 todd.base.patches.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-10 01:14:03,913 51456:4344219008][patches.py:36 todd.base.patches.<module>] INFO: `ipdb` is installed. Using it for debugging.\n",
      "\u001b[2m[2023-01-10 01:14:04,792 51456:4344219008][loggers.py:110 todd.base.registries.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import todd\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models should be built by users.\n",
    "The same model can be used by multiple runners, such as a trainer and a validator, simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class Model(todd.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(2.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> float:\n",
    "        return self._weight.item()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to models, datasets are built inside runners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> int:\n",
    "        return self._data[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Mixin for All Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerMixin(todd.utils.BaseRunner):\n",
    "\n",
    "    def _build_dataloader(\n",
    "        self,\n",
    "        config: todd.Config,\n",
    "    ) -> torch.utils.data.DataLoader:\n",
    "        dataset = Dataset(**config.pop('dataset'))\n",
    "        return torch.utils.data.DataLoader(dataset, **config)\n",
    "\n",
    "    def _run_iter(self, i: int, batch, memo: todd.utils.Memo) -> torch.Tensor:\n",
    "        y: torch.Tensor = self._model(batch)\n",
    "        loss = y.sum().abs()\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DRY_RUN` is turned on by default when CUDA devices are not available.\n",
    "To override this setting, manually set `DRY_RUN` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.utils.BaseRunner.Store.DRY_RUN = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import cast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and register the validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.utils.RunnerRegistry.register()\n",
    "class CustomValidator(RunnerMixin, todd.utils.Validator):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the validator config. \n",
    "`config` will be reused by trainers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader = {'batch_size': 1, 'dataset': {'n': 20}}\n",
      "load_state_dict = {'model': {'strict': False}}\n",
      "log = {'interval': 5}\n",
      "model = Model()\n",
      "state_dict = {'model': {}}\n",
      "type = CustomValidator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    model=model,\n",
    "    log=dict(interval=5),\n",
    "    load_state_dict=dict(model=dict(strict=False)),\n",
    "    state_dict=dict(model=dict()),\n",
    ")\n",
    "validator = todd.Config(\n",
    "    type='CustomValidator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(n=20)),\n",
    ")\n",
    "validator.update(config)\n",
    "print(validator.dumps())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and run the validator.\n",
    "Logs will be saved to the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-10 01:14:08,400 51456:4344219008][loggers.py:110 todd.utils.runners.4395637856.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-10 01:14:08,403 51456:4344219008][runners.py:166 todd.utils.runners.4395637856._run] INFO: Iter [5/20] Loss 10.000\n",
      "[2023-01-10 01:14:08,404 51456:4344219008][runners.py:166 todd.utils.runners.4395637856._run] INFO: Iter [10/20] Loss 20.000\n",
      "[2023-01-10 01:14:08,406 51456:4344219008][runners.py:166 todd.utils.runners.4395637856._run] INFO: Iter [15/20] Loss 30.000\n",
      "[2023-01-10 01:14:08,408 51456:4344219008][runners.py:166 todd.utils.runners.4395637856._run] INFO: Iter [20/20] Loss 40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20230110T011408f.log']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    validator.name = work_dir\n",
    "    runner = todd.utils.RunnerRegistry.build(validator)\n",
    "    cast(CustomValidator, runner).run()\n",
    "    print(os.listdir(work_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = config.copy()\n",
    "trainer.update(\n",
    "    dataloader=dict(batch_size=2, dataset=dict(n=67)),\n",
    "    optimizer=dict(type='SGD', lr=0.01),\n",
    "    load_state_dict=dict(optimizer=dict()),\n",
    "    state_dict=dict(optimizer=dict(), interval=20),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.utils.RunnerRegistry.register()\n",
    "class CustomIterBasedTrainer(RunnerMixin, todd.utils.IterBasedTrainer):\n",
    "\n",
    "    def _before_run_iter_log(\n",
    "        self,\n",
    "        i: int,\n",
    "        batch,\n",
    "        memo: todd.utils.Memo,\n",
    "    ) -> str | None:\n",
    "        info = super()._before_run_iter_log(i, batch, memo)\n",
    "        if info is None:\n",
    "            info = ''\n",
    "        model: Model = self.model\n",
    "        info += f\" Weight {model.weight:.3f}\"\n",
    "        info += f\" Batch {batch}\"\n",
    "        return info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `_before_run_iter_log` returns `None`, meaning that no message will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-10 01:14:08,603 51456:4344219008][loggers.py:110 todd.utils.runners.6279031744.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-10 01:14:08,608 51456:4344219008][runners.py:154 todd.utils.runners.6279031744._run] INFO:  Weight 1.640 Batch tensor([ 9, 10])\n",
      "[2023-01-10 01:14:08,610 51456:4344219008][runners.py:166 todd.utils.runners.6279031744._run] INFO: Iter [6/53] Loss 31.160\n",
      "[2023-01-10 01:14:08,613 51456:4344219008][runners.py:154 todd.utils.runners.6279031744._run] INFO:  Weight 0.290 Batch tensor([19, 20])\n",
      "[2023-01-10 01:14:08,615 51456:4344219008][runners.py:166 todd.utils.runners.6279031744._run] INFO: Iter [11/53] Loss 11.310\n",
      "[2023-01-10 01:14:08,618 51456:4344219008][runners.py:154 todd.utils.runners.6279031744._run] INFO:  Weight -0.180 Batch tensor([29, 30])\n",
      "[2023-01-10 01:14:08,620 51456:4344219008][runners.py:166 todd.utils.runners.6279031744._run] INFO: Iter [16/53] Loss 10.620\n",
      "[2023-01-10 01:14:08,624 51456:4344219008][runners.py:154 todd.utils.runners.6279031744._run] INFO:  Weight 0.490 Batch tensor([39, 40])\n",
      "[2023-01-10 01:14:08,625 51456:4344219008][runners.py:188 todd.utils.runners.6279031744.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpbwhfq222/iter_20.pth\n",
      "[2023-01-10 01:14:08,627 51456:4344219008][runners.py:166 todd.utils.runners.6279031744._run] INFO: Iter [21/53] Loss 38.710\n",
      "[2023-01-10 01:14:08,630 51456:4344219008][runners.py:154 todd.utils.runners.6279031744._run] INFO:  Weight -0.380 Batch tensor([49, 50])\n",
      "[2023-01-10 01:14:08,632 51456:4344219008][runners.py:166 todd.utils.runners.6279031744._run] INFO: Iter [26/53] Loss 37.620\n",
      "[2023-01-10 01:14:08,636 51456:4344219008][runners.py:154 todd.utils.runners.6279031744._run] INFO:  Weight 0.690 Batch tensor([59, 60])\n",
      "[2023-01-10 01:14:08,638 51456:4344219008][runners.py:166 todd.utils.runners.6279031744._run] INFO: Iter [31/53] Loss 82.110\n",
      "[2023-01-10 01:14:08,643 51456:4344219008][runners.py:154 todd.utils.runners.6279031744._run] INFO:  Weight -0.040 Batch tensor([ 9, 10])\n",
      "[2023-01-10 01:14:08,657 51456:4344219008][runners.py:166 todd.utils.runners.6279031744._run] INFO: Iter [40/53] Loss 0.760\n",
      "[2023-01-10 01:14:08,659 51456:4344219008][runners.py:188 todd.utils.runners.6279031744.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpbwhfq222/iter_40.pth\n",
      "[2023-01-10 01:14:08,695 51456:4344219008][runners.py:154 todd.utils.runners.6279031744._run] INFO:  Weight 0.230 Batch tensor([19, 20])\n",
      "[2023-01-10 01:14:08,713 51456:4344219008][runners.py:166 todd.utils.runners.6279031744._run] INFO: Iter [45/53] Loss 8.970\n",
      "[2023-01-10 01:14:08,731 51456:4344219008][runners.py:154 todd.utils.runners.6279031744._run] INFO:  Weight -0.240 Batch tensor([29, 30])\n",
      "[2023-01-10 01:14:08,733 51456:4344219008][runners.py:166 todd.utils.runners.6279031744._run] INFO: Iter [50/53] Loss 14.160\n",
      "[2023-01-10 01:14:08,737 51456:4344219008][runners.py:188 todd.utils.runners.6279031744.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpbwhfq222/latest.pth\n",
      "[2023-01-10 01:14:08,739 51456:4344219008][runners.py:270 todd.utils.runners.6279031744.validate] INFO: Skipping validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iter_20.pth', '20230110T011408f.log', 'iter_40.pth', 'latest.pth']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    iter_based_trainer = trainer.copy()\n",
    "    iter_based_trainer.update(\n",
    "        type='CustomIterBasedTrainer',\n",
    "        name=work_dir,\n",
    "        iters=53,\n",
    "    )\n",
    "    runner = todd.utils.RunnerRegistry.build(iter_based_trainer)\n",
    "    cast(CustomIterBasedTrainer, runner).run()\n",
    "    print(os.listdir(work_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainers increment `todd.Store.ITER` to keep track of the training progress.\n",
    "If multiple trainers are to be run, `todd.Store.ITER` must be manually reset to zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.utils.RunnerRegistry.register()\n",
    "class CustomEpochBasedTrainer(RunnerMixin, todd.utils.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-10 01:14:08,830 51456:4344219008][loggers.py:110 todd.utils.runners.4395642416.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-10 01:14:08,832 51456:4344219008][runners.py:422 todd.utils.runners.4395642416._run] INFO: Epoch [1/3] beginning\n",
      "[2023-01-10 01:14:08,842 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [5/34] Loss 1.330\n",
      "[2023-01-10 01:14:08,847 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [10/34] Loss 7.800\n",
      "[2023-01-10 01:14:08,851 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [15/34] Loss 15.930\n",
      "[2023-01-10 01:14:08,859 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [20/34] Loss 31.600\n",
      "[2023-01-10 01:14:08,862 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [25/34] Loss 46.530\n",
      "[2023-01-10 01:14:08,866 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [30/34] Loss 71.400\n",
      "[2023-01-10 01:14:08,876 51456:4344219008][runners.py:188 todd.utils.runners.4395642416.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmprwtnzye2/epoch_1.pth\n",
      "[2023-01-10 01:14:08,879 51456:4344219008][runners.py:435 todd.utils.runners.4395642416._run] INFO: Epoch [2/3] ended\n",
      "[2023-01-10 01:14:08,880 51456:4344219008][runners.py:422 todd.utils.runners.4395642416._run] INFO: Epoch [2/3] beginning\n",
      "[2023-01-10 01:14:08,883 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [5/34] Loss 1.710\n",
      "[2023-01-10 01:14:08,888 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [10/34] Loss 7.020\n",
      "[2023-01-10 01:14:08,892 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [15/34] Loss 17.110\n",
      "[2023-01-10 01:14:08,895 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [20/34] Loss 30.020\n",
      "[2023-01-10 01:14:08,898 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [25/34] Loss 48.510\n",
      "[2023-01-10 01:14:08,901 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [30/34] Loss 69.020\n",
      "[2023-01-10 01:14:08,904 51456:4344219008][runners.py:188 todd.utils.runners.4395642416.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmprwtnzye2/epoch_2.pth\n",
      "[2023-01-10 01:14:08,906 51456:4344219008][runners.py:435 todd.utils.runners.4395642416._run] INFO: Epoch [3/3] ended\n",
      "[2023-01-10 01:14:08,908 51456:4344219008][runners.py:422 todd.utils.runners.4395642416._run] INFO: Epoch [3/3] beginning\n",
      "[2023-01-10 01:14:08,911 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [5/34] Loss 1.710\n",
      "[2023-01-10 01:14:08,915 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [10/34] Loss 7.020\n",
      "[2023-01-10 01:14:08,922 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [15/34] Loss 17.110\n",
      "[2023-01-10 01:14:08,925 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [20/34] Loss 30.020\n",
      "[2023-01-10 01:14:08,928 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [25/34] Loss 48.510\n",
      "[2023-01-10 01:14:08,931 51456:4344219008][runners.py:166 todd.utils.runners.4395642416._run] INFO: Iter [30/34] Loss 69.020\n",
      "[2023-01-10 01:14:08,933 51456:4344219008][runners.py:188 todd.utils.runners.4395642416.write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmprwtnzye2/epoch_3.pth\n",
      "[2023-01-10 01:14:08,936 51456:4344219008][runners.py:435 todd.utils.runners.4395642416._run] INFO: Epoch [4/3] ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20230110T011408f.log', 'epoch_1.pth', 'epoch_2.pth', 'epoch_3.pth']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    epoch_based_trainer = trainer.copy()\n",
    "    epoch_based_trainer.update(\n",
    "        type='CustomEpochBasedTrainer',\n",
    "        name=work_dir,\n",
    "        epochs=3,\n",
    "    )\n",
    "    runner = todd.utils.RunnerRegistry.build(epoch_based_trainer)\n",
    "    cast(CustomEpochBasedTrainer, runner).run()\n",
    "    print(os.listdir(work_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.utils.BaseRunner.Store.DRY_RUN = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `DRY_RUN` is enabled, the runner will stop upon the first log message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-10 01:14:09,141 51456:4344219008][loggers.py:110 todd.utils.runners.6329007312.get_logger] DEBUG: logger initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-10 01:14:09,144 51456:4344219008][runners.py:166 todd.utils.runners.6329007312._run] INFO: Iter [5/20] Loss 0.050\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    validator.name = work_dir\n",
    "    runner = todd.utils.RunnerRegistry.build(validator)\n",
    "    cast(CustomValidator, runner).run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
