{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install --no-build-isolation --extra-index-url https://pypi.org/simple .. > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-15 14:18:04,477 47495:140704501939840][patches.py:9 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n",
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import todd\n",
    "from todd.runners import Memo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register_()\n",
    "class RunnerModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def _forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        runner: todd.runners.BaseRunner,\n",
    "        batch,\n",
    "        memo: Memo,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> Memo:\n",
    "        log: dict[str, Any] | None = memo.get(\"log\")\n",
    "        y = self._forward(batch[\"x\"])\n",
    "        loss = F.l1_loss(y, batch[\"y\"])\n",
    "        memo[\"loss\"] = loss\n",
    "        if log is not None:\n",
    "            log[\"batch\"] = str(batch)\n",
    "            log[\"weight\"] = f\"{self._weight.item():.3f}\"\n",
    "            log[\"loss\"] = f\"{loss:.3f}\"\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register_()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:06,593 47495:140704501939840][base.py:56 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7f4g0rku\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:06,912 47495:140704501939840][base.py:56 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:06,916 47495:140704501939840][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:06,919 47495:140704501939840][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-15 14:18:06,921 47495:140704501939840][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-15 14:18:06,924 47495:140704501939840][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpcc8vw48n\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[dict(type='LogCallback', interval=5)],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:07,227 47495:140704501939840][base.py:56 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:07,231 47495:140704501939840][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [1/8] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.000 loss=11.000\n",
      "[2024-02-15 14:18:07,232 47495:140704501939840][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [2/8] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.000 loss=14.000\n",
      "[2024-02-15 14:18:07,234 47495:140704501939840][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [3/8] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.000 loss=16.000\n",
      "[2024-02-15 14:18:07,236 47495:140704501939840][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [4/8] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:07,237 47495:140704501939840][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [5/8] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.000 loss=4.000\n",
      "[2024-02-15 14:18:07,238 47495:140704501939840][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [6/8] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.000 loss=11.000\n",
      "[2024-02-15 14:18:07,240 47495:140704501939840][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [7/8] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.000 loss=14.000\n",
      "[2024-02-15 14:18:07,241 47495:140704501939840][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [8/8] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.000 loss=16.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"iter_based_trainer\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:07,255 47495:140704501939840][base.py:56 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:07,256 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-15 14:18:07,258 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [1/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.000 loss=17.000\n",
      "[2024-02-15 14:18:07,260 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [2/15] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.000 loss=5.000\n",
      "[2024-02-15 14:18:07,262 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [3/15] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.000 loss=12.000\n",
      "[2024-02-15 14:18:07,264 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [4/15] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.000 loss=14.000\n",
      "[2024-02-15 14:18:07,265 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [5/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.000 loss=7.000\n",
      "[2024-02-15 14:18:07,267 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-15 14:18:07,269 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [6/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:07,271 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [7/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.000 loss=11.000\n",
      "[2024-02-15 14:18:07,272 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [8/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.000 loss=9.000\n",
      "[2024-02-15 14:18:07,274 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [9/15] batch={'x': tensor([10,  5]), 'y': tensor([20, 10])} weight=0.000 loss=15.000\n",
      "[2024-02-15 14:18:07,276 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [10/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:07,277 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-15 14:18:07,280 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [11/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.000 loss=9.000\n",
      "[2024-02-15 14:18:07,283 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.000 loss=19.000\n",
      "[2024-02-15 14:18:07,286 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [13/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.000 loss=3.000\n",
      "[2024-02-15 14:18:07,288 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [14/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.000 loss=12.000\n",
      "[2024-02-15 14:18:07,291 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [15/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.000 loss=12.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"epoch_based_trainer\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-15 14:18:07,448 47495:140704501939840][log.py:55 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.7.0\n",
      "todd_version: 0.4.0\n",
      "cuda_home: None\n",
      "git_commit_id: b8c8f0c\n",
      "git_status: \n",
      "M todd/runners/base.py\n",
      " M todd/runners/strategies/base.py\n",
      " M todd/runners/strategies/cuda.py\n",
      " M todd/runners/strategies/ddp.py\n",
      " M todd/runners/strategies/fsdp.py\n",
      " M todd/runners/trainer.py\n",
      " M todd/runners/validator.py\n",
      " M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-02-15 14:18:07,450 47495:140704501939840][base.py:56 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:07,455 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:07,458 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-15 14:18:07,461 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-15 14:18:07,464 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:07,500 47495:140704501939840][base.py:56 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:07,503 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:07,506 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-15 14:18:07,510 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-15 14:18:07,513 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1zxiy600\u001b[0m\n",
      "└── \u001b[1;36mlog_callback\u001b[0m\n",
      "    └── 2024-02-15T14-18-07_499816-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2024-02-15 14:18:07,500 47495:140704501939840][base.py:56 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2024-02-15 14:18:07,503 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:07,506 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-15 14:18:07,510 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-15 14:18:07,513 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='log_callback',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type='LogCallback',\n",
    "            interval=5,\n",
    "            with_file_handler=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/log_callback/*.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:08,144 47495:140704501939840][base.py:56 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:08,668 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:01 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:09,186 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:01 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-15 14:18:09,700 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-15 14:18:10,212 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:10,227 47495:140704501939840][base.py:56 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:11,740 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:04 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:15,755 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:05 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-15 14:18:20,773 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:03 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-15 14:18:25,795 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"EMA_ETA\", ema=dict(decay=0.2)),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1 * min(10, runner.iter_))\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-15 14:18:25,934 47495:140704501939840][log.py:55 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.7.0\n",
      "todd_version: 0.4.0\n",
      "cuda_home: None\n",
      "git_commit_id: b8c8f0c\n",
      "git_status: \n",
      "M todd/runners/base.py\n",
      " M todd/runners/strategies/base.py\n",
      " M todd/runners/strategies/cuda.py\n",
      " M todd/runners/strategies/ddp.py\n",
      " M todd/runners/strategies/fsdp.py\n",
      " M todd/runners/trainer.py\n",
      " M todd/runners/validator.py\n",
      " M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-02-15 14:18:25,935 47495:140704501939840][base.py:56 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:25,941 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:00 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:25,945 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:00 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-15 14:18:25,947 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-15 14:18:25,950 47495:140704501939840][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "            with_file_handler=True,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-15 14:18:26,024 47495:140704501939840][git.py:41 todd.Validator.git_callback init] INFO: Saving git diff to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpbzu2_nq7/git_callback/git_diff_2024-02-15T14-18-26_023733-08-00.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:26,028 47495:140704501939840][base.py:56 todd.Validator.git_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff --git a/todd/runners/base.py b/todd/runners/base.py\n",
      "index 2efd19e..fda811f 100644\n",
      "--- a/todd/runners/base.py\n",
      "+++ b/todd/runners/base.py\n",
      "@@ -8,10 +8,10 @@ import logging\n",
      " import os\n",
      " import pathlib\n",
      " import socket\n",
      "-from typing import TYPE_CHECKING, Any, Mapping\n",
      "+from typing import TYPE_CHECKING, Any, Generic, Mapping, TypeVar\n",
      " \n",
      "-import torch\n",
      "-import torch.utils.data\n",
      "+from torch import nn\n",
      "+from torch.utils.data import DataLoader, Dataset\n",
      " \n",
      " from ..base import (\n",
      "     CallbackRegistry,\n",
      "@@ -30,9 +30,11 @@ if TYPE_CHECKING:\n",
      "     from .callbacks import ComposedCallback\n",
      "     from .strategies import BaseStrategy\n",
      " \n",
      "+T = TypeVar('T', bound=nn.Module)\n",
      "+\n",
      " \n",
      " @RunnerRegistry.register_()\n",
      "-class BaseRunner(StateDictMixin):\n",
      "+class BaseRunner(StateDictMixin, Generic[T]):\n",
      " \n",
      "     def __init__(\n",
      "         self,\n",
      "@@ -75,11 +77,19 @@ class BaseRunner(StateDictMixin):\n",
      "         return self._iter\n",
      " \n",
      "     @property\n",
      "-    def strategy(self) -> 'BaseStrategy':\n",
      "+    def strategy(self) -> 'BaseStrategy[T]':\n",
      "         return self._strategy\n",
      " \n",
      "     @property\n",
      "-    def dataloader(self) -> torch.utils.data.DataLoader:\n",
      "+    def dataset(self) -> Dataset:\n",
      "+        return self._dataset\n",
      "+\n",
      "+    @property\n",
      "+    def model(self) -> T:\n",
      "+        return self._model\n",
      "+\n",
      "+    @property\n",
      "+    def dataloader(self) -> DataLoader:\n",
      "         return self._dataloader\n",
      " \n",
      "     @property\n",
      "@@ -104,11 +114,31 @@ class BaseRunner(StateDictMixin):\n",
      "         strategy: Config,\n",
      "         **kwargs,\n",
      "     ) -> None:\n",
      "-        self._strategy: 'BaseStrategy' = StrategyRegistry.build(\n",
      "+        self._strategy: 'BaseStrategy[T]' = StrategyRegistry.build(\n",
      "             strategy,\n",
      "             runner=self,\n",
      "         )\n",
      " \n",
      "+    def _build_dataset(self, *args, dataset: Config, **kwargs) -> None:\n",
      "+        self._dataset: Dataset = DatasetRegistry.build(dataset)\n",
      "+\n",
      "+    def _build_model(\n",
      "+        self,\n",
      "+        *args,\n",
      "+        model: Config,\n",
      "+        map_model: Config | None = None,\n",
      "+        wrap_model: Config | None = None,\n",
      "+        **kwargs,\n",
      "+    ) -> None:\n",
      "+        if map_model is None:\n",
      "+            map_model = Config()\n",
      "+        if wrap_model is None:\n",
      "+            wrap_model = Config()\n",
      "+        model_ = self._strategy.build_model(model)\n",
      "+        model_ = self._strategy.map_model(model_, map_model)\n",
      "+        model_ = self._strategy.wrap_model(model_, wrap_model)\n",
      "+        self._model = model_\n",
      "+\n",
      "     def _build_dataloader(self, *args, dataloader: Config, **kwargs) -> None:\n",
      "         \"\"\"Build the dataloader.\n",
      " \n",
      "@@ -116,13 +146,12 @@ class BaseRunner(StateDictMixin):\n",
      "             dataloader: dataloader config.\n",
      "         \"\"\"\n",
      "         dataloader = dataloader.copy()\n",
      "-        dataloader.dataset = DatasetRegistry.build(dataloader.dataset)\n",
      " \n",
      "         sampler = dataloader.pop('sampler', None)\n",
      "         if sampler is not None:\n",
      "             dataloader.sampler = SamplerRegistry.build(\n",
      "                 sampler,\n",
      "-                dataset=dataloader.dataset,\n",
      "+                dataset=self._dataset,\n",
      "             )\n",
      " \n",
      "         batch_sampler = dataloader.pop('batch_sampler', None)\n",
      "@@ -136,7 +165,7 @@ class BaseRunner(StateDictMixin):\n",
      "         if collate_fn is not None:\n",
      "             dataloader.collate_fn = CollateRegistry.build(collate_fn)\n",
      " \n",
      "-        self._dataloader = torch.utils.data.DataLoader(**dataloader)\n",
      "+        self._dataloader = DataLoader(self._dataset, **dataloader)\n",
      " \n",
      "     def _build_callbacks(\n",
      "         self,\n",
      "@@ -189,6 +218,8 @@ class BaseRunner(StateDictMixin):\n",
      " \n",
      "     def _build(self, *args, **kwargs) -> None:\n",
      "         self._build_strategy(*args, **kwargs)\n",
      "+        self._build_dataset(*args, **kwargs)\n",
      "+        self._build_model(*args, **kwargs)\n",
      "         self._build_dataloader(*args, **kwargs)\n",
      "         self._build_callbacks(*args, **kwargs)\n",
      "         self._build_work_dir(*args, **kwargs)\n",
      "@@ -204,7 +235,7 @@ class BaseRunner(StateDictMixin):\n",
      "         Returns:\n",
      "             Updated runtime memory.\n",
      "         \"\"\"\n",
      "-        return self._strategy.model(self, batch, memo, *args, **kwargs)\n",
      "+        return self._model(self, batch, memo, *args, **kwargs)\n",
      " \n",
      "     def _run(self, memo: Memo) -> Memo:\n",
      "         dataloader = memo['dataloader']\n",
      "diff --git a/todd/runners/strategies/base.py b/todd/runners/strategies/base.py\n",
      "index cba1015..a97d30f 100644\n",
      "--- a/todd/runners/strategies/base.py\n",
      "+++ b/todd/runners/strategies/base.py\n",
      "@@ -20,49 +20,32 @@ class BaseStrategy(RunnerHolderMixin, StateDictMixin, Generic[T]):\n",
      "     def __init__(\n",
      "         self,\n",
      "         *args,\n",
      "-        model: Config,\n",
      "-        map_model: Config | None = None,\n",
      "-        wrap_model: Config | None = None,\n",
      "+        setup: Config | None = None,\n",
      "         **kwargs,\n",
      "     ) -> None:\n",
      "         super().__init__(*args, **kwargs)\n",
      "-        self._build_model(model, map_model, wrap_model)\n",
      "+        if setup is None:\n",
      "+            setup = Config()\n",
      "+        self.setup(setup)\n",
      " \n",
      "-    def _build_model(\n",
      "-        self,\n",
      "-        config: Config,\n",
      "-        map_config: Config | None,\n",
      "-        wrap_config: Config | None,\n",
      "-    ) -> None:\n",
      "-        model = ModelRegistry.build(config)\n",
      "-        model = self.map_model(model, map_config)\n",
      "-        model = self.wrap_model(model, wrap_config)\n",
      "-        self._model = model\n",
      "+    def setup(self, config: Config) -> None:\n",
      "+        pass\n",
      " \n",
      "-    def map_model(\n",
      "-        self,\n",
      "-        model: nn.Module,\n",
      "-        config: Config | None = None,\n",
      "-    ) -> nn.Module:\n",
      "-        if config is None:\n",
      "-            config = Config()\n",
      "+    def build_model(self, config: Config) -> nn.Module:\n",
      "+        return ModelRegistry.build(config)\n",
      "+\n",
      "+    def map_model(self, model: nn.Module, config: Config) -> nn.Module:\n",
      "         return model\n",
      " \n",
      "-    def wrap_model(self, model: nn.Module, config: Config | None = None) -> T:\n",
      "-        if config is None:\n",
      "-            config = Config()\n",
      "+    def wrap_model(self, model: nn.Module, config: Config) -> T:\n",
      "         return cast(T, model)\n",
      " \n",
      "     def build_optimizer(self, config: Config) -> torch.optim.Optimizer:\n",
      "         return OptimizerRegistry.build(config, model=self.module)\n",
      " \n",
      "-    @property\n",
      "-    def model(self) -> T:\n",
      "-        return self._model\n",
      "-\n",
      "     @property\n",
      "     def module(self) -> nn.Module:\n",
      "-        return self._model\n",
      "+        return self._runner.model\n",
      " \n",
      "     def model_state_dict(self, *args, **kwargs) -> dict[str, Any]:\n",
      "         return self.module.state_dict(*args, **kwargs)\n",
      "diff --git a/todd/runners/strategies/cuda.py b/todd/runners/strategies/cuda.py\n",
      "index 07114c7..5073530 100644\n",
      "--- a/todd/runners/strategies/cuda.py\n",
      "+++ b/todd/runners/strategies/cuda.py\n",
      "@@ -18,19 +18,8 @@ T = TypeVar('T', bound=nn.Module)\n",
      " @StrategyRegistry.register_()\n",
      " class CUDAStrategy(BaseStrategy[T]):\n",
      " \n",
      "-    def __init__(\n",
      "-        self,\n",
      "-        *args,\n",
      "-        setup: Config | None = None,\n",
      "-        **kwargs,\n",
      "-    ) -> None:\n",
      "+    def setup(self, config: Config) -> None:\n",
      "         assert Store.CUDA\n",
      "-        if setup is None:\n",
      "-            setup = Config()\n",
      "-        self._setup(setup)\n",
      "-        super().__init__(*args, **kwargs)\n",
      "-\n",
      "-    def _setup(self, config: Config) -> None:\n",
      "         if not dist.is_initialized():\n",
      "             init_process_group = config.get(\n",
      "                 'init_process_group',\n",
      "@@ -39,10 +28,6 @@ class CUDAStrategy(BaseStrategy[T]):\n",
      "             dist.init_process_group(**init_process_group)\n",
      "         torch.cuda.set_device(get_local_rank() % torch.cuda.device_count())\n",
      " \n",
      "-    def map_model(\n",
      "-        self,\n",
      "-        model: nn.Module,\n",
      "-        config: Config | None = None,\n",
      "-    ) -> nn.Module:\n",
      "+    def map_model(self, model: nn.Module, config: Config) -> nn.Module:\n",
      "         model = super().map_model(model, config)\n",
      "         return model.cuda()\n",
      "diff --git a/todd/runners/strategies/ddp.py b/todd/runners/strategies/ddp.py\n",
      "index 287cd97..3367c2a 100644\n",
      "--- a/todd/runners/strategies/ddp.py\n",
      "+++ b/todd/runners/strategies/ddp.py\n",
      "@@ -16,13 +16,11 @@ T = TypeVar('T', bound=DDP)\n",
      " @StrategyRegistry.register_()\n",
      " class DDPStrategy(CUDAStrategy[T]):\n",
      " \n",
      "-    def wrap_model(self, model: nn.Module, config: Config | None = None) -> T:\n",
      "-        if config is None:\n",
      "-            config = Config()\n",
      "+    def wrap_model(self, model: nn.Module, config: Config) -> T:\n",
      "         model = super().wrap_model(model, config)\n",
      "         model = DDP(model, **config)\n",
      "         return cast(T, model)\n",
      " \n",
      "     @property\n",
      "     def module(self) -> nn.Module:\n",
      "-        return self._model.module\n",
      "+        return self._runner.model.module\n",
      "diff --git a/todd/runners/strategies/fsdp.py b/todd/runners/strategies/fsdp.py\n",
      "index 04aa818..fa71105 100644\n",
      "--- a/todd/runners/strategies/fsdp.py\n",
      "+++ b/todd/runners/strategies/fsdp.py\n",
      "@@ -19,22 +19,20 @@ T = TypeVar('T', bound=FSDP)\n",
      " @StrategyRegistry.register_()\n",
      " class FSDPStrategy(CUDAStrategy[T]):\n",
      " \n",
      "-    def wrap_model(self, model: nn.Module, config: Config | None = None) -> T:\n",
      "-        if config is None:\n",
      "-            config = Config()\n",
      "+    def wrap_model(self, model: nn.Module, config: Config) -> T:\n",
      "         model = super().wrap_model(model, config)\n",
      "         model = FSDP(model, **config)\n",
      "         return cast(T, model)\n",
      " \n",
      "     @property\n",
      "     def module(self) -> nn.Module:\n",
      "-        return self._model.module\n",
      "+        return self._runner.model.module\n",
      " \n",
      "     def build_optimizer(self, config: Config) -> torch.optim.Optimizer:\n",
      "-        return OptimizerRegistry.build(config, model=self._model)\n",
      "+        return OptimizerRegistry.build(config, model=self._runner.model)\n",
      " \n",
      "     def model_state_dict(self, *args, **kwargs) -> dict[str, Any]:\n",
      "-        return self._model.state_dict(*args, **kwargs)\n",
      "+        return self._runner.model.state_dict(*args, **kwargs)\n",
      " \n",
      "     def load_model_state_dict(\n",
      "         self,\n",
      "@@ -42,14 +40,15 @@ class FSDPStrategy(CUDAStrategy[T]):\n",
      "         *args,\n",
      "         **kwargs,\n",
      "     ) -> None:\n",
      "-        self._model.load_state_dict(state_dict, *args, **kwargs)\n",
      "+        self._runner.model.load_state_dict(state_dict, *args, **kwargs)\n",
      " \n",
      "     def optim_state_dict(\n",
      "         self,\n",
      "         *args,\n",
      "         **kwargs,\n",
      "     ) -> dict[str, Any]:\n",
      "-        return FSDP.full_optim_state_dict(self._model, self.trainer.optimizer)\n",
      "+        trainer = self.trainer\n",
      "+        return FSDP.full_optim_state_dict(trainer.model, trainer.optimizer)\n",
      " \n",
      "     def load_optim_state_dict(\n",
      "         self,\n",
      "@@ -58,8 +57,9 @@ class FSDPStrategy(CUDAStrategy[T]):\n",
      "         **kwargs,\n",
      "     ) -> None:\n",
      "         state_dict = dict(state_dict)\n",
      "+        trainer = self.trainer\n",
      "         sharded_state_dict = FSDP.scatter_full_optim_state_dict(\n",
      "             state_dict,\n",
      "-            self._model,\n",
      "+            trainer.model,\n",
      "         )\n",
      "-        self.trainer.optimizer.load_state_dict(sharded_state_dict)\n",
      "+        trainer.optimizer.load_state_dict(sharded_state_dict)\n",
      "diff --git a/todd/runners/trainer.py b/todd/runners/trainer.py\n",
      "index 5030f4c..1515561 100644\n",
      "--- a/todd/runners/trainer.py\n",
      "+++ b/todd/runners/trainer.py\n",
      "@@ -32,7 +32,7 @@ class Trainer(BaseRunner):\n",
      "         self._build_optimizer(*args, **kwargs)\n",
      " \n",
      "     def _setup(self) -> Memo:\n",
      "-        self._strategy.model.train()\n",
      "+        self._model.train()\n",
      "         return super()._setup()\n",
      " \n",
      "     def state_dict(self, *args, **kwargs) -> dict[str, Any]:\n",
      "diff --git a/todd/runners/validator.py b/todd/runners/validator.py\n",
      "index 820b568..2f96ed7 100644\n",
      "--- a/todd/runners/validator.py\n",
      "+++ b/todd/runners/validator.py\n",
      "@@ -13,7 +13,7 @@ from .types import Memo\n",
      " class Validator(BaseRunner):\n",
      " \n",
      "     def _setup(self) -> Memo:\n",
      "-        self._strategy.model.eval()\n",
      "+        self._model.eval()\n",
      "         return super()._setup()\n",
      " \n",
      "     @torch.no_grad()\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"git_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(type=\"GitCallback\", diff='HEAD -- \":(exclude)*.ipynb\"'),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/git_callback/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:26,370 47495:140704501939840][base.py:56 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:26,375 47495:140704501939840][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([10,  3]), 'y': tensor([20,  6])} weight=0.000 loss=13.000\n",
      "[2024-02-15 14:18:26,377 47495:140704501939840][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.032 loss=13.773\n",
      "[2024-02-15 14:18:26,379 47495:140704501939840][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.067 loss=2.899\n",
      "[2024-02-15 14:18:26,381 47495:140704501939840][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([9, 7]), 'y': tensor([18, 14])} weight=0.075 loss=15.400\n",
      "[2024-02-15 14:18:26,384 47495:140704501939840][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.115 loss=8.483\n",
      "[2024-02-15 14:18:26,386 47495:140704501939840][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([10,  3]), 'y': tensor([20,  6])} weight=0.137 loss=12.106\n",
      "[2024-02-15 14:18:26,388 47495:140704501939840][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.170 loss=12.810\n",
      "[2024-02-15 14:18:26,390 47495:140704501939840][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.205 loss=2.693\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"optimize_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:26,405 47495:140704501939840][base.py:56 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:26,408 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.000 loss=12.000 lr=['1.667e-03']\n",
      "[2024-02-15 14:18:26,409 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.010 loss=6.965 lr=['2.333e-03']\n",
      "[2024-02-15 14:18:26,412 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.018 loss=12.882 lr=['3.000e-03']\n",
      "[2024-02-15 14:18:26,414 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.038 loss=6.868 lr=['3.667e-03']\n",
      "[2024-02-15 14:18:26,416 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([9, 7]), 'y': tensor([18, 14])} weight=0.050 loss=15.596 lr=['4.333e-03']\n",
      "[2024-02-15 14:18:26,417 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.085 loss=11.489 lr=['5.000e-03']\n",
      "[2024-02-15 14:18:26,419 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.115 loss=6.597 lr=['5.000e-03']\n",
      "[2024-02-15 14:18:26,421 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.133 loss=12.138 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=5),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:26,436 47495:140704501939840][base.py:56 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:26,438 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "[2024-02-15 14:18:26,440 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.000 loss=7.000 lr=['1.667e-03']\n",
      "[2024-02-15 14:18:26,442 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.006 loss=2.991 lr=['1.667e-03']\n",
      "[2024-02-15 14:18:26,444 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [2/5]\n",
      "[2024-02-15 14:18:26,446 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.008 loss=2.987 lr=['2.778e-03']\n",
      "[2024-02-15 14:18:26,448 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/10] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.012 loss=6.956 lr=['2.778e-03']\n",
      "[2024-02-15 14:18:26,449 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [3/5]\n",
      "[2024-02-15 14:18:26,452 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/10] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.022 loss=3.956 lr=['3.889e-03']\n",
      "[2024-02-15 14:18:26,454 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/10] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.030 loss=5.910 lr=['3.889e-03']\n",
      "[2024-02-15 14:18:26,455 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [4/5]\n",
      "[2024-02-15 14:18:26,458 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/10] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.042 loss=3.917 lr=['5.000e-03']\n",
      "[2024-02-15 14:18:26,460 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/10] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.052 loss=5.845 lr=['5.000e-03']\n",
      "[2024-02-15 14:18:26,461 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [5/5]\n",
      "[2024-02-15 14:18:26,464 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [9/10] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.067 loss=2.900 lr=['5.000e-03']\n",
      "[2024-02-15 14:18:26,466 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [10/10] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.074 loss=6.740 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=4),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=3),\n",
    "            by_epoch=True,\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=5,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-15 14:18:26,481 47495:140704501939840][lr.py:93 todd.IterBasedTrainer.lr_scale_callback _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-02-15 14:18:26,484 47495:140704501939840][base.py:56 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:26,487 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.000 loss=19.000\n",
      "[2024-02-15 14:18:26,490 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.095 loss=11.430\n",
      "[2024-02-15 14:18:26,493 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.155 loss=6.457\n",
      "[2024-02-15 14:18:26,495 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.190 loss=4.525\n",
      "[2024-02-15 14:18:26,498 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.215 loss=10.710\n",
      "[2024-02-15 14:18:26,502 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.275 loss=16.388\n",
      "[2024-02-15 14:18:26,504 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.370 loss=9.780\n",
      "[2024-02-15 14:18:26,505 47495:140704501939840][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.430 loss=5.495\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_scale_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScaleCallback\",\n",
    "            lr_scaler=dict(base_batch_size=1),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:26,529 47495:140704501939840][base.py:56 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:26,531 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.000 loss=12.000\n",
      "[2024-02-15 14:18:26,532 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_1\n",
      "[2024-02-15 14:18:26,538 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.030 loss=16.745\n",
      "[2024-02-15 14:18:26,539 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-02-15 14:18:26,543 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.072 loss=3.855\n",
      "[2024-02-15 14:18:26,544 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_3\n",
      "[2024-02-15 14:18:26,549 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.082 loss=7.670\n",
      "[2024-02-15 14:18:26,550 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-02-15 14:18:26,554 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.102 loss=13.282\n",
      "[2024-02-15 14:18:26,555 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-02-15 14:18:26,560 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.137 loss=11.175\n",
      "[2024-02-15 14:18:26,561 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-15 14:18:26,565 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.167 loss=15.576\n",
      "[2024-02-15 14:18:26,566 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-02-15 14:18:26,570 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.210 loss=3.580\n",
      "[2024-02-15 14:18:26,572 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_5\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_7\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_8\u001b[0m\n",
      "\n",
      "12 directories, 40 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-15 14:18:27,074 47495:140704501939840][checkpoint.py:54 todd.IterBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-02-15 14:18:27,077 47495:140704501939840][base.py:65 todd.IterBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-15 14:18:27,079 47495:140704501939840][base.py:56 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:27,082 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.137 loss=11.175\n",
      "[2024-02-15 14:18:27,083 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-15 14:18:27,088 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.167 loss=10.995\n",
      "[2024-02-15 14:18:27,089 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-02-15 14:18:27,094 47495:140704501939840][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.197 loss=2.704\n",
      "[2024-02-15 14:18:27,095 47495:140704501939840][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5o2j_0hq/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {0: {'momentum_buffer': None}}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 5}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.1375))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_5 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_5'\n",
    "    for f in iter_5.glob('*.pth'):\n",
    "        print(f'{f.name}:')\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()\n",
    "\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_5),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:27,138 47495:140704501939840][base.py:56 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:27,140 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-15 14:18:27,142 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.000 loss=14.000\n",
      "[2024-02-15 14:18:27,145 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.035 loss=11.790\n",
      "[2024-02-15 14:18:27,146 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-02-15 14:18:27,150 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])} weight=0.065 loss=16.448\n",
      "[2024-02-15 14:18:27,152 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.108 loss=5.677\n",
      "[2024-02-15 14:18:27,153 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-02-15 14:18:27,157 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.123 loss=5.633\n",
      "[2024-02-15 14:18:27,158 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-15 14:18:27,161 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([3, 9]), 'y': tensor([ 6, 18])} weight=0.138 loss=11.175\n",
      "[2024-02-15 14:18:27,162 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-15 14:18:27,166 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.168 loss=8.246\n",
      "[2024-02-15 14:18:27,169 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.190 loss=7.240\n",
      "[2024-02-15 14:18:27,170 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-02-15 14:18:27,174 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.210 loss=9.845\n",
      "[2024-02-15 14:18:27,176 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([7, 8]), 'y': tensor([14, 16])} weight=0.237 loss=13.219\n",
      "[2024-02-15 14:18:27,177 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-15 14:18:27,181 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-15 14:18:27,183 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.275 loss=7.762\n",
      "[2024-02-15 14:18:27,185 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([6, 5]), 'y': tensor([12, 10])} weight=0.298 loss=9.364\n",
      "[2024-02-15 14:18:27,186 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-15 14:18:27,190 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.325 loss=7.537\n",
      "[2024-02-15 14:18:27,193 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.348 loss=15.699\n",
      "[2024-02-15 14:18:27,194 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-15 14:18:27,199 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.395 loss=5.617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_12\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_14\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_14\u001b[0m\n",
      "\n",
      "11 directories, 35 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-15 14:18:27,627 47495:140704501939840][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-02-15 14:18:27,631 47495:140704501939840][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-15 14:18:27,632 47495:140704501939840][base.py:56 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:27,633 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-15 14:18:27,637 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.210 loss=8.055\n",
      "[2024-02-15 14:18:27,639 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.232 loss=4.419\n",
      "[2024-02-15 14:18:27,640 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-15 14:18:27,644 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-15 14:18:27,646 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.245 loss=9.653\n",
      "[2024-02-15 14:18:27,648 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.272 loss=12.956\n",
      "[2024-02-15 14:18:27,649 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-15 14:18:27,653 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.310 loss=9.295\n",
      "[2024-02-15 14:18:27,655 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.337 loss=7.481\n",
      "[2024-02-15 14:18:27,657 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-15 14:18:27,660 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.360 loss=7.380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-15 14:18:28,081 47495:140704501939840][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-15 14:18:28,085 47495:140704501939840][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-15 14:18:28,086 47495:140704501939840][base.py:56 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:28,087 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-15 14:18:28,090 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.245 loss=15.795\n",
      "[2024-02-15 14:18:28,093 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.290 loss=8.550\n",
      "[2024-02-15 14:18:28,094 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-15 14:18:28,098 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.315 loss=7.582\n",
      "[2024-02-15 14:18:28,101 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.338 loss=7.481\n",
      "[2024-02-15 14:18:28,101 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp1ohmf3kk/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-15 14:18:28,106 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.360 loss=7.380\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=2),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_8 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_8'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_8),\n",
    "        )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    iter_10 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_10'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_10),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:28,135 47495:140704501939840][base.py:56 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:28,137 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-15 14:18:28,139 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.000 loss=13.000\n",
      "[2024-02-15 14:18:28,142 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.032 loss=12.789\n",
      "[2024-02-15 14:18:28,144 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.065 loss=18.382\n",
      "[2024-02-15 14:18:28,147 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.112 loss=2.831\n",
      "[2024-02-15 14:18:28,149 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.120 loss=6.580\n",
      "[2024-02-15 14:18:28,150 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9a6hzraa/checkpoint_callback/checkpoints/epoch_1\n",
      "[2024-02-15 14:18:28,153 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-15 14:18:28,156 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.138 loss=7.450\n",
      "[2024-02-15 14:18:28,158 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.157 loss=11.055\n",
      "[2024-02-15 14:18:28,160 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.188 loss=10.875\n",
      "[2024-02-15 14:18:28,162 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.218 loss=9.804\n",
      "[2024-02-15 14:18:28,163 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.245 loss=10.530\n",
      "[2024-02-15 14:18:28,165 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9a6hzraa/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-02-15 14:18:28,168 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-15 14:18:28,170 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.275 loss=10.350\n",
      "[2024-02-15 14:18:28,172 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.305 loss=9.323\n",
      "[2024-02-15 14:18:28,174 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.333 loss=8.337\n",
      "[2024-02-15 14:18:28,176 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([8, 7]), 'y': tensor([16, 14])} weight=0.358 loss=12.319\n",
      "[2024-02-15 14:18:28,179 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.395 loss=5.618\n",
      "[2024-02-15 14:18:28,180 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9a6hzraa/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9a6hzraa\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36mepoch_3\u001b[0m\n",
      "\n",
      "7 directories, 15 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-15 14:18:28,617 47495:140704501939840][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9a6hzraa/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-02-15 14:18:28,621 47495:140704501939840][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-15 14:18:28,622 47495:140704501939840][base.py:56 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:28,623 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-15 14:18:28,627 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.275 loss=12.075\n",
      "[2024-02-15 14:18:28,629 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.310 loss=7.605\n",
      "[2024-02-15 14:18:28,631 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.333 loss=14.174\n",
      "[2024-02-15 14:18:28,633 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.375 loss=8.938\n",
      "[2024-02-15 14:18:28,636 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.403 loss=3.195\n",
      "[2024-02-15 14:18:28,637 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9a6hzraa/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'epoch_2'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(epoch_2),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register_()\n",
    "class FaultyValidator(todd.runners.Validator):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError(\"faulty runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:28,676 47495:140704501939840][base.py:56 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-02-15 14:18:28,678 47495:140704501939840][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x158727810>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 253, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_47495/1715875531.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2024-02-15 14:18:28,676 47495:140704501939840][base.py:56 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2024-02-15 14:18:28,678 47495:140704501939840][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x158727810>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 253, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_47495/1715875531.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='FaultyValidator',\n",
    "    name='monitor_callback',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(type='MonitorCallback'),\n",
    "        dict(type='LogCallback', interval=5, with_file_handler=True),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/monitor_callback/*.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:29,025 47495:140704501939840][base.py:56 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:29,027 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-15 14:18:29,030 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([6, 4]), 'y': tensor([12,  8])} weight=0.000 loss=10.000\n",
      "[2024-02-15 14:18:29,033 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.025 loss=10.863\n",
      "[2024-02-15 14:18:29,036 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.052 loss=12.659\n",
      "[2024-02-15 14:18:29,038 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.085 loss=3.830\n",
      "[2024-02-15 14:18:29,040 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.095 loss=16.192\n",
      "[2024-02-15 14:18:29,041 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5vkjt2_j/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-02-15 14:18:29,046 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-15 14:18:29,049 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.137 loss=16.763\n",
      "[2024-02-15 14:18:29,051 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.182 loss=7.270\n",
      "[2024-02-15 14:18:29,053 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.202 loss=11.684\n",
      "[2024-02-15 14:18:29,055 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.235 loss=11.472\n",
      "[2024-02-15 14:18:29,057 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.267 loss=2.599\n",
      "[2024-02-15 14:18:29,059 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5vkjt2_j/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-02-15 14:18:29,062 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-15 14:18:29,065 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.275 loss=12.075\n",
      "[2024-02-15 14:18:29,067 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.310 loss=10.140\n",
      "[2024-02-15 14:18:29,069 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([8, 3]), 'y': tensor([16,  6])} weight=0.340 loss=9.130\n",
      "[2024-02-15 14:18:29,071 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.367 loss=8.163\n",
      "[2024-02-15 14:18:29,073 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.392 loss=6.430\n",
      "[2024-02-15 14:18:29,074 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5vkjt2_j/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-15 14:18:29,518 47495:140704501939840][base.py:56 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-15 14:18:29,519 47495:140704501939840][base.py:80 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5vkjt2_j/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-02-15 14:18:29,522 47495:140704501939840][base.py:65 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "[2024-02-15 14:18:29,523 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-15 14:18:29,526 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.275 loss=10.350\n",
      "[2024-02-15 14:18:29,528 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.305 loss=5.932\n",
      "[2024-02-15 14:18:29,530 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.322 loss=11.743\n",
      "[2024-02-15 14:18:29,532 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.357 loss=9.855\n",
      "[2024-02-15 14:18:29,535 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.387 loss=8.062\n",
      "[2024-02-15 14:18:29,536 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5vkjt2_j/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-02-15 14:18:29,539 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-15 14:18:29,542 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.412 loss=7.144\n",
      "[2024-02-15 14:18:29,545 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.435 loss=11.737\n",
      "[2024-02-15 14:18:29,548 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([10,  4]), 'y': tensor([20,  8])} weight=0.472 loss=10.693\n",
      "[2024-02-15 14:18:29,550 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.507 loss=2.985\n",
      "[2024-02-15 14:18:29,551 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.517 loss=9.636\n",
      "[2024-02-15 14:18:29,552 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5vkjt2_j/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-02-15 14:18:29,556 47495:140704501939840][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-15 14:18:29,558 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.550 loss=6.525\n",
      "[2024-02-15 14:18:29,561 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.572 loss=13.561\n",
      "[2024-02-15 14:18:29,563 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.620 loss=4.830\n",
      "[2024-02-15 14:18:29,565 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.637 loss=6.812\n",
      "[2024-02-15 14:18:29,567 47495:140704501939840][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.662 loss=6.688\n",
      "[2024-02-15 14:18:29,568 47495:140704501939840][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5vkjt2_j/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"strategy_load_model_from\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = (pathlib.Path(work_dirs) / 'strategy_load_model_from' / 'checkpoints' / 'epoch_2' / 'model.pth')\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.strategy.load_model_from(epoch_2)\n",
    "    runner.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
