{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd_ai 0.5.1\n",
      "Uninstalling todd_ai-0.5.1:\n",
      "  Successfully uninstalled todd_ai-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install --extra-index-url https://pypi.org/simple .. > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import todd\n",
    "from todd.runners import Memo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.registries.ModelRegistry.register_()\n",
    "class RunnerModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def _forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        runner: todd.runners.BaseRunner,\n",
    "        batch,\n",
    "        memo: Memo,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> Memo:\n",
    "        log: dict[str, Any] | None = memo.get(\"log\")\n",
    "        y = self._forward(batch[\"x\"])\n",
    "        loss = F.l1_loss(y, batch[\"y\"])\n",
    "        memo[\"loss\"] = loss\n",
    "        if log is not None:\n",
    "            log[\"batch\"] = str(batch)\n",
    "            log[\"weight\"] = f\"{self._weight.item():.3f}\"\n",
    "            log[\"loss\"] = f\"{loss:.3f}\"\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.registries.DatasetRegistry.register_()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:12,418 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:12,419 11393:140704352767616][base.py:69 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpeb8wo07n\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    logger=dict(),\n",
    "    callbacks=[],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:13,840 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:13,844 11393:140704352767616][base.py:69 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:13,855 11393:140704352767616][log.py:91 todd.Validator.validator after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:13,859 11393:140704352767616][log.py:91 todd.Validator.validator after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-08-10 09:10:13,864 11393:140704352767616][log.py:91 todd.Validator.validator after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-08-10 09:10:13,868 11393:140704352767616][log.py:91 todd.Validator.validator after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9fnrnrsf\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[dict(type='LogCallback', interval=5)],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:15,493 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:15,497 11393:140704352767616][base.py:69 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:15,497 11393:140704352767616][base.py:69 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:15,507 11393:140704352767616][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [1/8] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:15,510 11393:140704352767616][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [2/8] batch={'x': tensor([10,  6]), 'y': tensor([20, 12])} weight=0.000 loss=16.000\n",
      "[2024-08-10 09:10:15,514 11393:140704352767616][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [3/8] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.000 loss=8.000\n",
      "[2024-08-10 09:10:15,516 11393:140704352767616][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [4/8] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.000 loss=13.000\n",
      "[2024-08-10 09:10:15,518 11393:140704352767616][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [5/8] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.000 loss=8.000\n",
      "[2024-08-10 09:10:15,521 11393:140704352767616][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [6/8] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.000 loss=9.000\n",
      "[2024-08-10 09:10:15,523 11393:140704352767616][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [7/8] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.000 loss=15.000\n",
      "[2024-08-10 09:10:15,525 11393:140704352767616][log.py:91 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [8/8] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.000 loss=10.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"iter_based_trainer\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:15,564 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:15,570 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:15,570 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:15,576 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-08-10 09:10:15,592 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [1/15] batch={'x': tensor([6, 4]), 'y': tensor([12,  8])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:15,594 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [2/15] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.000 loss=13.000\n",
      "[2024-08-10 09:10:15,598 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [3/15] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.000 loss=11.000\n",
      "[2024-08-10 09:10:15,601 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [4/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.000 loss=5.000\n",
      "[2024-08-10 09:10:15,604 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [5/15] batch={'x': tensor([9, 7]), 'y': tensor([18, 14])} weight=0.000 loss=16.000\n",
      "[2024-08-10 09:10:15,606 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-08-10 09:10:15,610 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [6/15] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.000 loss=4.000\n",
      "[2024-08-10 09:10:15,612 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [7/15] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.000 loss=13.000\n",
      "[2024-08-10 09:10:15,615 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [8/15] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.000 loss=14.000\n",
      "[2024-08-10 09:10:15,618 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [9/15] batch={'x': tensor([9, 7]), 'y': tensor([18, 14])} weight=0.000 loss=16.000\n",
      "[2024-08-10 09:10:15,620 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [10/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.000 loss=8.000\n",
      "[2024-08-10 09:10:15,621 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-08-10 09:10:15,624 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.000 loss=13.000\n",
      "[2024-08-10 09:10:15,627 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [12/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.000 loss=6.000\n",
      "[2024-08-10 09:10:15,629 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [13/15] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.000 loss=6.000\n",
      "[2024-08-10 09:10:15,635 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [14/15] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.000 loss=15.000\n",
      "[2024-08-10 09:10:15,638 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [15/15] batch={'x': tensor([7, 8]), 'y': tensor([14, 16])} weight=0.000 loss=15.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"epoch_based_trainer\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:15,660 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "[2024-08-10 09:10:16,644 11393:140704352767616][log.py:53 todd.Validator.log_callback bind] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.10.0\n",
      "todd_version: 0.5.1\n",
      "cuda_home: None\n",
      "git_commit_id: 8cc8a4f\n",
      "git_status: \n",
      "M todd/runners/callbacks/checkpoint.py\n",
      " M todd/runners/callbacks/tensorboard.py\n",
      " M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-08-10 09:10:16,646 11393:140704352767616][base.py:69 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:16,650 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:16,653 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-08-10 09:10:16,656 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-08-10 09:10:16,659 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:16,676 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:16,679 11393:140704352767616][base.py:69 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:16,683 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:16,686 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-08-10 09:10:16,688 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-08-10 09:10:16,690 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpvo24ampe\u001b[0m\n",
      "└── \u001b[1;36mlog_callback\u001b[0m\n",
      "    └── 2024-08-10T09-10-16_678852-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "\u001b[2m[2024-08-10 09:10:16,679 11393:140704352767616][base.py:69 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:16,683 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:16,686 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-08-10 09:10:16,688 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-08-10 09:10:16,690 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='log_callback',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type='LogCallback',\n",
    "            interval=5,\n",
    "            with_file_handler=True,\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/log_callback/*.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:19,619 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:19,627 11393:140704352767616][base.py:69 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:19,627 11393:140704352767616][base.py:69 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:20,149 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:01 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:20,680 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:01 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-08-10 09:10:21,201 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-08-10 09:10:21,722 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:21,746 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:21,749 11393:140704352767616][base.py:69 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:21,749 11393:140704352767616][base.py:69 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:23,273 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:04 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:27,294 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:05 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-08-10 09:10:32,311 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:03 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-08-10 09:10:37,335 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"EMA_ETA\", ema=dict(decay=0.2)),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1 * min(10, runner.iter_))\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:37,352 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "[2024-08-10 09:10:37,586 11393:140704352767616][log.py:53 todd.Validator.log_callback bind] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.10.0\n",
      "todd_version: 0.5.1\n",
      "cuda_home: None\n",
      "git_commit_id: 8cc8a4f\n",
      "git_status: \n",
      "M todd/runners/callbacks/checkpoint.py\n",
      " M todd/runners/callbacks/tensorboard.py\n",
      " M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-08-10 09:10:37,588 11393:140704352767616][base.py:69 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:37,594 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:00 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:37,597 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:00 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-08-10 09:10:37,601 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-08-10 09:10:37,604 11393:140704352767616][log.py:91 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "            with_file_handler=True,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:37,622 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "[2024-08-10 09:10:37,687 11393:140704352767616][git.py:49 todd.Validator.git_callback bind] INFO: Saving git diff to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_qzztl2r/git_callback/git_diff_2024-08-10T09-10-37_687220-08-00.log\n",
      "\u001b[2m[2024-08-10 09:10:37,691 11393:140704352767616][base.py:69 todd.Validator.git_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff --git a/todd/runners/callbacks/checkpoint.py b/todd/runners/callbacks/checkpoint.py\n",
      "index f5932b5..127e509 100644\n",
      "--- a/todd/runners/callbacks/checkpoint.py\n",
      "+++ b/todd/runners/callbacks/checkpoint.py\n",
      "@@ -39,7 +39,7 @@ class CheckpointCallback(IntervalMixin[T], BaseCallback[T]):\n",
      "     def bind(self, *args, **kwargs) -> None:\n",
      "         super().bind(*args, **kwargs)\n",
      " \n",
      "-        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
      "+        self.work_dir.mkdir(parents=True, exist_ok=True)\n",
      " \n",
      "         if self.runner.auto_resume and self.latest_checkpoint_dir.exists():\n",
      "             load_from = self.latest_checkpoint_dir\n",
      "@@ -59,15 +59,15 @@ class CheckpointCallback(IntervalMixin[T], BaseCallback[T]):\n",
      "             self.runner.load_state_dict(state_dict, **self._load_state_dict)\n",
      " \n",
      "     @property\n",
      "-    def checkpoint_dir(self) -> pathlib.Path:\n",
      "+    def work_dir(self) -> pathlib.Path:\n",
      "         return self.runner.work_dir / 'checkpoints'\n",
      " \n",
      "     @property\n",
      "     def latest_checkpoint_dir(self) -> pathlib.Path:\n",
      "-        return self.checkpoint_dir / 'latest'\n",
      "+        return self._checkpoint_dir('latest')\n",
      " \n",
      "-    def _work_dir(self, name: str) -> pathlib.Path:\n",
      "-        return self.checkpoint_dir / name\n",
      "+    def _checkpoint_dir(self, name: str) -> pathlib.Path:\n",
      "+        return self.work_dir / name\n",
      " \n",
      "     def _save(self, name: str) -> None:\n",
      "         # for FSDP, all ranks should call state dict\n",
      "@@ -75,14 +75,14 @@ class CheckpointCallback(IntervalMixin[T], BaseCallback[T]):\n",
      " \n",
      "         if get_rank() != 0:\n",
      "             return\n",
      "-        work_dir = self._work_dir(name)\n",
      "-        work_dir.mkdir(parents=True, exist_ok=True)\n",
      "-        self.runner.logger.info(\"Saving state dict to %s\", work_dir)\n",
      "+        checkpoint_dir = self._checkpoint_dir(name)\n",
      "+        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
      "+        self.runner.logger.info(\"Saving state dict to %s\", checkpoint_dir)\n",
      "         for k, v in state_dict.items():\n",
      "-            torch.save(v, work_dir / f'{k}.pth')\n",
      "+            torch.save(v, checkpoint_dir / f'{k}.pth')\n",
      " \n",
      "         self.latest_checkpoint_dir.unlink(True)\n",
      "-        self.latest_checkpoint_dir.hardlink_to(name)\n",
      "+        self.latest_checkpoint_dir.symlink_to(checkpoint_dir)\n",
      " \n",
      "     def after_run_iter(self, batch, memo: Memo) -> None:\n",
      "         super().after_run_iter(batch, memo)\n",
      "diff --git a/todd/runners/callbacks/tensorboard.py b/todd/runners/callbacks/tensorboard.py\n",
      "index 4672f73..a51db77 100644\n",
      "--- a/todd/runners/callbacks/tensorboard.py\n",
      "+++ b/todd/runners/callbacks/tensorboard.py\n",
      "@@ -35,7 +35,7 @@ class TensorBoardCallback(IntervalMixin[T], BaseCallback[T]):\n",
      "         self._main_tag = main_tag\n",
      " \n",
      "     @property\n",
      "-    def log_dir(self) -> pathlib.Path:\n",
      "+    def work_dir(self) -> pathlib.Path:\n",
      "         return self.runner.work_dir / 'tensorboard'\n",
      " \n",
      "     def bind(self, *args, **kwargs) -> None:\n",
      "@@ -44,7 +44,7 @@ class TensorBoardCallback(IntervalMixin[T], BaseCallback[T]):\n",
      "         if get_rank() > 0:\n",
      "             return\n",
      "         self._summary_writer = SummaryWriter(\n",
      "-            self.log_dir,\n",
      "+            self.work_dir,\n",
      "             **self._summary_writer_config,\n",
      "         )\n",
      " \n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"git_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(type=\"GitCallback\", diff='HEAD -- \":(exclude)*.ipynb\"'),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/git_callback/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:38,951 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:38,953 11393:140704352767616][base.py:69 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,954 11393:140704352767616][optimize.py:118 todd.IterBasedTrainer.optimize_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,954 11393:140704352767616][optimize.py:126 todd.IterBasedTrainer.optimize_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,955 11393:140704352767616][optimize.py:130 todd.IterBasedTrainer.optimize_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,953 11393:140704352767616][base.py:69 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,954 11393:140704352767616][optimize.py:118 todd.IterBasedTrainer.optimize_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,954 11393:140704352767616][optimize.py:126 todd.IterBasedTrainer.optimize_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,955 11393:140704352767616][optimize.py:130 todd.IterBasedTrainer.optimize_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:38,969 11393:140704352767616][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.000 loss=9.000\n",
      "[2024-08-10 09:10:38,971 11393:140704352767616][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.022 loss=8.899\n",
      "[2024-08-10 09:10:38,973 11393:140704352767616][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.045 loss=8.797\n",
      "[2024-08-10 09:10:38,975 11393:140704352767616][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.067 loss=18.359\n",
      "[2024-08-10 09:10:38,977 11393:140704352767616][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.115 loss=8.483\n",
      "[2024-08-10 09:10:38,980 11393:140704352767616][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.137 loss=11.175\n",
      "[2024-08-10 09:10:38,982 11393:140704352767616][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.167 loss=10.995\n",
      "[2024-08-10 09:10:38,983 11393:140704352767616][log.py:91 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.197 loss=10.815\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"optimize_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:38,996 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:38,998 11393:140704352767616][base.py:69 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,998 11393:140704352767616][optimize.py:118 todd.IterBasedTrainer.lr_schedule_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,999 11393:140704352767616][optimize.py:126 todd.IterBasedTrainer.lr_schedule_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,000 11393:140704352767616][optimize.py:130 todd.IterBasedTrainer.lr_schedule_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:39,003 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.000 loss=12.000 lr=['1.667e-03']\n",
      "\u001b[2m[2024-08-10 09:10:38,998 11393:140704352767616][base.py:69 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,998 11393:140704352767616][optimize.py:118 todd.IterBasedTrainer.lr_schedule_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:38,999 11393:140704352767616][optimize.py:126 todd.IterBasedTrainer.lr_schedule_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,000 11393:140704352767616][optimize.py:130 todd.IterBasedTrainer.lr_schedule_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:39,003 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.000 loss=12.000 lr=['1.667e-03']\n",
      "[2024-08-10 09:10:39,006 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.010 loss=15.920 lr=['2.333e-03']\n",
      "[2024-08-10 09:10:39,008 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.029 loss=9.857 lr=['3.000e-03']\n",
      "[2024-08-10 09:10:39,009 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.044 loss=3.913 lr=['3.667e-03']\n",
      "[2024-08-10 09:10:39,011 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.051 loss=12.668 lr=['4.333e-03']\n",
      "[2024-08-10 09:10:39,013 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.079 loss=5.763 lr=['5.000e-03']\n",
      "[2024-08-10 09:10:39,015 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.094 loss=9.529 lr=['5.000e-03']\n",
      "[2024-08-10 09:10:39,017 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.119 loss=12.225 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=5),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:39,031 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:39,032 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,033 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.lr_schedule_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,034 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.lr_schedule_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,034 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.lr_schedule_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:39,035 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "[2024-08-10 09:10:39,038 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/10] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.000 loss=7.000 lr=['1.667e-03']\n",
      "[2024-08-10 09:10:39,039 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/10] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.006 loss=2.991 lr=['1.667e-03']\n",
      "[2024-08-10 09:10:39,040 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [2/5]\n",
      "\u001b[2m[2024-08-10 09:10:39,032 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,033 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.lr_schedule_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,034 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.lr_schedule_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,034 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.lr_schedule_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:39,035 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "[2024-08-10 09:10:39,038 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/10] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.000 loss=7.000 lr=['1.667e-03']\n",
      "[2024-08-10 09:10:39,039 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/10] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.006 loss=2.991 lr=['1.667e-03']\n",
      "[2024-08-10 09:10:39,040 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [2/5]\n",
      "[2024-08-10 09:10:39,043 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/10] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.008 loss=4.979 lr=['2.778e-03']\n",
      "[2024-08-10 09:10:39,044 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/10] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.015 loss=4.962 lr=['2.778e-03']\n",
      "[2024-08-10 09:10:39,046 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [3/5]\n",
      "[2024-08-10 09:10:39,048 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/10] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.022 loss=4.944 lr=['3.889e-03']\n",
      "[2024-08-10 09:10:39,049 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/10] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.032 loss=4.920 lr=['3.889e-03']\n",
      "[2024-08-10 09:10:39,050 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [4/5]\n",
      "[2024-08-10 09:10:39,052 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/10] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.042 loss=4.896 lr=['5.000e-03']\n",
      "[2024-08-10 09:10:39,054 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/10] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.054 loss=4.865 lr=['5.000e-03']\n",
      "[2024-08-10 09:10:39,055 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [5/5]\n",
      "[2024-08-10 09:10:39,056 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [9/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.067 loss=6.767 lr=['5.000e-03']\n",
      "[2024-08-10 09:10:39,058 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [10/10] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.084 loss=2.874 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=4),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=3),\n",
    "            by_epoch=True,\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=5,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:39,142 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "[2024-08-10 09:10:39,144 11393:140704352767616][lr.py:95 todd.IterBasedTrainer.lr_scale_callback bind] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-08-10 09:10:39,145 11393:140704352767616][base.py:69 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,146 11393:140704352767616][optimize.py:118 todd.IterBasedTrainer.lr_scale_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,146 11393:140704352767616][optimize.py:126 todd.IterBasedTrainer.lr_scale_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "[2024-08-10 09:10:39,144 11393:140704352767616][lr.py:95 todd.IterBasedTrainer.lr_scale_callback bind] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-08-10 09:10:39,145 11393:140704352767616][base.py:69 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,146 11393:140704352767616][optimize.py:118 todd.IterBasedTrainer.lr_scale_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,146 11393:140704352767616][optimize.py:126 todd.IterBasedTrainer.lr_scale_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,147 11393:140704352767616][optimize.py:130 todd.IterBasedTrainer.lr_scale_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:39,163 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.000 loss=19.000\n",
      "[2024-08-10 09:10:39,166 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.095 loss=2.858\n",
      "[2024-08-10 09:10:39,169 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([8, 6]), 'y': tensor([16, 12])} weight=0.110 loss=13.230\n",
      "[2024-08-10 09:10:39,171 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.180 loss=7.280\n",
      "[2024-08-10 09:10:39,173 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.220 loss=9.790\n",
      "[2024-08-10 09:10:39,175 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([1, 5]), 'y': tensor([ 2, 10])} weight=0.275 loss=5.175\n",
      "[2024-08-10 09:10:39,176 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.305 loss=11.017\n",
      "[2024-08-10 09:10:39,178 11393:140704352767616][log.py:91 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([7, 3]), 'y': tensor([14,  6])} weight=0.370 loss=8.150\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_scale_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScaleCallback\",\n",
    "            lr_scaler=dict(base_batch_size=1),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:39,198 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:39,200 11393:140704352767616][base.py:69 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,201 11393:140704352767616][optimize.py:118 todd.IterBasedTrainer.checkpoint_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,202 11393:140704352767616][optimize.py:126 todd.IterBasedTrainer.checkpoint_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:39,203 11393:140704352767616][optimize.py:130 todd.IterBasedTrainer.checkpoint_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:39,205 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:39,207 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_1\n",
      "[2024-08-10 09:10:39,212 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.025 loss=2.963\n",
      "[2024-08-10 09:10:39,213 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-08-10 09:10:39,216 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.032 loss=14.756\n",
      "[2024-08-10 09:10:39,217 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_3\n",
      "[2024-08-10 09:10:39,223 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.070 loss=17.370\n",
      "[2024-08-10 09:10:39,224 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-08-10 09:10:39,229 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.115 loss=8.483\n",
      "[2024-08-10 09:10:39,231 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-08-10 09:10:39,235 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.137 loss=16.763\n",
      "[2024-08-10 09:10:39,237 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-08-10 09:10:39,242 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.182 loss=12.722\n",
      "[2024-08-10 09:10:39,243 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-08-10 09:10:39,247 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.217 loss=3.565\n",
      "[2024-08-10 09:10:39,248 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_5\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_7\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_8\u001b[0m\n",
      "\n",
      "12 directories, 40 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:41,062 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "[2024-08-10 09:10:41,064 11393:140704352767616][checkpoint.py:54 todd.IterBasedTrainer.checkpoint_callback bind] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-08-10 09:10:41,067 11393:140704352767616][base.py:69 todd.IterBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-08-10 09:10:41,069 11393:140704352767616][base.py:69 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:41,070 11393:140704352767616][optimize.py:118 todd.IterBasedTrainer.checkpoint_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:41,070 11393:140704352767616][optimize.py:126 todd.IterBasedTrainer.checkpoint_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:41,071 11393:140704352767616][optimize.py:130 todd.IterBasedTrainer.checkpoint_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:41,074 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.137 loss=7.450\n",
      "[2024-08-10 09:10:41,075 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-08-10 09:10:41,082 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([8, 3]), 'y': tensor([16,  6])} weight=0.157 loss=10.134\n",
      "[2024-08-10 09:10:41,083 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-08-10 09:10:41,086 11393:140704352767616][log.py:91 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.185 loss=13.612\n",
      "[2024-08-10 09:10:41,087 11393:140704352767616][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpm_guz5jx/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {0: {'momentum_buffer': None}}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 5}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.1375))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_5 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_5'\n",
    "    for f in iter_5.glob('*.pth'):\n",
    "        print(f'{f.name}:')\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()\n",
    "\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_5),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:41,148 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:41,150 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:41,151 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:41,151 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:41,152 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:41,153 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-08-10 09:10:41,156 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([7, 3]), 'y': tensor([14,  6])} weight=0.000 loss=10.000\n",
      "[2024-08-10 09:10:41,158 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.025 loss=6.913\n",
      "[2024-08-10 09:10:41,159 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-08-10 09:10:41,163 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.042 loss=10.766\n",
      "[2024-08-10 09:10:41,166 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.070 loss=14.475\n",
      "[2024-08-10 09:10:41,167 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-08-10 09:10:41,172 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.107 loss=11.355\n",
      "[2024-08-10 09:10:41,173 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-08-10 09:10:41,175 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.137 loss=12.106\n",
      "[2024-08-10 09:10:41,176 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-08-10 09:10:41,180 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.170 loss=10.065\n",
      "[2024-08-10 09:10:41,182 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([3, 9]), 'y': tensor([ 6, 18])} weight=0.197 loss=10.815\n",
      "[2024-08-10 09:10:41,183 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-08-10 09:10:41,187 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.227 loss=6.204\n",
      "[2024-08-10 09:10:41,189 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.245 loss=10.530\n",
      "[2024-08-10 09:10:41,190 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-08-10 09:10:41,193 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-08-10 09:10:41,195 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([3, 9]), 'y': tensor([ 6, 18])} weight=0.275 loss=10.350\n",
      "[2024-08-10 09:10:41,197 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.305 loss=10.170\n",
      "[2024-08-10 09:10:41,198 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-08-10 09:10:41,201 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.335 loss=4.995\n",
      "[2024-08-10 09:10:41,203 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([7, 8]), 'y': tensor([14, 16])} weight=0.350 loss=12.375\n",
      "[2024-08-10 09:10:41,204 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-08-10 09:10:41,208 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.387 loss=8.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_12\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_14\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_14\u001b[0m\n",
      "\n",
      "11 directories, 35 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:42,981 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "[2024-08-10 09:10:42,983 11393:140704352767616][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback bind] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-08-10 09:10:42,987 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-08-10 09:10:42,988 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:42,989 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:42,990 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:42,990 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:42,992 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-08-10 09:10:42,996 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.227 loss=8.863\n",
      "[2024-08-10 09:10:42,997 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.252 loss=4.369\n",
      "[2024-08-10 09:10:42,998 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-08-10 09:10:43,001 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-08-10 09:10:43,004 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.265 loss=6.940\n",
      "[2024-08-10 09:10:43,005 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.285 loss=14.577\n",
      "[2024-08-10 09:10:43,006 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-08-10 09:10:43,010 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.327 loss=7.526\n",
      "[2024-08-10 09:10:43,011 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.350 loss=10.725\n",
      "[2024-08-10 09:10:43,012 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-08-10 09:10:43,016 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.382 loss=6.470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:44,856 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "[2024-08-10 09:10:44,858 11393:140704352767616][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback bind] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-08-10 09:10:44,862 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-08-10 09:10:44,863 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:44,864 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:44,864 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:44,865 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:44,866 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-08-10 09:10:44,858 11393:140704352767616][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback bind] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-08-10 09:10:44,862 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-08-10 09:10:44,863 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:44,864 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:44,864 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:44,865 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:44,866 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-08-10 09:10:44,869 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.265 loss=6.073\n",
      "[2024-08-10 09:10:44,871 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.282 loss=16.316\n",
      "[2024-08-10 09:10:44,872 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-08-10 09:10:44,876 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.330 loss=7.515\n",
      "[2024-08-10 09:10:44,877 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.352 loss=10.709\n",
      "[2024-08-10 09:10:44,878 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpszivugje/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-08-10 09:10:44,882 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.385 loss=5.653\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=2),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_8 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_8'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_8),\n",
    "        )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    iter_10 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_10'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_10),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:44,907 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:44,909 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:44,910 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:44,910 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:44,911 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:44,911 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-08-10 09:10:44,914 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.000 loss=12.000\n",
      "[2024-08-10 09:10:44,916 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.030 loss=11.820\n",
      "[2024-08-10 09:10:44,918 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.060 loss=10.670\n",
      "[2024-08-10 09:10:44,919 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])} weight=0.087 loss=16.256\n",
      "[2024-08-10 09:10:44,921 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.130 loss=2.805\n",
      "[2024-08-10 09:10:44,922 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8v6lb36t/checkpoint_callback/checkpoints/epoch_1\n",
      "[2024-08-10 09:10:44,925 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-08-10 09:10:44,926 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([2, 7]), 'y': tensor([ 4, 14])} weight=0.137 loss=8.381\n",
      "[2024-08-10 09:10:44,929 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.160 loss=16.560\n",
      "[2024-08-10 09:10:44,932 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.205 loss=4.488\n",
      "[2024-08-10 09:10:44,933 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.217 loss=12.477\n",
      "[2024-08-10 09:10:44,936 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([3, 6]), 'y': tensor([ 6, 12])} weight=0.252 loss=7.864\n",
      "[2024-08-10 09:10:44,937 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8v6lb36t/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-08-10 09:10:44,940 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-08-10 09:10:44,942 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.275 loss=8.625\n",
      "[2024-08-10 09:10:44,944 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.300 loss=14.450\n",
      "[2024-08-10 09:10:44,945 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([3, 9]), 'y': tensor([ 6, 18])} weight=0.343 loss=9.945\n",
      "[2024-08-10 09:10:44,947 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.373 loss=7.324\n",
      "[2024-08-10 09:10:44,949 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.395 loss=5.618\n",
      "[2024-08-10 09:10:44,950 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8v6lb36t/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8v6lb36t\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8v6lb36t/checkpoint_callback/checkpoints/epoch_3\u001b[0m\n",
      "\n",
      "7 directories, 15 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:46,765 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "[2024-08-10 09:10:46,767 11393:140704352767616][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback bind] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8v6lb36t/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-08-10 09:10:46,771 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-08-10 09:10:46,772 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:46,773 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:46,774 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:46,774 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:46,775 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-08-10 09:10:46,767 11393:140704352767616][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback bind] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8v6lb36t/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-08-10 09:10:46,771 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-08-10 09:10:46,772 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:46,773 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:46,774 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:46,774 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.checkpoint_callback before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:46,775 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-08-10 09:10:46,779 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.275 loss=12.938\n",
      "[2024-08-10 09:10:46,781 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.312 loss=10.969\n",
      "[2024-08-10 09:10:46,783 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.345 loss=4.137\n",
      "[2024-08-10 09:10:46,784 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.357 loss=9.855\n",
      "[2024-08-10 09:10:46,786 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([7, 3]), 'y': tensor([14,  6])} weight=0.387 loss=8.062\n",
      "[2024-08-10 09:10:46,787 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8v6lb36t/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'epoch_2'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(epoch_2),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.registries.RunnerRegistry.register_()\n",
    "class FaultyValidator(todd.runners.Validator):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError(\"faulty runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:46,820 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:46,822 11393:140704352767616][base.py:69 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-08-10 09:10:46,824 11393:140704352767616][monitor.py:31 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x10b33ea10>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 295, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_11393/454645826.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m[2024-08-10 09:10:46,822 11393:140704352767616][base.py:69 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-08-10 09:10:46,824 11393:140704352767616][monitor.py:31 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x10b33ea10>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 295, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_11393/454645826.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='FaultyValidator',\n",
    "    name='monitor_callback',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(type='MonitorCallback'),\n",
    "        dict(type='LogCallback', interval=5, with_file_handler=True),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/monitor_callback/*.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:48,072 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:48,074 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:48,075 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.strategy_load_model_from before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:48,076 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.strategy_load_model_from before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:48,076 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.strategy_load_model_from before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:48,077 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-08-10 09:10:48,080 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.000 loss=6.000\n",
      "[2024-08-10 09:10:48,082 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.015 loss=11.910\n",
      "[2024-08-10 09:10:48,083 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.045 loss=12.707\n",
      "[2024-08-10 09:10:48,085 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.078 loss=10.574\n",
      "[2024-08-10 09:10:48,086 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.105 loss=12.318\n",
      "[2024-08-10 09:10:48,088 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpdtuy90xh/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-08-10 09:10:48,092 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-08-10 09:10:48,095 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.138 loss=6.519\n",
      "[2024-08-10 09:10:48,097 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.155 loss=15.682\n",
      "[2024-08-10 09:10:48,098 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.198 loss=9.914\n",
      "[2024-08-10 09:10:48,100 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.225 loss=6.212\n",
      "[2024-08-10 09:10:48,101 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.243 loss=11.424\n",
      "[2024-08-10 09:10:48,102 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpdtuy90xh/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-08-10 09:10:48,105 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-08-10 09:10:48,107 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.275 loss=3.450\n",
      "[2024-08-10 09:10:48,108 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.285 loss=12.005\n",
      "[2024-08-10 09:10:48,110 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.320 loss=8.400\n",
      "[2024-08-10 09:10:48,111 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.345 loss=12.413\n",
      "[2024-08-10 09:10:48,113 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.382 loss=9.705\n",
      "[2024-08-10 09:10:48,114 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpdtuy90xh/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-10 09:10:49,897 11393:140704352767616][patches.py:214 todd dataloader_build_pre_hook] INFO: `worker_init_fn` is recommended to be 'default', instead of None.\n",
      "\u001b[2m[2024-08-10 09:10:49,900 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:49,901 11393:140704352767616][base.py:84 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpdtuy90xh/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-08-10 09:10:49,903 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-08-10 09:10:49,904 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.strategy_load_model_from before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:49,905 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.strategy_load_model_from before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:49,905 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.strategy_load_model_from before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:49,906 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "\u001b[2m[2024-08-10 09:10:49,900 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-08-10 09:10:49,901 11393:140704352767616][base.py:84 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpdtuy90xh/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-08-10 09:10:49,903 11393:140704352767616][base.py:69 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-08-10 09:10:49,904 11393:140704352767616][optimize.py:118 todd.EpochBasedTrainer.strategy_load_model_from before_run] DEBUG: Training modules\n",
      "''\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:49,905 11393:140704352767616][optimize.py:126 todd.EpochBasedTrainer.strategy_load_model_from before_run] DEBUG: Requires grad parameters\n",
      "'_weight'\u001b[m\n",
      "\u001b[2m[2024-08-10 09:10:49,905 11393:140704352767616][optimize.py:130 todd.EpochBasedTrainer.strategy_load_model_from before_run] DEBUG: Total number of requires grad parameters: 1\u001b[m\n",
      "[2024-08-10 09:10:49,906 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-08-10 09:10:49,910 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([8, 3]), 'y': tensor([16,  6])} weight=0.275 loss=9.488\n",
      "[2024-08-10 09:10:49,911 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.303 loss=4.244\n",
      "[2024-08-10 09:10:49,913 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.315 loss=11.795\n",
      "[2024-08-10 09:10:49,915 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.350 loss=9.900\n",
      "[2024-08-10 09:10:49,917 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.380 loss=10.530\n",
      "[2024-08-10 09:10:49,918 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpdtuy90xh/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-08-10 09:10:49,921 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-08-10 09:10:49,924 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.412 loss=12.700\n",
      "[2024-08-10 09:10:49,926 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.452 loss=5.416\n",
      "[2024-08-10 09:10:49,927 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.470 loss=13.005\n",
      "[2024-08-10 09:10:49,929 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.512 loss=5.206\n",
      "[2024-08-10 09:10:49,931 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.530 loss=5.880\n",
      "[2024-08-10 09:10:49,932 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpdtuy90xh/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-08-10 09:10:49,935 11393:140704352767616][log.py:97 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-08-10 09:10:49,937 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.550 loss=7.250\n",
      "[2024-08-10 09:10:49,939 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.575 loss=9.975\n",
      "[2024-08-10 09:10:49,940 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.610 loss=7.645\n",
      "[2024-08-10 09:10:49,942 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 6]), 'y': tensor([ 6, 12])} weight=0.637 loss=6.131\n",
      "[2024-08-10 09:10:49,944 11393:140704352767616][log.py:91 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.660 loss=7.370\n",
      "[2024-08-10 09:10:49,945 11393:140704352767616][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpdtuy90xh/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"strategy_load_model_from\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = (pathlib.Path(work_dirs) / 'strategy_load_model_from' / 'checkpoints' / 'epoch_2' / 'model.pth')\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.strategy.load_model_from(epoch_2)\n",
    "    runner.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
