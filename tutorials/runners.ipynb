{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install --no-build-isolation --extra-index-url https://pypi.org/simple .. > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-23 16:10:26,112 87795:140704521019008][patches.py:7 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n",
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import todd\n",
    "from todd.runners import Memo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register_()\n",
    "class RunnerModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def _forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        runner: todd.runners.BaseRunner,\n",
    "        batch,\n",
    "        memo: Memo,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> Memo:\n",
    "        log: dict[str, Any] | None = memo.get(\"log\")\n",
    "        y = self._forward(batch[\"x\"])\n",
    "        loss = F.l1_loss(y, batch[\"y\"])\n",
    "        memo[\"loss\"] = loss\n",
    "        if log is not None:\n",
    "            log[\"batch\"] = str(batch)\n",
    "            log[\"weight\"] = f\"{self._weight.item():.3f}\"\n",
    "            log[\"loss\"] = f\"{loss:.3f}\"\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register_()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:27,913 87795:140704521019008][base.py:53 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp42nx3r3u\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:28,245 87795:140704521019008][base.py:53 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:28,251 87795:140704521019008][log.py:99 todd.Validator.validator after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2023-11-23 16:10:28,254 87795:140704521019008][log.py:99 todd.Validator.validator after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2023-11-23 16:10:28,257 87795:140704521019008][log.py:99 todd.Validator.validator after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2023-11-23 16:10:28,259 87795:140704521019008][log.py:99 todd.Validator.validator after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpme50ogqq\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[dict(type='LogCallback', interval=5)],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:28,557 87795:140704521019008][base.py:53 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:28,559 87795:140704521019008][log.py:99 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [1/8] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:28,561 87795:140704521019008][log.py:99 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [2/8] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:28,563 87795:140704521019008][log.py:99 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [3/8] batch={'x': tensor([3, 9]), 'y': tensor([ 6, 18])} weight=0.000 loss=12.000\n",
      "[2023-11-23 16:10:28,565 87795:140704521019008][log.py:99 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [4/8] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.000 loss=14.000\n",
      "[2023-11-23 16:10:28,567 87795:140704521019008][log.py:99 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [5/8] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.000 loss=7.000\n",
      "[2023-11-23 16:10:28,569 87795:140704521019008][log.py:99 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [6/8] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:28,570 87795:140704521019008][log.py:99 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [7/8] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:28,572 87795:140704521019008][log.py:99 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [8/8] batch={'x': tensor([3, 9]), 'y': tensor([ 6, 18])} weight=0.000 loss=12.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"iter_based_trainer\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config, work_dir=dict(root=work_dirs)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:28,584 87795:140704521019008][base.py:53 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:28,585 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-11-23 16:10:28,587 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [1/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.000 loss=7.000\n",
      "[2023-11-23 16:10:28,588 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [2/15] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:28,590 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [3/15] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:28,591 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [4/15] batch={'x': tensor([10,  5]), 'y': tensor([20, 10])} weight=0.000 loss=15.000\n",
      "[2023-11-23 16:10:28,592 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [5/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:28,593 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-11-23 16:10:28,595 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [6/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.000 loss=18.000\n",
      "[2023-11-23 16:10:28,598 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [7/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.000 loss=10.000\n",
      "[2023-11-23 16:10:28,599 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [8/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.000 loss=5.000\n",
      "[2023-11-23 16:10:28,601 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [9/15] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:28,603 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [10/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:28,603 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-11-23 16:10:28,605 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [11/15] batch={'x': tensor([8, 7]), 'y': tensor([16, 14])} weight=0.000 loss=15.000\n",
      "[2023-11-23 16:10:28,606 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [12/15] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.000 loss=7.000\n",
      "[2023-11-23 16:10:28,607 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [13/15] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:28,609 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.000 loss=7.000\n",
      "[2023-11-23 16:10:28,610 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [15/15] batch={'x': tensor([10,  5]), 'y': tensor([20, 10])} weight=0.000 loss=15.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"epoch_based_trainer\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config, work_dir=dict(root=work_dirs)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-23 16:10:28,738 87795:140704521019008][log.py:61 todd.Validator.log_callback init] INFO: \n",
      "Platform: macOS-14.0\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0edd804\n",
      "Git status: M tutorials/runners.ipynb\n",
      "\u001b[2m[2023-11-23 16:10:28,739 87795:140704521019008][base.py:53 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:28,743 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2023-11-23 16:10:28,746 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2023-11-23 16:10:28,748 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2023-11-23 16:10:28,751 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config, work_dir=dict(root=work_dirs)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:28,769 87795:140704521019008][base.py:53 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:28,772 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2023-11-23 16:10:28,774 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2023-11-23 16:10:28,777 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2023-11-23 16:10:28,779 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpcxx6k_vf\u001b[0m\n",
      "└── \u001b[1;36mlog_callback\u001b[0m\n",
      "    └── 2023-11-23T16-10-28_769479-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2023-11-23 16:10:28,769 87795:140704521019008][base.py:53 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-11-23 16:10:28,772 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2023-11-23 16:10:28,774 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2023-11-23 16:10:28,777 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2023-11-23 16:10:28,779 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='log_callback',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type='LogCallback',\n",
    "            interval=5,\n",
    "            with_file_handler=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/log_callback/*.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:29,358 87795:140704521019008][base.py:53 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:29,887 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:01 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2023-11-23 16:10:30,408 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:01 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2023-11-23 16:10:30,926 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2023-11-23 16:10:31,442 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config, work_dir=dict(root=work_dirs)\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:31,457 87795:140704521019008][base.py:53 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:32,981 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:03 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2023-11-23 16:10:37,000 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:04 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2023-11-23 16:10:42,020 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:03 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2023-11-23 16:10:47,043 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"EMA_ETA\", decay=0.2),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config, work_dir=dict(root=work_dirs)\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1 * min(10, runner.iter_))\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-23 16:10:47,177 87795:140704521019008][log.py:61 todd.Validator.log_callback init] INFO: \n",
      "Platform: macOS-14.0\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0edd804\n",
      "Git status: M tutorials/runners.ipynb\n",
      "\u001b[2m[2023-11-23 16:10:47,179 87795:140704521019008][base.py:53 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:47,185 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:00 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2023-11-23 16:10:47,188 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:00 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2023-11-23 16:10:47,190 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2023-11-23 16:10:47,193 87795:140704521019008][log.py:99 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "            with_file_handler=True,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config, work_dir=dict(root=work_dirs)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:47,209 87795:140704521019008][base.py:53 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:47,213 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.000 loss=5.000 lr=['1.667e-03']\n",
      "[2023-11-23 16:10:47,218 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.004 loss=7.983 lr=['2.333e-03']\n",
      "[2023-11-23 16:10:47,221 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.013 loss=12.912 lr=['3.000e-03']\n",
      "[2023-11-23 16:10:47,223 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.033 loss=11.802 lr=['3.667e-03']\n",
      "[2023-11-23 16:10:47,224 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.055 loss=16.532 lr=['4.333e-03']\n",
      "[2023-11-23 16:10:47,227 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.092 loss=4.770 lr=['5.000e-03']\n",
      "[2023-11-23 16:10:47,228 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.104 loss=7.583 lr=['5.000e-03']\n",
      "[2023-11-23 16:10:47,230 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.124 loss=12.192 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=5),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config, work_dir=dict(root=work_dirs)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:47,246 87795:140704521019008][base.py:53 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:47,251 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "[2023-11-23 16:10:47,254 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/10] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.000 loss=6.000 lr=['1.667e-03']\n",
      "[2023-11-23 16:10:47,256 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/10] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.005 loss=3.990 lr=['1.667e-03']\n",
      "[2023-11-23 16:10:47,257 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [2/5]\n",
      "[2023-11-23 16:10:47,260 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/10] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.008 loss=4.979 lr=['2.778e-03']\n",
      "[2023-11-23 16:10:47,261 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/10] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.015 loss=4.962 lr=['2.778e-03']\n",
      "[2023-11-23 16:10:47,262 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [3/5]\n",
      "[2023-11-23 16:10:47,265 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/10] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.022 loss=3.956 lr=['3.889e-03']\n",
      "[2023-11-23 16:10:47,268 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/10] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.030 loss=5.910 lr=['3.889e-03']\n",
      "[2023-11-23 16:10:47,269 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [4/5]\n",
      "[2023-11-23 16:10:47,272 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/10] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.042 loss=3.917 lr=['5.000e-03']\n",
      "[2023-11-23 16:10:47,275 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/10] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.052 loss=5.845 lr=['5.000e-03']\n",
      "[2023-11-23 16:10:47,277 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [5/5]\n",
      "[2023-11-23 16:10:47,279 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [9/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.067 loss=2.900 lr=['5.000e-03']\n",
      "[2023-11-23 16:10:47,282 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [10/10] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.074 loss=6.740 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=4),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=3),\n",
    "            by_epoch=True,\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=5,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config, work_dir=dict(root=work_dirs)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-23 16:10:47,297 87795:140704521019008][lr.py:92 todd.IterBasedTrainer.lr_scale_callback _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2023-11-23 16:10:47,299 87795:140704521019008][base.py:53 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:47,303 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.000 loss=8.000\n",
      "[2023-11-23 16:10:47,310 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.040 loss=12.740\n",
      "[2023-11-23 16:10:47,322 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])} weight=0.105 loss=16.108\n",
      "[2023-11-23 16:10:47,326 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.190 loss=3.620\n",
      "[2023-11-23 16:10:47,330 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.210 loss=11.635\n",
      "[2023-11-23 16:10:47,336 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.275 loss=6.900\n",
      "[2023-11-23 16:10:47,340 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.315 loss=10.953\n",
      "[2023-11-23 16:10:47,345 87795:140704521019008][log.py:99 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])} weight=0.380 loss=13.770\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_scale_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScaleCallback\",\n",
    "            lr_scaler=dict(base_batch_size=1),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config, work_dir=dict(root=work_dirs)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:47,374 87795:140704521019008][base.py:53 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:47,377 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([2, 5]), 'y': tensor([ 4, 10])} weight=0.000 loss=7.000\n",
      "[2023-11-23 16:10:47,378 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_1\n",
      "[2023-11-23 16:10:47,387 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.018 loss=17.843\n",
      "[2023-11-23 16:10:47,391 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_2\n",
      "[2023-11-23 16:10:47,399 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.062 loss=12.594\n",
      "[2023-11-23 16:10:47,401 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_3\n",
      "[2023-11-23 16:10:47,410 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.095 loss=3.810\n",
      "[2023-11-23 16:10:47,411 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_4\n",
      "[2023-11-23 16:10:47,416 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.105 loss=12.318\n",
      "[2023-11-23 16:10:47,418 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_5\n",
      "[2023-11-23 16:10:47,423 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([2, 5]), 'y': tensor([ 4, 10])} weight=0.137 loss=6.519\n",
      "[2023-11-23 16:10:47,424 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_6\n",
      "[2023-11-23 16:10:47,428 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.155 loss=16.605\n",
      "[2023-11-23 16:10:47,429 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_7\n",
      "[2023-11-23 16:10:47,434 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.200 loss=11.700\n",
      "[2023-11-23 16:10:47,436 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_5\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_7\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_8\u001b[0m\n",
      "\n",
      "12 directories, 40 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-23 16:10:47,874 87795:140704521019008][checkpoint.py:46 todd.IterBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_5\n",
      "[2023-11-23 16:10:47,877 87795:140704521019008][base.py:56 todd.IterBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2023-11-23 16:10:47,878 87795:140704521019008][base.py:53 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:47,882 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])} weight=0.137 loss=15.831\n",
      "[2023-11-23 16:10:47,882 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_6\n",
      "[2023-11-23 16:10:47,887 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.180 loss=15.470\n",
      "[2023-11-23 16:10:47,888 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_7\n",
      "[2023-11-23 16:10:47,891 87795:140704521019008][log.py:99 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.222 loss=7.999\n",
      "[2023-11-23 16:10:47,892 87795:140704521019008][checkpoint.py:72 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7mep22bd/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {0: {'momentum_buffer': None}}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 5}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.1375))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_5 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_5'\n",
    "    for f in iter_5.glob('*.pth'):\n",
    "        print(f'{f.name}:')\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()\n",
    "\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_5),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:47,933 87795:140704521019008][base.py:53 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:47,935 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-11-23 16:10:47,938 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.000 loss=7.000\n",
      "[2023-11-23 16:10:47,940 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.018 loss=16.851\n",
      "[2023-11-23 16:10:47,941 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_2\n",
      "[2023-11-23 16:10:47,945 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.060 loss=4.850\n",
      "[2023-11-23 16:10:47,948 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.073 loss=8.674\n",
      "[2023-11-23 16:10:47,950 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_4\n",
      "[2023-11-23 16:10:47,955 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.095 loss=16.192\n",
      "[2023-11-23 16:10:47,955 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-11-23 16:10:47,958 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.138 loss=8.381\n",
      "[2023-11-23 16:10:47,959 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_6\n",
      "[2023-11-23 16:10:47,963 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.160 loss=2.760\n",
      "[2023-11-23 16:10:47,965 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.167 loss=15.576\n",
      "[2023-11-23 16:10:47,966 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_8\n",
      "[2023-11-23 16:10:47,970 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.210 loss=8.055\n",
      "[2023-11-23 16:10:47,971 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.232 loss=15.024\n",
      "[2023-11-23 16:10:47,972 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_10\n",
      "[2023-11-23 16:10:47,975 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-11-23 16:10:47,976 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.275 loss=7.762\n",
      "[2023-11-23 16:10:47,978 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.297 loss=11.066\n",
      "[2023-11-23 16:10:47,979 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_12\n",
      "[2023-11-23 16:10:47,984 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.330 loss=10.855\n",
      "[2023-11-23 16:10:47,987 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.362 loss=2.456\n",
      "[2023-11-23 16:10:47,987 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_14\n",
      "[2023-11-23 16:10:47,991 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.370 loss=13.855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_12\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_14\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_14\u001b[0m\n",
      "\n",
      "11 directories, 35 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-23 16:10:48,437 87795:140704521019008][checkpoint.py:46 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_8\n",
      "[2023-11-23 16:10:48,440 87795:140704521019008][base.py:56 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2023-11-23 16:10:48,441 87795:140704521019008][base.py:53 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:48,442 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-11-23 16:10:48,445 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.210 loss=17.005\n",
      "[2023-11-23 16:10:48,447 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.257 loss=6.970\n",
      "[2023-11-23 16:10:48,448 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_10\n",
      "[2023-11-23 16:10:48,452 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-11-23 16:10:48,454 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.278 loss=5.168\n",
      "[2023-11-23 16:10:48,456 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.292 loss=11.953\n",
      "[2023-11-23 16:10:48,456 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_12\n",
      "[2023-11-23 16:10:48,460 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([7, 9]), 'y': tensor([14, 18])} weight=0.327 loss=13.380\n",
      "[2023-11-23 16:10:48,462 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.367 loss=11.427\n",
      "[2023-11-23 16:10:48,462 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_14\n",
      "[2023-11-23 16:10:48,466 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.402 loss=3.994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-23 16:10:48,898 87795:140704521019008][checkpoint.py:46 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_10\n",
      "[2023-11-23 16:10:48,902 87795:140704521019008][base.py:56 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2023-11-23 16:10:48,903 87795:140704521019008][base.py:53 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:48,904 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-11-23 16:10:48,907 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([6, 5]), 'y': tensor([12, 10])} weight=0.278 loss=9.474\n",
      "[2023-11-23 16:10:48,909 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.305 loss=8.475\n",
      "[2023-11-23 16:10:48,909 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_12\n",
      "[2023-11-23 16:10:48,913 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.330 loss=8.350\n",
      "[2023-11-23 16:10:48,915 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.355 loss=4.113\n",
      "[2023-11-23 16:10:48,916 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpon963xtt/checkpoint_callback/checkpoints/iter_14\n",
      "[2023-11-23 16:10:48,920 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.368 loss=15.509\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=2),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_8 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_8'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_8),\n",
    "        )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    iter_10 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_10'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_10),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:48,946 87795:140704521019008][base.py:53 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:48,948 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-11-23 16:10:48,951 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([8, 3]), 'y': tensor([16,  6])} weight=0.000 loss=11.000\n",
      "[2023-11-23 16:10:48,953 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.027 loss=12.821\n",
      "[2023-11-23 16:10:48,955 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.060 loss=18.430\n",
      "[2023-11-23 16:10:48,957 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.108 loss=6.624\n",
      "[2023-11-23 16:10:48,959 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.125 loss=4.688\n",
      "[2023-11-23 16:10:48,960 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2w_40pe/checkpoint_callback/checkpoints/epoch_1\n",
      "[2023-11-23 16:10:48,963 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-11-23 16:10:48,967 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.138 loss=9.312\n",
      "[2023-11-23 16:10:48,969 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.162 loss=11.025\n",
      "[2023-11-23 16:10:48,971 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([3, 9]), 'y': tensor([ 6, 18])} weight=0.192 loss=10.845\n",
      "[2023-11-23 16:10:48,973 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([10,  4]), 'y': tensor([20,  8])} weight=0.222 loss=12.443\n",
      "[2023-11-23 16:10:48,975 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.257 loss=6.099\n",
      "[2023-11-23 16:10:48,976 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2w_40pe/checkpoint_callback/checkpoints/epoch_2\n",
      "[2023-11-23 16:10:48,978 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-11-23 16:10:48,980 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.275 loss=11.212\n",
      "[2023-11-23 16:10:48,982 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.308 loss=6.770\n",
      "[2023-11-23 16:10:48,985 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([10,  3]), 'y': tensor([20,  6])} weight=0.328 loss=10.871\n",
      "[2023-11-23 16:10:48,987 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.360 loss=4.100\n",
      "[2023-11-23 16:10:48,988 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([9, 7]), 'y': tensor([18, 14])} weight=0.373 loss=13.020\n",
      "[2023-11-23 16:10:48,989 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2w_40pe/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2w_40pe\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36mepoch_3\u001b[0m\n",
      "\n",
      "7 directories, 15 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-23 16:10:49,417 87795:140704521019008][checkpoint.py:46 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2w_40pe/checkpoint_callback/checkpoints/epoch_2\n",
      "[2023-11-23 16:10:49,421 87795:140704521019008][base.py:56 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2023-11-23 16:10:49,422 87795:140704521019008][base.py:53 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:49,423 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-11-23 16:10:49,425 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.275 loss=14.663\n",
      "[2023-11-23 16:10:49,427 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.317 loss=5.889\n",
      "[2023-11-23 16:10:49,429 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.335 loss=10.823\n",
      "[2023-11-23 16:10:49,431 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.368 loss=8.979\n",
      "[2023-11-23 16:10:49,433 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 5]), 'y': tensor([ 4, 10])} weight=0.395 loss=5.617\n",
      "[2023-11-23 16:10:49,434 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp2w_40pe/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'epoch_2'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(epoch_2),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register_()\n",
    "class FaultyValidator(todd.runners.Validator):\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError(\"faulty runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:49,470 87795:140704521019008][base.py:53 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2023-11-23 16:10:49,471 87795:140704521019008][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x122333a50>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 221, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_87795/3179889958.py\", line 4, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2023-11-23 16:10:49,470 87795:140704521019008][base.py:53 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-11-23 16:10:49,471 87795:140704521019008][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x122333a50>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 221, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_87795/3179889958.py\", line 4, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='FaultyValidator',\n",
    "    name='monitor_callback',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[\n",
    "        dict(type='MonitorCallback'),\n",
    "        dict(type='LogCallback', interval=5, with_file_handler=True),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/monitor_callback/*.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:49,779 87795:140704521019008][base.py:53 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:49,781 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-11-23 16:10:49,785 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.000 loss=8.000\n",
      "[2023-11-23 16:10:49,787 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.020 loss=7.920\n",
      "[2023-11-23 16:10:49,789 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.040 loss=13.720\n",
      "[2023-11-23 16:10:49,791 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.075 loss=9.625\n",
      "[2023-11-23 16:10:49,793 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.100 loss=14.250\n",
      "[2023-11-23 16:10:49,794 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfgawtv3w/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2023-11-23 16:10:49,797 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-11-23 16:10:49,800 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.138 loss=8.381\n",
      "[2023-11-23 16:10:49,803 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([9, 7]), 'y': tensor([18, 14])} weight=0.160 loss=14.720\n",
      "[2023-11-23 16:10:49,805 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([6, 4]), 'y': tensor([12,  8])} weight=0.200 loss=9.000\n",
      "[2023-11-23 16:10:49,808 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.225 loss=6.213\n",
      "[2023-11-23 16:10:49,810 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.242 loss=11.424\n",
      "[2023-11-23 16:10:49,811 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfgawtv3w/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2023-11-23 16:10:49,815 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-11-23 16:10:49,818 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.275 loss=6.900\n",
      "[2023-11-23 16:10:49,820 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.295 loss=5.967\n",
      "[2023-11-23 16:10:49,822 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([6, 5]), 'y': tensor([12, 10])} weight=0.312 loss=9.281\n",
      "[2023-11-23 16:10:49,824 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.340 loss=8.300\n",
      "[2023-11-23 16:10:49,825 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.365 loss=15.533\n",
      "[2023-11-23 16:10:49,826 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfgawtv3w/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-11-23 16:10:50,272 87795:140704521019008][base.py:53 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-11-23 16:10:50,273 87795:140704521019008][base.py:70 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfgawtv3w/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2023-11-23 16:10:50,275 87795:140704521019008][base.py:56 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "[2023-11-23 16:10:50,276 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-11-23 16:10:50,279 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.275 loss=11.212\n",
      "[2023-11-23 16:10:50,281 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.307 loss=9.309\n",
      "[2023-11-23 16:10:50,284 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.335 loss=9.157\n",
      "[2023-11-23 16:10:50,286 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.362 loss=7.369\n",
      "[2023-11-23 16:10:50,288 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.385 loss=8.882\n",
      "[2023-11-23 16:10:50,289 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfgawtv3w/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2023-11-23 16:10:50,292 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-11-23 16:10:50,294 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([7, 8]), 'y': tensor([14, 16])} weight=0.412 loss=11.906\n",
      "[2023-11-23 16:10:50,296 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.450 loss=8.525\n",
      "[2023-11-23 16:10:50,299 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.477 loss=8.374\n",
      "[2023-11-23 16:10:50,301 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.505 loss=9.717\n",
      "[2023-11-23 16:10:50,303 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.538 loss=3.656\n",
      "[2023-11-23 16:10:50,304 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfgawtv3w/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2023-11-23 16:10:50,306 87795:140704521019008][log.py:105 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-11-23 16:10:50,308 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.550 loss=10.875\n",
      "[2023-11-23 16:10:50,310 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.588 loss=4.944\n",
      "[2023-11-23 16:10:50,311 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.605 loss=4.185\n",
      "[2023-11-23 16:10:50,313 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.620 loss=12.420\n",
      "[2023-11-23 16:10:50,315 87795:140704521019008][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 7]), 'y': tensor([ 4, 14])} weight=0.665 loss=6.007\n",
      "[2023-11-23 16:10:50,317 87795:140704521019008][checkpoint.py:72 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfgawtv3w/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"strategy_load_model_from\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = (pathlib.Path(work_dirs) / 'strategy_load_model_from' / 'checkpoints' / 'epoch_2' / 'model.pth')\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.strategy.load_model_from(epoch_2)\n",
    "    runner.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
