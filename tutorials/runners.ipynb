{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing /Users/bytedance/Developer/todd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.6.1)\n",
      "Requirement already satisfied: lmdb in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (1.4.1)\n",
      "Requirement already satisfied: opencv-python in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.7.0.72)\n",
      "Requirement already satisfied: pandas in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: python-pptx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.6.21)\n",
      "Requirement already satisfied: timm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.9.2)\n",
      "Requirement already satisfied: toml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.5.0)\n",
      "Requirement already satisfied: yapf in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from opencv-python->todd-ai==0.4.0) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2023.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (4.9.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (9.5.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.14.1)\n",
      "Requirement already satisfied: safetensors in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.3.1)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from yapf->todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->todd-ai==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.12.0)\n",
      "Requirement already satisfied: sympy in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (2023.5.0)\n",
      "Requirement already satisfied: requests in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (2.30.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (23.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from jinja2->torch>=1.7->timm->todd-ai==0.4.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from sympy->torch>=1.7->timm->todd-ai==0.4.0) (1.3.0)\n",
      "Building wheels for collected packages: todd-ai\n",
      "  Building wheel for todd-ai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for todd-ai: filename=todd_ai-0.4.0-py3-none-any.whl size=108342 sha256=64ec3da2a1d6bc795e9b61154640dde374c6de4e01ac78a3bf3276e91a88edd0\n",
      "  Stored in directory: /private/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/pip-ephem-wheel-cache-hcvjacdf/wheels/15/ef/5a/9fc12e257ce5cef16b333a2ed6c992ff9cbcc9167f7199e6ac\n",
      "Successfully built todd-ai\n",
      "Installing collected packages: todd-ai\n",
      "Successfully installed todd-ai-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "[2023-08-03 16:02:56,081 41663:140704487245376][patches.py:14 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tempfile\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict, cast\n",
    "\n",
    "import todd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "Memo = dict[str, Any]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register()\n",
    "class RunnerModel(todd.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> Memo:\n",
    "        y: torch.Tensor = self._strategy.model(batch['x'])\n",
    "        loss = F.l1_loss(y, batch['y'])\n",
    "        memo['loss'] = loss\n",
    "        if 'log' in memo:\n",
    "            memo['log']['loss'] = f'{loss.item():.3f}'\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerMixin(RunnerMixin):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> Memo:\n",
    "        memo = super()._run_iter(batch, memo)\n",
    "        if 'log' in memo:\n",
    "            model = cast(RunnerModel, self._strategy.module)\n",
    "            memo['log']['weight'] = f'{model.weight.item():.3f}'\n",
    "            memo['log']['batch'] = str(batch)\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomValidator(RunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomIterBasedTrainer(TrainerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomEpochBasedTrainer(TrainerMixin, todd.runners.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_demo = todd.Config(\n",
    "    type='CustomValidator',\n",
    "    name='custom_validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-03 16:02:56,711 41663:140704487245376][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpo1uzwao_\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_demo.callbacks=dict(type='LogCallback', interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-03 16:02:57,031 41663:140704487245376][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:02:57,035 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.009522 loss=10.000\n",
      "[2023-08-03 16:02:57,041 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.008501 loss=20.000\n",
      "[2023-08-03 16:02:57,043 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.003692 loss=30.000\n",
      "[2023-08-03 16:02:57,047 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp8fpip1p\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_demo = validator_demo.copy()\n",
    "trainer_demo.pop('type')\n",
    "trainer_demo.dataloader = todd.Config(\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    dataset=dict(type='RunnerDataset', n=67),\n",
    ")\n",
    "trainer_demo.optimizer = todd.Config(type='SGD', lr=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_based_trainer_demo = trainer_demo.copy()\n",
    "iter_based_trainer_demo.type = 'CustomIterBasedTrainer'\n",
    "iter_based_trainer_demo.name = 'custom_iter_based_trainer'\n",
    "iter_based_trainer_demo.iters = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-03 16:02:57,348 41663:140704487245376][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:02:57,351 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.021418 loss=74.000 weight=0.000 batch={'x': tensor([11, 63]), 'y': tensor([ 22, 126])}\n",
      "[2023-08-03 16:02:57,353 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.018666 loss=49.000 weight=0.000 batch={'x': tensor([48,  1]), 'y': tensor([96,  2])}\n",
      "[2023-08-03 16:02:57,356 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.016945 loss=119.000 weight=0.000 batch={'x': tensor([67, 52]), 'y': tensor([134, 104])}\n",
      "[2023-08-03 16:02:57,358 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.015517 loss=61.000 weight=0.000 batch={'x': tensor([28, 33]), 'y': tensor([56, 66])}\n",
      "[2023-08-03 16:02:57,360 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.012843 loss=63.000 weight=0.000 batch={'x': tensor([58,  5]), 'y': tensor([116,  10])}\n",
      "[2023-08-03 16:02:57,362 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.010266 loss=29.000 weight=0.000 batch={'x': tensor([ 2, 27]), 'y': tensor([ 4, 54])}\n",
      "[2023-08-03 16:02:57,364 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.007893 loss=39.000 weight=0.000 batch={'x': tensor([24, 15]), 'y': tensor([48, 30])}\n",
      "[2023-08-03 16:02:57,366 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.005525 loss=105.000 weight=0.000 batch={'x': tensor([54, 51]), 'y': tensor([108, 102])}\n",
      "[2023-08-03 16:02:57,368 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.003407 loss=46.000 weight=0.000 batch={'x': tensor([26, 20]), 'y': tensor([52, 40])}\n",
      "[2023-08-03 16:02:57,370 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.001248 loss=96.000 weight=0.000 batch={'x': tensor([40, 56]), 'y': tensor([ 80, 112])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        iter_based_trainer_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_based_trainer_demo = trainer_demo.copy()\n",
    "epoch_based_trainer_demo.type = 'CustomEpochBasedTrainer'\n",
    "epoch_based_trainer_demo.name = 'custom_epoch_based_trainer'\n",
    "epoch_based_trainer_demo.epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-03 16:02:57,386 41663:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:02:57,387 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:02:57,391 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.068618 loss=69.000 weight=0.000 batch={'x': tensor([58, 11]), 'y': tensor([116,  22])}\n",
      "[2023-08-03 16:02:57,393 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.053645 loss=79.000 weight=0.000 batch={'x': tensor([57, 22]), 'y': tensor([114,  44])}\n",
      "[2023-08-03 16:02:57,395 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.046191 loss=28.000 weight=0.000 batch={'x': tensor([21,  7]), 'y': tensor([42, 14])}\n",
      "[2023-08-03 16:02:57,398 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.042460 loss=55.000 weight=0.000 batch={'x': tensor([49,  6]), 'y': tensor([98, 12])}\n",
      "[2023-08-03 16:02:57,400 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.038571 loss=103.000 weight=0.000 batch={'x': tensor([64, 39]), 'y': tensor([128,  78])}\n",
      "[2023-08-03 16:02:57,402 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.035182 loss=35.000 weight=0.000 batch={'x': tensor([ 8, 27]), 'y': tensor([16, 54])}\n",
      "[2023-08-03 16:02:57,404 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:02:57,406 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.036004 loss=61.000 weight=0.000 batch={'x': tensor([36, 25]), 'y': tensor([72, 50])}\n",
      "[2023-08-03 16:02:57,408 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.032871 loss=16.000 weight=0.000 batch={'x': tensor([12,  4]), 'y': tensor([24,  8])}\n",
      "[2023-08-03 16:02:57,411 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.029553 loss=82.000 weight=0.000 batch={'x': tensor([53, 29]), 'y': tensor([106,  58])}\n",
      "[2023-08-03 16:02:57,412 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.026246 loss=117.000 weight=0.000 batch={'x': tensor([62, 55]), 'y': tensor([124, 110])}\n",
      "[2023-08-03 16:02:57,415 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.023440 loss=91.000 weight=0.000 batch={'x': tensor([50, 41]), 'y': tensor([100,  82])}\n",
      "[2023-08-03 16:02:57,417 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.020723 loss=70.000 weight=0.000 batch={'x': tensor([21, 49]), 'y': tensor([42, 98])}\n",
      "[2023-08-03 16:02:57,419 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.018039 loss=43.000 weight=0.000 batch={'x': tensor([ 9, 34]), 'y': tensor([18, 68])}\n",
      "[2023-08-03 16:02:57,421 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:02:57,424 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.016581 loss=79.000 weight=0.000 batch={'x': tensor([29, 50]), 'y': tensor([ 58, 100])}\n",
      "[2023-08-03 16:02:57,426 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.013867 loss=68.000 weight=0.000 batch={'x': tensor([21, 47]), 'y': tensor([42, 94])}\n",
      "[2023-08-03 16:02:57,428 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.011109 loss=72.000 weight=0.000 batch={'x': tensor([15, 57]), 'y': tensor([ 30, 114])}\n",
      "[2023-08-03 16:02:57,430 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.008545 loss=84.000 weight=0.000 batch={'x': tensor([32, 52]), 'y': tensor([ 64, 104])}\n",
      "[2023-08-03 16:02:57,432 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.006002 loss=44.000 weight=0.000 batch={'x': tensor([ 2, 42]), 'y': tensor([ 4, 84])}\n",
      "[2023-08-03 16:02:57,434 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.003461 loss=30.000 weight=0.000 batch={'x': tensor([17, 13]), 'y': tensor([34, 26])}\n",
      "[2023-08-03 16:02:57,437 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.000985 loss=83.000 weight=0.000 batch={'x': tensor([25, 58]), 'y': tensor([ 50, 116])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        epoch_based_trainer_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_callback_demo = validator_demo.copy()\n",
    "log_callback = log_callback_demo.callbacks\n",
    "log_callback.collect_env = todd.Config(verbose=False)\n",
    "log_callback.with_file_handler = True\n",
    "log_callback_demo.callbacks = [log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:02:57,548 41663:140704487245376][log.py:50 todd.CustomValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:02:57,550 41663:140704487245376][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:02:57,553 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.005829 loss=10.000\n",
      "[2023-08-03 16:02:57,555 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.004178 loss=20.000\n",
      "[2023-08-03 16:02:57,558 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.002172 loss=30.000\n",
      "[2023-08-03 16:02:57,560 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp86nwktuk\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "    └── 2023-08-03T16-02-57_456959-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2023-08-03 16:02:57,548 41663:140704487245376][log.py:50 todd.CustomValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "[2023-08-03 16:02:57,550 41663:140704487245376][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-08-03 16:02:57,553 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.005829 loss=10.000\n",
      "[2023-08-03 16:02:57,555 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.004178 loss=20.000\n",
      "[2023-08-03 16:02:57,558 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.002172 loss=30.000\n",
      "[2023-08-03 16:02:57,560 41663:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        log_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_callback_demo = iter_based_trainer_demo.copy()\n",
    "optimize_callback = todd.Config(type='OptimizeCallback')\n",
    "optimize_callback_demo.callbacks = [optimize_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:02:58,253 41663:140704487245376][log.py:50 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:02:58,255 41663:140704487245376][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:02:58,265 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.084317 loss=72.979 weight=0.752 batch={'x': tensor([50, 67]), 'y': tensor([100, 134])}\n",
      "[2023-08-03 16:02:58,269 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.052069 loss=7.556 weight=1.767 batch={'x': tensor([36, 29]), 'y': tensor([72, 58])}\n",
      "[2023-08-03 16:02:58,273 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.042796 loss=0.244 weight=2.038 batch={'x': tensor([10,  3]), 'y': tensor([20,  6])}\n",
      "[2023-08-03 16:02:58,277 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.033343 loss=0.304 weight=1.798 batch={'x': tensor([1, 2]), 'y': tensor([2, 4])}\n",
      "[2023-08-03 16:02:58,280 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.026699 loss=4.887 weight=1.915 batch={'x': tensor([57, 58]), 'y': tensor([114, 116])}\n",
      "[2023-08-03 16:02:58,284 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.020895 loss=2.656 weight=1.938 batch={'x': tensor([34, 51]), 'y': tensor([ 68, 102])}\n",
      "[2023-08-03 16:02:58,287 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.015815 loss=1.955 weight=1.943 batch={'x': tensor([ 4, 64]), 'y': tensor([  8, 128])}\n",
      "[2023-08-03 16:02:58,291 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.011314 loss=11.014 weight=1.778 batch={'x': tensor([66, 33]), 'y': tensor([132,  66])}\n",
      "[2023-08-03 16:02:58,294 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.006721 loss=2.933 weight=2.058 batch={'x': tensor([53, 49]), 'y': tensor([106,  98])}\n",
      "[2023-08-03 16:02:58,297 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.002444 loss=0.114 weight=1.983 batch={'x': tensor([8, 5]), 'y': tensor([16, 10])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        optimize_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_schedule_callback = todd.Config(\n",
    "    type='LRScheduleCallback',\n",
    "    lr_scheduler=dict(type='LinearLR', total_iters=10),\n",
    ")\n",
    "lr_schedule_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_callback,\n",
    "    log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:02:58,414 41663:140704487245376][log.py:50 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:02:58,415 41663:140704487245376][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:02:58,421 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.035760 loss=81.702 weight=0.398 batch={'x': tensor([52, 50]), 'y': tensor([104, 100])} lr=['3.333e-03']\n",
      "[2023-08-03 16:02:58,425 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.032942 loss=44.455 weight=0.966 batch={'x': tensor([40, 46]), 'y': tensor([80, 92])} lr=['5.000e-03']\n",
      "[2023-08-03 16:02:58,429 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.028774 loss=5.317 weight=1.854 batch={'x': tensor([24, 49]), 'y': tensor([48, 98])} lr=['5.000e-03']\n",
      "[2023-08-03 16:02:58,432 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.024024 loss=2.444 weight=2.102 batch={'x': tensor([21, 27]), 'y': tensor([42, 54])} lr=['5.000e-03']\n",
      "[2023-08-03 16:02:58,435 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.020179 loss=0.249 weight=1.994 batch={'x': tensor([30, 58]), 'y': tensor([ 60, 116])} lr=['5.000e-03']\n",
      "[2023-08-03 16:02:58,439 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.016349 loss=1.307 weight=2.084 batch={'x': tensor([22,  9]), 'y': tensor([44, 18])} lr=['5.000e-03']\n",
      "[2023-08-03 16:02:58,442 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.012970 loss=9.916 weight=2.279 batch={'x': tensor([66,  5]), 'y': tensor([132,  10])} lr=['5.000e-03']\n",
      "[2023-08-03 16:02:58,445 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.009146 loss=3.872 weight=2.152 batch={'x': tensor([14, 37]), 'y': tensor([28, 74])} lr=['5.000e-03']\n",
      "[2023-08-03 16:02:58,448 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.005478 loss=9.692 weight=1.834 batch={'x': tensor([62, 55]), 'y': tensor([124, 110])} lr=['5.000e-03']\n",
      "[2023-08-03 16:02:58,451 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.002016 loss=4.514 weight=1.844 batch={'x': tensor([10, 48]), 'y': tensor([20, 96])} lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "lr_schedule_by_epoch_callback = lr_schedule_callback.copy()\n",
    "lr_schedule_by_epoch_callback.by_epoch = True\n",
    "lr_schedule_by_epoch_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_by_epoch_callback,\n",
    "    log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:02:58,601 41663:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:02:58,602 41663:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:02:58,604 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:02:58,609 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.110638 loss=40.806 weight=0.226 batch={'x': tensor([36, 10]), 'y': tensor([72, 20])} lr=['1.667e-03']\n",
      "[2023-08-03 16:02:58,613 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.082515 loss=85.021 weight=0.454 batch={'x': tensor([59, 51]), 'y': tensor([118, 102])} lr=['1.667e-03']\n",
      "[2023-08-03 16:02:58,616 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.072390 loss=11.675 weight=0.833 batch={'x': tensor([ 3, 17]), 'y': tensor([ 6, 34])} lr=['1.667e-03']\n",
      "[2023-08-03 16:02:58,619 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.064374 loss=32.287 weight=1.077 batch={'x': tensor([42, 28]), 'y': tensor([84, 56])} lr=['1.667e-03']\n",
      "[2023-08-03 16:02:58,623 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.058957 loss=25.592 weight=1.308 batch={'x': tensor([53, 21]), 'y': tensor([106,  42])} lr=['1.667e-03']\n",
      "[2023-08-03 16:02:58,626 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.054545 loss=17.415 weight=1.595 batch={'x': tensor([25, 61]), 'y': tensor([ 50, 122])} lr=['1.667e-03']\n",
      "[2023-08-03 16:02:58,629 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:02:58,631 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.052245 loss=1.850 weight=1.938 batch={'x': tensor([12, 48]), 'y': tensor([24, 96])} lr=['2.000e-03']\n",
      "[2023-08-03 16:02:58,634 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.046932 loss=0.553 weight=1.972 batch={'x': tensor([31,  9]), 'y': tensor([62, 18])} lr=['2.000e-03']\n",
      "[2023-08-03 16:02:58,637 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.042228 loss=0.659 weight=1.949 batch={'x': tensor([19,  7]), 'y': tensor([38, 14])} lr=['2.000e-03']\n",
      "[2023-08-03 16:02:58,640 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.038306 loss=0.706 weight=1.974 batch={'x': tensor([25, 30]), 'y': tensor([50, 60])} lr=['2.000e-03']\n",
      "[2023-08-03 16:02:58,643 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.033987 loss=0.930 weight=1.979 batch={'x': tensor([54, 36]), 'y': tensor([108,  72])} lr=['2.000e-03']\n",
      "[2023-08-03 16:02:58,647 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.030092 loss=1.764 weight=2.048 batch={'x': tensor([24, 49]), 'y': tensor([48, 98])} lr=['2.000e-03']\n",
      "[2023-08-03 16:02:58,650 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.026226 loss=3.225 weight=1.945 batch={'x': tensor([51, 67]), 'y': tensor([102, 134])} lr=['2.000e-03']\n",
      "[2023-08-03 16:02:58,652 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:02:58,654 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.023142 loss=2.530 weight=1.922 batch={'x': tensor([55, 10]), 'y': tensor([110,  20])} lr=['2.333e-03']\n",
      "[2023-08-03 16:02:58,659 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.019799 loss=0.785 weight=1.976 batch={'x': tensor([54, 11]), 'y': tensor([108,  22])} lr=['2.333e-03']\n",
      "[2023-08-03 16:02:58,663 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.016422 loss=2.060 weight=2.053 batch={'x': tensor([44, 34]), 'y': tensor([88, 68])} lr=['2.333e-03']\n",
      "[2023-08-03 16:02:58,668 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.012793 loss=0.433 weight=2.017 batch={'x': tensor([33, 19]), 'y': tensor([66, 38])} lr=['2.333e-03']\n",
      "[2023-08-03 16:02:58,671 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.009041 loss=0.603 weight=2.024 batch={'x': tensor([ 3, 48]), 'y': tensor([ 6, 96])} lr=['2.333e-03']\n",
      "[2023-08-03 16:02:58,677 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.005441 loss=1.256 weight=1.959 batch={'x': tensor([12, 50]), 'y': tensor([ 24, 100])} lr=['2.333e-03']\n",
      "[2023-08-03 16:02:58,687 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001676 loss=1.192 weight=2.025 batch={'x': tensor([49, 47]), 'y': tensor([98, 94])} lr=['2.333e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_by_epoch_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scaler_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_scaler_callback = todd.Config(\n",
    "    type='LRScaleCallback',\n",
    "    lr_scaler=dict(base_batch_size=1),\n",
    ")\n",
    "lr_scaler_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_scaler_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:02:58,725 41663:140704487245376][lr.py:92 todd.CustomIterBasedTrainer.custom_iter_based_trainer _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "[2023-08-03 16:02:58,831 41663:140704487245376][log.py:50 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:02:58,833 41663:140704487245376][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:02:58,839 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.042614 loss=31.800 weight=1.400 batch={'x': tensor([50, 56]), 'y': tensor([100, 112])}\n",
      "[2023-08-03 16:02:58,842 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.034022 loss=15.870 weight=2.460 batch={'x': tensor([52, 17]), 'y': tensor([104,  34])}\n",
      "[2023-08-03 16:02:58,847 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.030633 loss=19.110 weight=2.420 batch={'x': tensor([24, 67]), 'y': tensor([ 48, 134])}\n",
      "[2023-08-03 16:02:58,850 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.025232 loss=11.160 weight=1.690 batch={'x': tensor([27, 45]), 'y': tensor([54, 90])}\n",
      "[2023-08-03 16:02:58,853 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.020811 loss=8.085 weight=1.790 batch={'x': tensor([46, 31]), 'y': tensor([92, 62])}\n",
      "[2023-08-03 16:02:58,857 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.017528 loss=8.645 weight=2.190 batch={'x': tensor([38, 53]), 'y': tensor([ 76, 106])}\n",
      "[2023-08-03 16:02:58,860 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.013215 loss=2.920 weight=2.080 batch={'x': tensor([58, 15]), 'y': tensor([116,  30])}\n",
      "[2023-08-03 16:02:58,864 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.009738 loss=7.035 weight=1.790 batch={'x': tensor([55, 12]), 'y': tensor([110,  24])}\n",
      "[2023-08-03 16:02:58,868 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.005946 loss=0.513 weight=1.975 batch={'x': tensor([40,  1]), 'y': tensor([80,  2])}\n",
      "[2023-08-03 16:02:58,871 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.002200 loss=1.437 weight=2.125 batch={'x': tensor([10, 13]), 'y': tensor([20, 26])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_scaler_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback_demo = iter_based_trainer_demo.copy()\n",
    "checkpoint_callback = todd.Config(type='CheckpointCallback', interval=10)\n",
    "checkpoint_callback_demo.callbacks = [checkpoint_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:02:59,044 41663:140704487245376][log.py:50 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:02:59,047 41663:140704487245376][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:02:59,066 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.145910 loss=74.000 weight=0.000 batch={'x': tensor([20, 54]), 'y': tensor([ 40, 108])}\n",
      "[2023-08-03 16:02:59,072 41663:140704487245376][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp3ixjhm7m/custom_iter_based_trainer/checkpoints/iter_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:02:59,086 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.151364 loss=122.000 weight=0.000 batch={'x': tensor([55, 67]), 'y': tensor([110, 134])}\n",
      "[2023-08-03 16:02:59,116 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.163585 loss=23.000 weight=0.000 batch={'x': tensor([ 6, 17]), 'y': tensor([12, 34])}\n",
      "[2023-08-03 16:02:59,173 41663:140704487245376][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp3ixjhm7m/custom_iter_based_trainer/checkpoints/iter_20\n",
      "[2023-08-03 16:02:59,177 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.207686 loss=68.000 weight=0.000 batch={'x': tensor([30, 38]), 'y': tensor([60, 76])}\n",
      "[2023-08-03 16:02:59,181 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.145588 loss=32.000 weight=0.000 batch={'x': tensor([19, 13]), 'y': tensor([38, 26])}\n",
      "[2023-08-03 16:02:59,186 41663:140704487245376][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp3ixjhm7m/custom_iter_based_trainer/checkpoints/iter_30\n",
      "[2023-08-03 16:02:59,191 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.107277 loss=60.000 weight=0.000 batch={'x': tensor([59,  1]), 'y': tensor([118,   2])}\n",
      "[2023-08-03 16:02:59,195 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.074121 loss=95.000 weight=0.000 batch={'x': tensor([37, 58]), 'y': tensor([ 74, 116])}\n",
      "[2023-08-03 16:02:59,202 41663:140704487245376][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp3ixjhm7m/custom_iter_based_trainer/checkpoints/iter_40\n",
      "[2023-08-03 16:02:59,206 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.050301 loss=85.000 weight=0.000 batch={'x': tensor([36, 49]), 'y': tensor([72, 98])}\n",
      "[2023-08-03 16:02:59,210 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.028235 loss=98.000 weight=0.000 batch={'x': tensor([66, 32]), 'y': tensor([132,  64])}\n",
      "[2023-08-03 16:02:59,215 41663:140704487245376][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp3ixjhm7m/custom_iter_based_trainer/checkpoints/iter_50\n",
      "[2023-08-03 16:02:59,221 41663:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.010217 loss=82.000 weight=0.000 batch={'x': tensor([40, 42]), 'y': tensor([80, 84])}\n",
      "[2023-08-03 16:02:59,226 41663:140704487245376][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp3ixjhm7m/custom_iter_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp3ixjhm7m\u001b[0m\n",
      "└── \u001b[1;36mcustom_iter_based_trainer\u001b[0m\n",
      "    ├── 2023-08-03T16-02-58_909521-08-00.log\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_20\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_30\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_40\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_50\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[1;36mlatest\u001b[0m\n",
      "            ├── callbacks.pth\n",
      "            ├── meta.pth\n",
      "            ├── model.pth\n",
      "            ├── optim.pth\n",
      "            └── strategy.pth\n",
      "\n",
      "9 directories, 31 files\n",
      "\n",
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 50}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_50 = pathlib.Path(work_dirs) / 'custom_iter_based_trainer' / 'checkpoints' / 'iter_50'\n",
    "    for f in iter_50.glob('*.pth'):\n",
    "        print(f\"{f.name}:\")\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "checkpoint_by_epoch_callback = checkpoint_callback.copy()\n",
    "checkpoint_by_epoch_callback.update(interval=1, by_epoch=True)\n",
    "checkpoint_by_epoch_callback_demo.callbacks = [\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:02:59,784 41663:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:02:59,786 41663:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:02:59,787 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:02:59,792 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.093586 loss=89.000 weight=0.000 batch={'x': tensor([58, 31]), 'y': tensor([116,  62])}\n",
      "[2023-08-03 16:02:59,795 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.069230 loss=56.000 weight=0.000 batch={'x': tensor([24, 32]), 'y': tensor([48, 64])}\n",
      "[2023-08-03 16:02:59,798 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.064270 loss=87.000 weight=0.000 batch={'x': tensor([57, 30]), 'y': tensor([114,  60])}\n",
      "[2023-08-03 16:02:59,801 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.057248 loss=30.000 weight=0.000 batch={'x': tensor([11, 19]), 'y': tensor([22, 38])}\n",
      "[2023-08-03 16:02:59,806 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.058458 loss=78.000 weight=0.000 batch={'x': tensor([52, 26]), 'y': tensor([104,  52])}\n",
      "[2023-08-03 16:02:59,812 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.059971 loss=16.000 weight=0.000 batch={'x': tensor([ 3, 13]), 'y': tensor([ 6, 26])}\n",
      "[2023-08-03 16:02:59,816 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpljqekocb/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-03 16:02:59,818 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:02:59,822 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.067381 loss=95.000 weight=0.000 batch={'x': tensor([33, 62]), 'y': tensor([ 66, 124])}\n",
      "[2023-08-03 16:02:59,827 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.062425 loss=96.000 weight=0.000 batch={'x': tensor([40, 56]), 'y': tensor([ 80, 112])}\n",
      "[2023-08-03 16:02:59,832 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.056882 loss=120.000 weight=0.000 batch={'x': tensor([65, 55]), 'y': tensor([130, 110])}\n",
      "[2023-08-03 16:02:59,836 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.051186 loss=42.000 weight=0.000 batch={'x': tensor([27, 15]), 'y': tensor([54, 30])}\n",
      "[2023-08-03 16:02:59,841 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.046053 loss=42.000 weight=0.000 batch={'x': tensor([24, 18]), 'y': tensor([48, 36])}\n",
      "[2023-08-03 16:02:59,844 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.040133 loss=78.000 weight=0.000 batch={'x': tensor([37, 41]), 'y': tensor([74, 82])}\n",
      "[2023-08-03 16:02:59,849 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.035204 loss=17.000 weight=0.000 batch={'x': tensor([ 3, 14]), 'y': tensor([ 6, 28])}\n",
      "[2023-08-03 16:02:59,852 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpljqekocb/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-03 16:02:59,855 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:02:59,861 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.033594 loss=70.000 weight=0.000 batch={'x': tensor([53, 17]), 'y': tensor([106,  34])}\n",
      "[2023-08-03 16:02:59,866 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.028394 loss=82.000 weight=0.000 batch={'x': tensor([28, 54]), 'y': tensor([ 56, 108])}\n",
      "[2023-08-03 16:02:59,869 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.022475 loss=29.000 weight=0.000 batch={'x': tensor([ 5, 24]), 'y': tensor([10, 48])}\n",
      "[2023-08-03 16:02:59,873 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.017132 loss=64.000 weight=0.000 batch={'x': tensor([25, 39]), 'y': tensor([50, 78])}\n",
      "[2023-08-03 16:02:59,876 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.011931 loss=51.000 weight=0.000 batch={'x': tensor([41, 10]), 'y': tensor([82, 20])}\n",
      "[2023-08-03 16:02:59,880 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.006866 loss=65.000 weight=0.000 batch={'x': tensor([33, 32]), 'y': tensor([66, 64])}\n",
      "[2023-08-03 16:02:59,883 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001922 loss=60.000 weight=0.000 batch={'x': tensor([42, 18]), 'y': tensor([84, 36])}\n",
      "[2023-08-03 16:02:59,885 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpljqekocb/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-03 16:02:59,891 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpljqekocb/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpljqekocb\u001b[0m\n",
      "└── \u001b[1;36mcustom_epoch_based_trainer\u001b[0m\n",
      "    ├── 2023-08-03T16-02-59_691688-08-00.log\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[1;36mlatest\u001b[0m\n",
      "            ├── callbacks.pth\n",
      "            ├── meta.pth\n",
      "            ├── model.pth\n",
      "            ├── optim.pth\n",
      "            └── strategy.pth\n",
      "\n",
      "7 directories, 21 files\n",
      "\n",
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {}}\n",
      "\n",
      "meta.pth:\n",
      "{'epoch': 2, 'iter_': 68}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_by_epoch_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'custom_epoch_based_trainer' / 'checkpoints' / 'epoch_2'\n",
    "    for f in epoch_2.glob('*.pth'):\n",
    "        print(f\"{f.name}:\")\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_load_from_callback_demo = checkpoint_by_epoch_callback_demo.copy()\n",
    "checkpoint_load_from_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:03:00,465 41663:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:03:00,467 41663:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:03:00,468 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:03:00,473 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.102995 loss=15.906 weight=0.727 batch={'x': tensor([11, 14]), 'y': tensor([22, 28])}\n",
      "[2023-08-03 16:03:00,476 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.075661 loss=8.160 weight=1.490 batch={'x': tensor([13, 19]), 'y': tensor([26, 38])}\n",
      "[2023-08-03 16:03:00,479 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.065047 loss=3.639 weight=2.102 batch={'x': tensor([25, 46]), 'y': tensor([50, 92])}\n",
      "[2023-08-03 16:03:00,482 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.059323 loss=1.544 weight=2.162 batch={'x': tensor([ 4, 15]), 'y': tensor([ 8, 30])}\n",
      "[2023-08-03 16:03:00,485 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.053469 loss=0.645 weight=1.985 batch={'x': tensor([49, 37]), 'y': tensor([98, 74])}\n",
      "[2023-08-03 16:03:00,488 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.048461 loss=2.880 weight=1.910 batch={'x': tensor([29, 35]), 'y': tensor([58, 70])}\n",
      "[2023-08-03 16:03:00,491 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7klahc_f/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-03 16:03:00,494 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:03:00,496 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.053895 loss=1.275 weight=1.950 batch={'x': tensor([ 7, 44]), 'y': tensor([14, 88])}\n",
      "[2023-08-03 16:03:00,499 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.048110 loss=6.435 weight=1.835 batch={'x': tensor([27, 51]), 'y': tensor([ 54, 102])}\n",
      "[2023-08-03 16:03:00,502 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.042678 loss=5.075 weight=1.855 batch={'x': tensor([ 5, 65]), 'y': tensor([ 10, 130])}\n",
      "[2023-08-03 16:03:00,505 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.038325 loss=0.135 weight=2.005 batch={'x': tensor([18, 36]), 'y': tensor([36, 72])}\n",
      "[2023-08-03 16:03:00,508 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.034589 loss=6.753 weight=2.185 batch={'x': tensor([19, 54]), 'y': tensor([ 38, 108])}\n",
      "[2023-08-03 16:03:00,511 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.030506 loss=9.836 weight=1.848 batch={'x': tensor([66, 63]), 'y': tensor([132, 126])}\n",
      "[2023-08-03 16:03:00,514 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.026448 loss=0.295 weight=1.995 batch={'x': tensor([56, 62]), 'y': tensor([112, 124])}\n",
      "[2023-08-03 16:03:00,517 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7klahc_f/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-03 16:03:00,519 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:03:00,522 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.024564 loss=3.960 weight=1.890 batch={'x': tensor([21, 51]), 'y': tensor([ 42, 102])}\n",
      "[2023-08-03 16:03:00,525 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.020533 loss=9.141 weight=2.178 batch={'x': tensor([54, 49]), 'y': tensor([108,  98])}\n",
      "[2023-08-03 16:03:00,528 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.016498 loss=0.750 weight=2.020 batch={'x': tensor([ 9, 66]), 'y': tensor([ 18, 132])}\n",
      "[2023-08-03 16:03:00,531 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.012623 loss=0.800 weight=1.960 batch={'x': tensor([29, 11]), 'y': tensor([58, 22])}\n",
      "[2023-08-03 16:03:00,534 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.008813 loss=2.490 weight=2.060 batch={'x': tensor([25, 58]), 'y': tensor([ 50, 116])}\n",
      "[2023-08-03 16:03:00,537 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.005098 loss=1.820 weight=1.960 batch={'x': tensor([26, 65]), 'y': tensor([ 52, 130])}\n",
      "[2023-08-03 16:03:00,541 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001462 loss=2.900 weight=2.073 batch={'x': tensor([60, 20]), 'y': tensor([120,  40])}\n",
      "[2023-08-03 16:03:00,543 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7klahc_f/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-03 16:03:00,546 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7klahc_f/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:03:00,930 41663:140704487245376][checkpoint.py:44 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7klahc_f/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-03 16:03:01,012 41663:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:03:01,013 41663:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:03:01,015 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:03:01,018 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.061776 loss=3.081 weight=1.788 batch={'x': tensor([28,  1]), 'y': tensor([56,  2])}\n",
      "[2023-08-03 16:03:01,021 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.026676 loss=1.400 weight=2.040 batch={'x': tensor([66,  4]), 'y': tensor([132,   8])}\n",
      "[2023-08-03 16:03:01,025 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.018975 loss=1.572 weight=1.815 batch={'x': tensor([12,  5]), 'y': tensor([24, 10])}\n",
      "[2023-08-03 16:03:01,028 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.013496 loss=5.000 weight=2.250 batch={'x': tensor([18, 22]), 'y': tensor([36, 44])}\n",
      "[2023-08-03 16:03:01,031 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.008895 loss=2.700 weight=2.090 batch={'x': tensor([ 2, 58]), 'y': tensor([  4, 116])}\n",
      "[2023-08-03 16:03:01,034 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.004948 loss=5.769 weight=2.178 batch={'x': tensor([33, 32]), 'y': tensor([66, 64])}\n",
      "[2023-08-03 16:03:01,036 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001366 loss=4.950 weight=2.150 batch={'x': tensor([46, 20]), 'y': tensor([92, 40])}\n",
      "[2023-08-03 16:03:01,038 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7klahc_f/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-03 16:03:01,041 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7klahc_f/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_load_from_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_load_from_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "        load_from=os.path.join(work_dirs, 'custom_epoch_based_trainer', 'checkpoints', 'epoch_2')\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaultyRunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError('faulty runner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyValidator(FaultyRunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyIterBasedTrainer(FaultyRunnerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyEpochBasedTrainer(\n",
    "    FaultyRunnerMixin,\n",
    "    todd.runners.EpochBasedTrainer,\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_callback_demo = validator_demo.copy()\n",
    "monitor_callback_demo.type = 'FaultyValidator'\n",
    "monitor_callback = todd.Config(type='MonitorCallback')\n",
    "monitor_callback_demo.callbacks = [monitor_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:03:01,179 41663:140704487245376][log.py:50 todd.FaultyValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:03:01,181 41663:140704487245376][base.py:53 todd.FaultyValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2023-08-03 16:03:01,184 41663:140704487245376][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x1543f5310>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 193, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_41663/2137902126.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:03:01,179 41663:140704487245376][log.py:50 todd.FaultyValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "[2023-08-03 16:03:01,181 41663:140704487245376][base.py:53 todd.FaultyValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-08-03 16:03:01,184 41663:140704487245376][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x1543f5310>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 193, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_41663/2137902126.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        monitor_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_load_model_from_demo = checkpoint_load_from_callback_demo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:03:01,563 41663:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:03:01,565 41663:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:03:01,567 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:03:01,572 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.092887 loss=74.084 weight=0.472 batch={'x': tensor([39, 58]), 'y': tensor([ 78, 116])}\n",
      "[2023-08-03 16:03:01,575 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.072579 loss=34.450 weight=1.350 batch={'x': tensor([54, 52]), 'y': tensor([108, 104])}\n",
      "[2023-08-03 16:03:01,578 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.063603 loss=1.750 weight=2.070 batch={'x': tensor([22, 28]), 'y': tensor([44, 56])}\n",
      "[2023-08-03 16:03:01,581 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.057326 loss=0.463 weight=2.013 batch={'x': tensor([18, 56]), 'y': tensor([ 36, 112])}\n",
      "[2023-08-03 16:03:01,584 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.051642 loss=2.870 weight=2.070 batch={'x': tensor([42, 40]), 'y': tensor([84, 80])}\n",
      "[2023-08-03 16:03:01,586 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.046879 loss=2.756 weight=2.113 batch={'x': tensor([ 4, 45]), 'y': tensor([ 8, 90])}\n",
      "[2023-08-03 16:03:01,589 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjbhfngzs/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-03 16:03:01,592 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:03:01,594 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.051850 loss=0.263 weight=2.005 batch={'x': tensor([57, 48]), 'y': tensor([114,  96])}\n",
      "[2023-08-03 16:03:01,597 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.046522 loss=2.318 weight=2.045 batch={'x': tensor([51, 52]), 'y': tensor([102, 104])}\n",
      "[2023-08-03 16:03:01,600 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.041577 loss=4.041 weight=1.868 batch={'x': tensor([42, 19]), 'y': tensor([84, 38])}\n",
      "[2023-08-03 16:03:01,603 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.037146 loss=3.850 weight=2.138 batch={'x': tensor([49,  7]), 'y': tensor([98, 14])}\n",
      "[2023-08-03 16:03:01,606 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.033084 loss=2.310 weight=2.083 batch={'x': tensor([18, 38]), 'y': tensor([36, 76])}\n",
      "[2023-08-03 16:03:01,609 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.029142 loss=1.650 weight=1.945 batch={'x': tensor([13, 47]), 'y': tensor([26, 94])}\n",
      "[2023-08-03 16:03:01,611 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.025356 loss=4.538 weight=2.138 batch={'x': tensor([32, 34]), 'y': tensor([64, 68])}\n",
      "[2023-08-03 16:03:01,614 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjbhfngzs/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-03 16:03:01,616 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:03:01,619 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.023606 loss=0.522 weight=1.945 batch={'x': tensor([ 7, 12]), 'y': tensor([14, 24])}\n",
      "[2023-08-03 16:03:01,622 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.019657 loss=0.415 weight=1.990 batch={'x': tensor([34, 49]), 'y': tensor([68, 98])}\n",
      "[2023-08-03 16:03:01,625 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.015839 loss=0.795 weight=2.015 batch={'x': tensor([45, 61]), 'y': tensor([ 90, 122])}\n",
      "[2023-08-03 16:03:01,627 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.012061 loss=10.175 weight=2.275 batch={'x': tensor([27, 47]), 'y': tensor([54, 94])}\n",
      "[2023-08-03 16:03:01,630 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.008403 loss=3.063 weight=2.063 batch={'x': tensor([46, 52]), 'y': tensor([ 92, 104])}\n",
      "[2023-08-03 16:03:01,633 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.004832 loss=0.656 weight=2.038 batch={'x': tensor([13, 22]), 'y': tensor([26, 44])}\n",
      "[2023-08-03 16:03:01,635 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001363 loss=6.440 weight=2.140 batch={'x': tensor([38, 54]), 'y': tensor([ 76, 108])}\n",
      "[2023-08-03 16:03:01,637 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjbhfngzs/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-03 16:03:01,640 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjbhfngzs/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:03:02,113 41663:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 0797962\n",
      "Git status: \n",
      "D  bin/git_rev_parse_head\n",
      "D  bin/git_status_porcelain\n",
      "M  bin/odpsrun\n",
      "D  bin/odpsrun.py\n",
      "M  setup.py\n",
      "M  todd/base/__init__.py\n",
      "M  todd/base/registries.py\n",
      " M todd/runners/callbacks/log.py\n",
      "M  todd/utils/__init__.py\n",
      "RM todd/base/envs.py -> todd/utils/envs.py\n",
      "\u001b[2m[2023-08-03 16:03:02,115 41663:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:03:02,116 41663:140704487245376][base.py:60 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjbhfngzs/custom_epoch_based_trainer/checkpoints/epoch_2/model.pth\n",
      "[2023-08-03 16:03:02,119 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:03:02,122 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.072575 loss=0.056 weight=2.008 batch={'x': tensor([ 2, 13]), 'y': tensor([ 4, 26])}\n",
      "[2023-08-03 16:03:02,125 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.062707 loss=8.760 weight=2.183 batch={'x': tensor([54, 42]), 'y': tensor([108,  84])}\n",
      "[2023-08-03 16:03:02,128 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.056469 loss=0.920 weight=1.960 batch={'x': tensor([15, 31]), 'y': tensor([30, 62])}\n",
      "[2023-08-03 16:03:02,131 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.052119 loss=1.513 weight=2.055 batch={'x': tensor([22, 33]), 'y': tensor([44, 66])}\n",
      "[2023-08-03 16:03:02,134 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.048107 loss=1.926 weight=2.058 batch={'x': tensor([16, 51]), 'y': tensor([ 32, 102])}\n",
      "[2023-08-03 16:03:02,137 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.043891 loss=3.019 weight=1.943 batch={'x': tensor([64, 41]), 'y': tensor([128,  82])}\n",
      "[2023-08-03 16:03:02,140 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjbhfngzs/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-03 16:03:02,142 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:03:02,144 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.049398 loss=3.645 weight=2.135 batch={'x': tensor([14, 40]), 'y': tensor([28, 80])}\n",
      "[2023-08-03 16:03:02,147 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.044705 loss=0.740 weight=1.980 batch={'x': tensor([63, 11]), 'y': tensor([126,  22])}\n",
      "[2023-08-03 16:03:02,150 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.040224 loss=2.544 weight=1.863 batch={'x': tensor([25, 12]), 'y': tensor([50, 24])}\n",
      "[2023-08-03 16:03:02,153 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.035914 loss=0.304 weight=1.993 batch={'x': tensor([48, 33]), 'y': tensor([96, 66])}\n",
      "[2023-08-03 16:03:02,156 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.031844 loss=0.465 weight=1.985 batch={'x': tensor([32, 30]), 'y': tensor([64, 60])}\n",
      "[2023-08-03 16:03:02,159 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.028199 loss=2.828 weight=2.145 batch={'x': tensor([ 3, 36]), 'y': tensor([ 6, 72])}\n",
      "[2023-08-03 16:03:02,162 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.024586 loss=1.560 weight=2.080 batch={'x': tensor([29, 10]), 'y': tensor([58, 20])}\n",
      "[2023-08-03 16:03:02,164 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjbhfngzs/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-03 16:03:02,166 41663:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:03:02,169 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.022892 loss=0.548 weight=2.015 batch={'x': tensor([59, 14]), 'y': tensor([118,  28])}\n",
      "[2023-08-03 16:03:02,172 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.019099 loss=6.180 weight=1.743 batch={'x': tensor([ 2, 46]), 'y': tensor([ 4, 92])}\n",
      "[2023-08-03 16:03:02,175 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.015433 loss=2.329 weight=2.173 batch={'x': tensor([ 8, 19]), 'y': tensor([16, 38])}\n",
      "[2023-08-03 16:03:02,178 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.011795 loss=8.063 weight=2.215 batch={'x': tensor([37, 38]), 'y': tensor([74, 76])}\n",
      "[2023-08-03 16:03:02,180 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.008237 loss=8.312 weight=1.825 batch={'x': tensor([33, 62]), 'y': tensor([ 66, 124])}\n",
      "[2023-08-03 16:03:02,183 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.004751 loss=2.805 weight=2.128 batch={'x': tensor([40,  4]), 'y': tensor([80,  8])}\n",
      "[2023-08-03 16:03:02,186 41663:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001343 loss=3.487 weight=1.768 batch={'x': tensor([ 6, 24]), 'y': tensor([12, 48])}\n",
      "[2023-08-03 16:03:02,188 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjbhfngzs/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-03 16:03:02,215 41663:140704487245376][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjbhfngzs/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        strategy_load_model_from_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        strategy_load_model_from_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.load_model_from(os.path.join(work_dirs, 'custom_epoch_based_trainer', 'checkpoints', 'epoch_2', 'model.pth'))\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
