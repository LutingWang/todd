{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing /Users/bytedance/Developer/todd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.6.1)\n",
      "Requirement already satisfied: lmdb in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (1.4.1)\n",
      "Requirement already satisfied: opencv-python in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.7.0.72)\n",
      "Requirement already satisfied: pandas in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: python-pptx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.6.21)\n",
      "Requirement already satisfied: timm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.9.2)\n",
      "Requirement already satisfied: toml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.5.0)\n",
      "Requirement already satisfied: yapf in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from opencv-python->todd-ai==0.4.0) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2023.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (4.9.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (9.5.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.14.1)\n",
      "Requirement already satisfied: safetensors in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.3.1)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from yapf->todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->todd-ai==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.12.0)\n",
      "Requirement already satisfied: sympy in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (2023.5.0)\n",
      "Requirement already satisfied: requests in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (2.30.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (23.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from jinja2->torch>=1.7->timm->todd-ai==0.4.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from sympy->torch>=1.7->timm->todd-ai==0.4.0) (1.3.0)\n",
      "Building wheels for collected packages: todd-ai\n",
      "  Building wheel for todd-ai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for todd-ai: filename=todd_ai-0.4.0-py3-none-any.whl size=105634 sha256=39b0e0179582ac40bd53bee4deecc592718233dfe55597935498ad9d61f4a8f8\n",
      "  Stored in directory: /private/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/pip-ephem-wheel-cache-y88sdvgq/wheels/15/ef/5a/9fc12e257ce5cef16b333a2ed6c992ff9cbcc9167f7199e6ac\n",
      "Successfully built todd-ai\n",
      "Installing collected packages: todd-ai\n",
      "Successfully installed todd-ai-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "[2023-08-02 11:10:00,664 13384:140704362395200][patches.py:14 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tempfile\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict, cast\n",
    "\n",
    "import todd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "Memo = dict[str, Any]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register()\n",
    "class RunnerModel(todd.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> Memo:\n",
    "        y: torch.Tensor = self._strategy.model(batch['x'])\n",
    "        loss = F.l1_loss(y, batch['y'])\n",
    "        memo['loss'] = loss\n",
    "        if 'log' in memo:\n",
    "            memo['log']['loss'] = f'{loss.item():.3f}'\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerMixin(RunnerMixin):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> Memo:\n",
    "        memo = super()._run_iter(batch, memo)\n",
    "        if 'log' in memo:\n",
    "            model = cast(RunnerModel, self._strategy.module)\n",
    "            memo['log']['weight'] = f'{model.weight.item():.3f}'\n",
    "            memo['log']['batch'] = str(batch)\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomValidator(RunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomIterBasedTrainer(TrainerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomEpochBasedTrainer(TrainerMixin, todd.runners.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_demo = todd.Config(\n",
    "    type='CustomValidator',\n",
    "    name='custom_validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:01,319 13384:140704362395200][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp6pd0na5u\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_demo.callbacks=dict(type='LogCallback', interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:01,707 13384:140704362395200][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:01,711 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.006453 loss=10.000\n",
      "[2023-08-02 11:10:01,714 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.005764 loss=20.000\n",
      "[2023-08-02 11:10:01,717 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.002708 loss=30.000\n",
      "[2023-08-02 11:10:01,720 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpub6r45sx\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_demo = validator_demo.copy()\n",
    "trainer_demo.pop('type')\n",
    "trainer_demo.dataloader = todd.Config(\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    dataset=dict(type='RunnerDataset', n=67),\n",
    ")\n",
    "trainer_demo.optimizer = todd.Config(type='SGD', lr=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_based_trainer_demo = trainer_demo.copy()\n",
    "iter_based_trainer_demo.type = 'CustomIterBasedTrainer'\n",
    "iter_based_trainer_demo.name = 'custom_iter_based_trainer'\n",
    "iter_based_trainer_demo.iters = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:02,013 13384:140704362395200][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:02,025 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.099830 loss=12.000 weight=0.000 batch={'x': tensor([10,  2]), 'y': tensor([20,  4])}\n",
      "[2023-08-02 11:10:02,027 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.054189 loss=81.000 weight=0.000 batch={'x': tensor([48, 33]), 'y': tensor([96, 66])}\n",
      "[2023-08-02 11:10:02,029 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.037002 loss=118.000 weight=0.000 batch={'x': tensor([55, 63]), 'y': tensor([110, 126])}\n",
      "[2023-08-02 11:10:02,031 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.027492 loss=34.000 weight=0.000 batch={'x': tensor([ 9, 25]), 'y': tensor([18, 50])}\n",
      "[2023-08-02 11:10:02,033 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.020730 loss=42.000 weight=0.000 batch={'x': tensor([15, 27]), 'y': tensor([30, 54])}\n",
      "[2023-08-02 11:10:02,035 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.015666 loss=81.000 weight=0.000 batch={'x': tensor([47, 34]), 'y': tensor([94, 68])}\n",
      "[2023-08-02 11:10:02,038 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.011745 loss=118.000 weight=0.000 batch={'x': tensor([58, 60]), 'y': tensor([116, 120])}\n",
      "[2023-08-02 11:10:02,039 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.007971 loss=117.000 weight=0.000 batch={'x': tensor([56, 61]), 'y': tensor([112, 122])}\n",
      "[2023-08-02 11:10:02,041 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.004620 loss=52.000 weight=0.000 batch={'x': tensor([ 6, 46]), 'y': tensor([12, 92])}\n",
      "[2023-08-02 11:10:02,042 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.001644 loss=61.000 weight=0.000 batch={'x': tensor([40, 21]), 'y': tensor([80, 42])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        iter_based_trainer_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_based_trainer_demo = trainer_demo.copy()\n",
    "epoch_based_trainer_demo.type = 'CustomEpochBasedTrainer'\n",
    "epoch_based_trainer_demo.name = 'custom_epoch_based_trainer'\n",
    "epoch_based_trainer_demo.epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:02,058 13384:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:02,059 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 11:10:02,062 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.056687 loss=119.000 weight=0.000 batch={'x': tensor([62, 57]), 'y': tensor([124, 114])}\n",
      "[2023-08-02 11:10:02,064 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.045402 loss=80.000 weight=0.000 batch={'x': tensor([29, 51]), 'y': tensor([ 58, 102])}\n",
      "[2023-08-02 11:10:02,066 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.039515 loss=94.000 weight=0.000 batch={'x': tensor([61, 33]), 'y': tensor([122,  66])}\n",
      "[2023-08-02 11:10:02,068 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.035768 loss=68.000 weight=0.000 batch={'x': tensor([10, 58]), 'y': tensor([ 20, 116])}\n",
      "[2023-08-02 11:10:02,070 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.033307 loss=58.000 weight=0.000 batch={'x': tensor([14, 44]), 'y': tensor([28, 88])}\n",
      "[2023-08-02 11:10:02,072 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.031579 loss=88.000 weight=0.000 batch={'x': tensor([32, 56]), 'y': tensor([ 64, 112])}\n",
      "[2023-08-02 11:10:02,074 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 11:10:02,075 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.031760 loss=106.000 weight=0.000 batch={'x': tensor([47, 59]), 'y': tensor([ 94, 118])}\n",
      "[2023-08-02 11:10:02,077 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.028872 loss=63.000 weight=0.000 batch={'x': tensor([30, 33]), 'y': tensor([60, 66])}\n",
      "[2023-08-02 11:10:02,079 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.026081 loss=95.000 weight=0.000 batch={'x': tensor([28, 67]), 'y': tensor([ 56, 134])}\n",
      "[2023-08-02 11:10:02,081 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.023385 loss=87.000 weight=0.000 batch={'x': tensor([22, 65]), 'y': tensor([ 44, 130])}\n",
      "[2023-08-02 11:10:02,083 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.020766 loss=89.000 weight=0.000 batch={'x': tensor([44, 45]), 'y': tensor([88, 90])}\n",
      "[2023-08-02 11:10:02,085 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.018244 loss=47.000 weight=0.000 batch={'x': tensor([16, 31]), 'y': tensor([32, 62])}\n",
      "[2023-08-02 11:10:02,087 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.015901 loss=64.000 weight=0.000 batch={'x': tensor([51, 13]), 'y': tensor([102,  26])}\n",
      "[2023-08-02 11:10:02,089 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 11:10:02,090 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.014334 loss=65.000 weight=0.000 batch={'x': tensor([13, 52]), 'y': tensor([ 26, 104])}\n",
      "[2023-08-02 11:10:02,092 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.011935 loss=27.000 weight=0.000 batch={'x': tensor([10, 17]), 'y': tensor([20, 34])}\n",
      "[2023-08-02 11:10:02,094 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.009574 loss=107.000 weight=0.000 batch={'x': tensor([56, 51]), 'y': tensor([112, 102])}\n",
      "[2023-08-02 11:10:02,095 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.007304 loss=94.000 weight=0.000 batch={'x': tensor([40, 54]), 'y': tensor([ 80, 108])}\n",
      "[2023-08-02 11:10:02,097 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.005114 loss=88.000 weight=0.000 batch={'x': tensor([50, 38]), 'y': tensor([100,  76])}\n",
      "[2023-08-02 11:10:02,099 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.002967 loss=56.000 weight=0.000 batch={'x': tensor([32, 24]), 'y': tensor([64, 48])}\n",
      "[2023-08-02 11:10:02,101 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.000843 loss=57.000 weight=0.000 batch={'x': tensor([41, 16]), 'y': tensor([82, 32])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        epoch_based_trainer_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_callback_demo = validator_demo.copy()\n",
    "log_callback = log_callback_demo.callbacks\n",
    "log_callback.with_file_handler = True\n",
    "log_callback_demo.callbacks = [log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:02,119 13384:140704362395200][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:02,122 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.005148 loss=10.000\n",
      "[2023-08-02 11:10:02,123 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.003528 loss=20.000\n",
      "[2023-08-02 11:10:02,125 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.001726 loss=30.000\n",
      "[2023-08-02 11:10:02,127 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmputthl73d\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "    └── 2023-08-02T11-10-02_119194-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2023-08-02 11:10:02,119 13384:140704362395200][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-08-02 11:10:02,122 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.005148 loss=10.000\n",
      "[2023-08-02 11:10:02,123 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.003528 loss=20.000\n",
      "[2023-08-02 11:10:02,125 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.001726 loss=30.000\n",
      "[2023-08-02 11:10:02,127 13384:140704362395200][log.py:70 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        log_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_callback_demo = iter_based_trainer_demo.copy()\n",
    "optimize_callback = todd.Config(type='OptimizeCallback')\n",
    "optimize_callback_demo.callbacks = [optimize_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:02,672 13384:140704362395200][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:02,683 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.093504 loss=28.546 weight=0.607 batch={'x': tensor([11, 30]), 'y': tensor([22, 60])}\n",
      "[2023-08-02 11:10:02,687 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.057074 loss=46.480 weight=1.170 batch={'x': tensor([53, 59]), 'y': tensor([106, 118])}\n",
      "[2023-08-02 11:10:02,690 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.042243 loss=6.270 weight=2.110 batch={'x': tensor([50, 64]), 'y': tensor([100, 128])}\n",
      "[2023-08-02 11:10:02,693 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.032632 loss=6.024 weight=2.152 batch={'x': tensor([36, 43]), 'y': tensor([72, 86])}\n",
      "[2023-08-02 11:10:02,696 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.025535 loss=3.206 weight=1.887 batch={'x': tensor([25, 32]), 'y': tensor([50, 64])}\n",
      "[2023-08-02 11:10:02,733 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.045496 loss=1.485 weight=1.910 batch={'x': tensor([29,  4]), 'y': tensor([58,  8])}\n",
      "[2023-08-02 11:10:02,736 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.032322 loss=0.619 weight=1.977 batch={'x': tensor([ 8, 47]), 'y': tensor([16, 94])}\n",
      "[2023-08-02 11:10:02,739 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.021486 loss=3.024 weight=2.102 batch={'x': tensor([41, 18]), 'y': tensor([82, 36])}\n",
      "[2023-08-02 11:10:02,742 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.012254 loss=5.276 weight=1.842 batch={'x': tensor([13, 54]), 'y': tensor([ 26, 108])}\n",
      "[2023-08-02 11:10:02,745 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.004307 loss=3.094 weight=2.082 batch={'x': tensor([27, 48]), 'y': tensor([54, 96])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        optimize_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_schedule_callback = todd.Config(\n",
    "    type='LRScheduleCallback',\n",
    "    lr_scheduler=dict(type='LinearLR', total_iters=10),\n",
    ")\n",
    "lr_schedule_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_callback,\n",
    "    log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:02,770 13384:140704362395200][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:02,775 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.027667 loss=73.010 weight=0.262 batch={'x': tensor([47, 37]), 'y': tensor([94, 74])} lr=['3.333e-03']\n",
      "[2023-08-02 11:10:02,778 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.026428 loss=40.358 weight=1.003 batch={'x': tensor([63, 18]), 'y': tensor([126,  36])} lr=['5.000e-03']\n",
      "[2023-08-02 11:10:02,781 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.023981 loss=4.562 weight=1.875 batch={'x': tensor([27, 46]), 'y': tensor([54, 92])} lr=['5.000e-03']\n",
      "[2023-08-02 11:10:02,784 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.020704 loss=0.700 weight=1.930 batch={'x': tensor([ 7, 13]), 'y': tensor([14, 26])} lr=['5.000e-03']\n",
      "[2023-08-02 11:10:02,788 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.017916 loss=0.581 weight=2.012 batch={'x': tensor([35, 58]), 'y': tensor([ 70, 116])} lr=['5.000e-03']\n",
      "[2023-08-02 11:10:02,791 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.014662 loss=10.238 weight=1.775 batch={'x': tensor([25, 66]), 'y': tensor([ 50, 132])} lr=['5.000e-03']\n",
      "[2023-08-02 11:10:02,794 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.011489 loss=0.538 weight=1.987 batch={'x': tensor([55, 31]), 'y': tensor([110,  62])} lr=['5.000e-03']\n",
      "[2023-08-02 11:10:02,797 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.008138 loss=0.870 weight=1.980 batch={'x': tensor([30, 57]), 'y': tensor([ 60, 114])} lr=['5.000e-03']\n",
      "[2023-08-02 11:10:02,799 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.004913 loss=2.125 weight=2.042 batch={'x': tensor([36, 64]), 'y': tensor([ 72, 128])} lr=['5.000e-03']\n",
      "[2023-08-02 11:10:02,802 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.001827 loss=2.519 weight=2.077 batch={'x': tensor([24, 41]), 'y': tensor([48, 82])} lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "lr_schedule_by_epoch_callback = lr_schedule_callback.copy()\n",
    "lr_schedule_by_epoch_callback.by_epoch = True\n",
    "lr_schedule_by_epoch_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_by_epoch_callback,\n",
    "    log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:02,821 13384:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:02,822 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 11:10:02,825 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.069782 loss=66.560 weight=0.293 batch={'x': tensor([22, 56]), 'y': tensor([ 44, 112])} lr=['1.667e-03']\n",
      "[2023-08-02 11:10:02,828 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.060499 loss=29.213 weight=0.575 batch={'x': tensor([10, 31]), 'y': tensor([20, 62])} lr=['1.667e-03']\n",
      "[2023-08-02 11:10:02,831 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.056463 loss=36.549 weight=0.802 batch={'x': tensor([54,  7]), 'y': tensor([108,  14])} lr=['1.667e-03']\n",
      "[2023-08-02 11:10:02,835 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.053222 loss=28.654 weight=1.118 batch={'x': tensor([38, 27]), 'y': tensor([76, 54])} lr=['1.667e-03']\n",
      "[2023-08-02 11:10:02,838 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.050383 loss=40.005 weight=1.370 batch={'x': tensor([64, 63]), 'y': tensor([128, 126])} lr=['1.667e-03']\n",
      "[2023-08-02 11:10:02,841 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.046754 loss=5.850 weight=1.675 batch={'x': tensor([23, 13]), 'y': tensor([46, 26])} lr=['1.667e-03']\n",
      "[2023-08-02 11:10:02,844 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 11:10:02,845 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.045370 loss=2.732 weight=1.934 batch={'x': tensor([56, 27]), 'y': tensor([112,  54])} lr=['2.000e-03']\n",
      "[2023-08-02 11:10:02,848 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.041281 loss=1.096 weight=2.042 batch={'x': tensor([ 3, 49]), 'y': tensor([ 6, 98])} lr=['2.000e-03']\n",
      "[2023-08-02 11:10:02,851 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.037191 loss=0.152 weight=1.982 batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])} lr=['2.000e-03']\n",
      "[2023-08-02 11:10:02,854 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.033801 loss=0.754 weight=2.018 batch={'x': tensor([65, 18]), 'y': tensor([130,  36])} lr=['2.000e-03']\n",
      "[2023-08-02 11:10:02,857 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.030336 loss=1.903 weight=2.055 batch={'x': tensor([16, 53]), 'y': tensor([ 32, 106])} lr=['2.000e-03']\n",
      "[2023-08-02 11:10:02,860 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.027032 loss=1.167 weight=1.922 batch={'x': tensor([25,  5]), 'y': tensor([50, 10])} lr=['2.000e-03']\n",
      "[2023-08-02 11:10:02,863 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.023735 loss=2.315 weight=2.057 batch={'x': tensor([47, 34]), 'y': tensor([94, 68])} lr=['2.000e-03']\n",
      "[2023-08-02 11:10:02,866 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 11:10:02,868 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.020949 loss=0.409 weight=1.990 batch={'x': tensor([34, 44]), 'y': tensor([68, 88])} lr=['2.333e-03']\n",
      "[2023-08-02 11:10:02,871 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.017589 loss=4.263 weight=1.932 batch={'x': tensor([60, 66]), 'y': tensor([120, 132])} lr=['2.333e-03']\n",
      "[2023-08-02 11:10:02,874 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.014309 loss=3.754 weight=2.076 batch={'x': tensor([54, 45]), 'y': tensor([108,  90])} lr=['2.333e-03']\n",
      "[2023-08-02 11:10:02,877 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.011026 loss=1.374 weight=2.036 batch={'x': tensor([29, 47]), 'y': tensor([58, 94])} lr=['2.333e-03']\n",
      "[2023-08-02 11:10:02,880 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.007747 loss=0.963 weight=2.026 batch={'x': tensor([14, 61]), 'y': tensor([ 28, 122])} lr=['2.333e-03']\n",
      "[2023-08-02 11:10:02,883 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.004487 loss=3.255 weight=1.928 batch={'x': tensor([23, 67]), 'y': tensor([ 46, 134])} lr=['2.333e-03']\n",
      "[2023-08-02 11:10:02,886 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001277 loss=4.594 weight=1.913 batch={'x': tensor([62, 43]), 'y': tensor([124,  86])} lr=['2.333e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_by_epoch_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scaler_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_scaler_callback = todd.Config(\n",
    "    type='LRScaleCallback',\n",
    "    lr_scaler=dict(base_batch_size=1),\n",
    ")\n",
    "lr_scaler_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_scaler_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 11:10:02,903 13384:140704362395200][lr.py:92 todd.CustomIterBasedTrainer.custom_iter_based_trainer _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2023-08-02 11:10:02,904 13384:140704362395200][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:02,908 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.022032 loss=11.285 weight=1.630 batch={'x': tensor([45, 16]), 'y': tensor([90, 32])}\n",
      "[2023-08-02 11:10:02,911 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.022708 loss=2.253 weight=2.085 batch={'x': tensor([47,  6]), 'y': tensor([94, 12])}\n",
      "[2023-08-02 11:10:02,914 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.021566 loss=7.600 weight=2.190 batch={'x': tensor([14, 66]), 'y': tensor([ 28, 132])}\n",
      "[2023-08-02 11:10:02,917 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.019133 loss=17.325 weight=1.615 batch={'x': tensor([44, 46]), 'y': tensor([88, 92])}\n",
      "[2023-08-02 11:10:02,920 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.016323 loss=1.840 weight=1.885 batch={'x': tensor([ 9, 23]), 'y': tensor([18, 46])}\n",
      "[2023-08-02 11:10:02,923 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.013649 loss=22.755 weight=2.370 batch={'x': tensor([58, 65]), 'y': tensor([116, 130])}\n",
      "[2023-08-02 11:10:02,926 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.010610 loss=18.690 weight=1.580 batch={'x': tensor([38, 51]), 'y': tensor([ 76, 102])}\n",
      "[2023-08-02 11:10:02,929 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.007581 loss=11.393 weight=1.755 batch={'x': tensor([52, 41]), 'y': tensor([104,  82])}\n",
      "[2023-08-02 11:10:02,932 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.004618 loss=7.990 weight=2.170 batch={'x': tensor([35, 59]), 'y': tensor([ 70, 118])}\n",
      "[2023-08-02 11:10:02,935 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.001737 loss=3.900 weight=2.200 batch={'x': tensor([18, 21]), 'y': tensor([36, 42])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_scaler_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback_demo = iter_based_trainer_demo.copy()\n",
    "checkpoint_callback = todd.Config(type='CheckpointCallback', interval=10)\n",
    "checkpoint_callback_demo.callbacks = [checkpoint_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:02,961 13384:140704362395200][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:02,993 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.011914 loss=19.000 weight=0.000 batch={'x': tensor([ 1, 18]), 'y': tensor([ 2, 36])}\n",
      "[2023-08-02 11:10:02,997 13384:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_10lxbyi/custom_iter_based_trainer/checkpoints/iter_10\n",
      "[2023-08-02 11:10:03,000 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.035196 loss=122.000 weight=0.000 batch={'x': tensor([62, 60]), 'y': tensor([124, 120])}\n",
      "[2023-08-02 11:10:03,003 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.028297 loss=69.000 weight=0.000 batch={'x': tensor([38, 31]), 'y': tensor([76, 62])}\n",
      "[2023-08-02 11:10:03,006 13384:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_10lxbyi/custom_iter_based_trainer/checkpoints/iter_20\n",
      "[2023-08-02 11:10:03,008 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.026737 loss=78.000 weight=0.000 batch={'x': tensor([28, 50]), 'y': tensor([ 56, 100])}\n",
      "[2023-08-02 11:10:03,011 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.020952 loss=20.000 weight=0.000 batch={'x': tensor([14,  6]), 'y': tensor([28, 12])}\n",
      "[2023-08-02 11:10:03,013 13384:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_10lxbyi/custom_iter_based_trainer/checkpoints/iter_30\n",
      "[2023-08-02 11:10:03,015 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.017851 loss=87.000 weight=0.000 batch={'x': tensor([39, 48]), 'y': tensor([78, 96])}\n",
      "[2023-08-02 11:10:03,018 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.013332 loss=98.000 weight=0.000 batch={'x': tensor([45, 53]), 'y': tensor([ 90, 106])}\n",
      "[2023-08-02 11:10:03,020 13384:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_10lxbyi/custom_iter_based_trainer/checkpoints/iter_40\n",
      "[2023-08-02 11:10:03,023 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.009951 loss=6.000 weight=0.000 batch={'x': tensor([4, 2]), 'y': tensor([8, 4])}\n",
      "[2023-08-02 11:10:03,025 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.005919 loss=62.000 weight=0.000 batch={'x': tensor([26, 36]), 'y': tensor([52, 72])}\n",
      "[2023-08-02 11:10:03,032 13384:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_10lxbyi/custom_iter_based_trainer/checkpoints/iter_50\n",
      "[2023-08-02 11:10:03,034 13384:140704362395200][log.py:70 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.002546 loss=75.000 weight=0.000 batch={'x': tensor([40, 35]), 'y': tensor([80, 70])}\n",
      "[2023-08-02 11:10:03,039 13384:140704362395200][checkpoint.py:59 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_10lxbyi/custom_iter_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_10lxbyi\u001b[0m\n",
      "└── \u001b[1;36mcustom_iter_based_trainer\u001b[0m\n",
      "    ├── 2023-08-02T11-10-02_960868-08-00.log\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_20\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_30\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_40\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_50\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[1;36mlatest\u001b[0m\n",
      "            ├── callbacks.pth\n",
      "            ├── meta.pth\n",
      "            ├── model.pth\n",
      "            ├── optim.pth\n",
      "            └── strategy.pth\n",
      "\n",
      "9 directories, 31 files\n",
      "\n",
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 50}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_50 = pathlib.Path(work_dirs) / 'custom_iter_based_trainer' / 'checkpoints' / 'iter_50'\n",
    "    for f in iter_50.glob('*.pth'):\n",
    "        print(f\"{f.name}:\")\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "checkpoint_by_epoch_callback = checkpoint_callback.copy()\n",
    "checkpoint_by_epoch_callback.update(interval=1, by_epoch=True)\n",
    "checkpoint_by_epoch_callback_demo.callbacks = [\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:03,467 13384:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:03,468 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 11:10:03,472 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.081441 loss=94.000 weight=0.000 batch={'x': tensor([31, 63]), 'y': tensor([ 62, 126])}\n",
      "[2023-08-02 11:10:03,475 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.064198 loss=51.000 weight=0.000 batch={'x': tensor([28, 23]), 'y': tensor([56, 46])}\n",
      "[2023-08-02 11:10:03,477 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.054532 loss=105.000 weight=0.000 batch={'x': tensor([41, 64]), 'y': tensor([ 82, 128])}\n",
      "[2023-08-02 11:10:03,480 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.049922 loss=88.000 weight=0.000 batch={'x': tensor([56, 32]), 'y': tensor([112,  64])}\n",
      "[2023-08-02 11:10:03,483 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.046391 loss=63.000 weight=0.000 batch={'x': tensor([48, 15]), 'y': tensor([96, 30])}\n",
      "[2023-08-02 11:10:03,486 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.043205 loss=105.000 weight=0.000 batch={'x': tensor([66, 39]), 'y': tensor([132,  78])}\n",
      "[2023-08-02 11:10:03,489 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8t9yrhts/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-02 11:10:03,492 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 11:10:03,495 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.051249 loss=77.000 weight=0.000 batch={'x': tensor([49, 28]), 'y': tensor([98, 56])}\n",
      "[2023-08-02 11:10:03,497 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.045430 loss=101.000 weight=0.000 batch={'x': tensor([64, 37]), 'y': tensor([128,  74])}\n",
      "[2023-08-02 11:10:03,501 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.041442 loss=57.000 weight=0.000 batch={'x': tensor([54,  3]), 'y': tensor([108,   6])}\n",
      "[2023-08-02 11:10:03,505 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.038586 loss=28.000 weight=0.000 batch={'x': tensor([ 9, 19]), 'y': tensor([18, 38])}\n",
      "[2023-08-02 11:10:03,509 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.034909 loss=83.000 weight=0.000 batch={'x': tensor([58, 25]), 'y': tensor([116,  50])}\n",
      "[2023-08-02 11:10:03,512 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.030798 loss=5.000 weight=0.000 batch={'x': tensor([1, 4]), 'y': tensor([2, 8])}\n",
      "[2023-08-02 11:10:03,516 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.027156 loss=45.000 weight=0.000 batch={'x': tensor([16, 29]), 'y': tensor([32, 58])}\n",
      "[2023-08-02 11:10:03,518 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8t9yrhts/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-02 11:10:03,521 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 11:10:03,524 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.025413 loss=63.000 weight=0.000 batch={'x': tensor([31, 32]), 'y': tensor([62, 64])}\n",
      "[2023-08-02 11:10:03,527 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.021303 loss=54.000 weight=0.000 batch={'x': tensor([49,  5]), 'y': tensor([98, 10])}\n",
      "[2023-08-02 11:10:03,530 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.016941 loss=78.000 weight=0.000 batch={'x': tensor([54, 24]), 'y': tensor([108,  48])}\n",
      "[2023-08-02 11:10:03,532 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.012848 loss=93.000 weight=0.000 batch={'x': tensor([58, 35]), 'y': tensor([116,  70])}\n",
      "[2023-08-02 11:10:03,534 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.008850 loss=35.000 weight=0.000 batch={'x': tensor([20, 15]), 'y': tensor([40, 30])}\n",
      "[2023-08-02 11:10:03,539 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.005219 loss=60.000 weight=0.000 batch={'x': tensor([37, 23]), 'y': tensor([74, 46])}\n",
      "[2023-08-02 11:10:03,542 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001480 loss=38.000 weight=0.000 batch={'x': tensor([ 2, 36]), 'y': tensor([ 4, 72])}\n",
      "[2023-08-02 11:10:03,544 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8t9yrhts/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-02 11:10:03,547 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8t9yrhts/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8t9yrhts\u001b[0m\n",
      "└── \u001b[1;36mcustom_epoch_based_trainer\u001b[0m\n",
      "    ├── 2023-08-02T11-10-03_467398-08-00.log\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[1;36mlatest\u001b[0m\n",
      "            ├── callbacks.pth\n",
      "            ├── meta.pth\n",
      "            ├── model.pth\n",
      "            ├── optim.pth\n",
      "            └── strategy.pth\n",
      "\n",
      "7 directories, 21 files\n",
      "\n",
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {}}\n",
      "\n",
      "meta.pth:\n",
      "{'epoch': 2, 'iter_': 68}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_by_epoch_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'custom_epoch_based_trainer' / 'checkpoints' / 'epoch_2'\n",
    "    for f in epoch_2.glob('*.pth'):\n",
    "        print(f\"{f.name}:\")\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_load_from_callback_demo = checkpoint_by_epoch_callback_demo.copy()\n",
    "checkpoint_load_from_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:03,971 13384:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:03,975 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 11:10:03,982 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.128952 loss=38.122 weight=0.505 batch={'x': tensor([ 7, 44]), 'y': tensor([14, 88])}\n",
      "[2023-08-02 11:10:03,995 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.184423 loss=18.270 weight=1.420 batch={'x': tensor([ 6, 57]), 'y': tensor([ 12, 114])}\n",
      "[2023-08-02 11:10:04,000 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.143852 loss=3.006 weight=2.093 batch={'x': tensor([39, 26]), 'y': tensor([78, 52])}\n",
      "[2023-08-02 11:10:04,021 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.187579 loss=0.075 weight=1.998 batch={'x': tensor([15, 45]), 'y': tensor([30, 90])}\n",
      "[2023-08-02 11:10:04,060 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.259616 loss=3.570 weight=1.873 batch={'x': tensor([53,  3]), 'y': tensor([106,   6])}\n",
      "[2023-08-02 11:10:04,064 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.212364 loss=4.000 weight=2.125 batch={'x': tensor([ 1, 63]), 'y': tensor([  2, 126])}\n",
      "[2023-08-02 11:10:04,067 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7q1kw15s/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-02 11:10:04,070 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 11:10:04,074 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.189491 loss=0.113 weight=2.003 batch={'x': tensor([55, 35]), 'y': tensor([110,  70])}\n",
      "[2023-08-02 11:10:04,078 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.158970 loss=7.030 weight=2.190 batch={'x': tensor([41, 33]), 'y': tensor([82, 66])}\n",
      "[2023-08-02 11:10:04,082 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.134668 loss=0.844 weight=1.978 batch={'x': tensor([16, 59]), 'y': tensor([ 32, 118])}\n",
      "[2023-08-02 11:10:04,085 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.114510 loss=1.460 weight=2.183 batch={'x': tensor([13,  3]), 'y': tensor([26,  6])}\n",
      "[2023-08-02 11:10:04,090 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.097879 loss=4.041 weight=1.848 batch={'x': tensor([ 2, 51]), 'y': tensor([  4, 102])}\n",
      "[2023-08-02 11:10:04,094 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.083442 loss=5.850 weight=1.870 batch={'x': tensor([58, 32]), 'y': tensor([116,  64])}\n",
      "[2023-08-02 11:10:04,098 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.069957 loss=5.267 weight=1.785 batch={'x': tensor([30, 19]), 'y': tensor([60, 38])}\n",
      "[2023-08-02 11:10:04,101 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7q1kw15s/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-02 11:10:04,103 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 11:10:04,107 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.060358 loss=4.203 weight=2.103 batch={'x': tensor([57, 25]), 'y': tensor([114,  50])}\n",
      "[2023-08-02 11:10:04,111 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.048886 loss=4.001 weight=1.918 batch={'x': tensor([58, 39]), 'y': tensor([116,  78])}\n",
      "[2023-08-02 11:10:04,114 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.038260 loss=3.520 weight=1.890 batch={'x': tensor([ 8, 56]), 'y': tensor([ 16, 112])}\n",
      "[2023-08-02 11:10:04,118 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.028530 loss=3.527 weight=1.915 batch={'x': tensor([51, 32]), 'y': tensor([102,  64])}\n",
      "[2023-08-02 11:10:04,122 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.019517 loss=0.919 weight=2.123 batch={'x': tensor([6, 9]), 'y': tensor([12, 18])}\n",
      "[2023-08-02 11:10:04,125 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.011055 loss=9.652 weight=1.753 batch={'x': tensor([19, 59]), 'y': tensor([ 38, 118])}\n",
      "[2023-08-02 11:10:04,129 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.003066 loss=2.557 weight=1.835 batch={'x': tensor([ 4, 27]), 'y': tensor([ 8, 54])}\n",
      "[2023-08-02 11:10:04,131 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7q1kw15s/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-02 11:10:04,134 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7q1kw15s/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-02 11:10:04,521 13384:140704362395200][checkpoint.py:44 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7q1kw15s/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "\u001b[2m[2023-08-02 11:10:04,528 13384:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:04,529 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 11:10:04,533 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.064496 loss=2.870 weight=2.103 batch={'x': tensor([19, 37]), 'y': tensor([38, 74])}\n",
      "[2023-08-02 11:10:04,536 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.028763 loss=3.840 weight=1.920 batch={'x': tensor([34, 62]), 'y': tensor([ 68, 124])}\n",
      "[2023-08-02 11:10:04,540 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.020438 loss=3.710 weight=1.868 batch={'x': tensor([33, 23]), 'y': tensor([66, 46])}\n",
      "[2023-08-02 11:10:04,543 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.014255 loss=3.386 weight=2.108 batch={'x': tensor([15, 48]), 'y': tensor([30, 96])}\n",
      "[2023-08-02 11:10:04,546 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.009477 loss=4.999 weight=1.893 batch={'x': tensor([28, 65]), 'y': tensor([ 56, 130])}\n",
      "[2023-08-02 11:10:04,549 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.005283 loss=12.960 weight=1.760 batch={'x': tensor([41, 67]), 'y': tensor([ 82, 134])}\n",
      "[2023-08-02 11:10:04,552 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001460 loss=3.180 weight=2.060 batch={'x': tensor([66, 40]), 'y': tensor([132,  80])}\n",
      "[2023-08-02 11:10:04,554 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7q1kw15s/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-02 11:10:04,557 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7q1kw15s/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_load_from_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_load_from_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "        load_from=os.path.join(work_dirs, 'custom_epoch_based_trainer', 'checkpoints', 'epoch_2')\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaultyRunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError('faulty runner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyValidator(FaultyRunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyIterBasedTrainer(FaultyRunnerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyEpochBasedTrainer(\n",
    "    FaultyRunnerMixin,\n",
    "    todd.runners.EpochBasedTrainer,\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_callback_demo = validator_demo.copy()\n",
    "monitor_callback_demo.type = 'FaultyValidator'\n",
    "monitor_callback = todd.Config(type='MonitorCallback')\n",
    "monitor_callback_demo.callbacks = [monitor_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:04,609 13384:140704362395200][base.py:53 todd.FaultyValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2023-08-02 11:10:04,611 13384:140704362395200][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x14a486310>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 193, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_13384/2137902126.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2023-08-02 11:10:04,609 13384:140704362395200][base.py:53 todd.FaultyValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-08-02 11:10:04,611 13384:140704362395200][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x14a486310>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 193, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_13384/2137902126.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        monitor_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_load_model_from_demo = checkpoint_load_from_callback_demo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:04,927 13384:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:04,929 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 11:10:04,933 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.083401 loss=10.986 weight=0.707 batch={'x': tensor([ 5, 12]), 'y': tensor([10, 24])}\n",
      "[2023-08-02 11:10:04,936 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.071732 loss=14.455 weight=1.510 batch={'x': tensor([53,  6]), 'y': tensor([106,  12])}\n",
      "[2023-08-02 11:10:04,940 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.067744 loss=2.938 weight=2.125 batch={'x': tensor([22, 25]), 'y': tensor([44, 50])}\n",
      "[2023-08-02 11:10:04,944 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.062234 loss=0.244 weight=2.013 batch={'x': tensor([ 7, 32]), 'y': tensor([14, 64])}\n",
      "[2023-08-02 11:10:04,947 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.056604 loss=3.125 weight=2.062 batch={'x': tensor([36, 64]), 'y': tensor([ 72, 128])}\n",
      "[2023-08-02 11:10:04,950 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.051638 loss=7.838 weight=2.138 batch={'x': tensor([60, 54]), 'y': tensor([120, 108])}\n",
      "[2023-08-02 11:10:04,954 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9z1rxd0g/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-02 11:10:04,956 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 11:10:04,959 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.058070 loss=5.062 weight=1.888 batch={'x': tensor([48, 42]), 'y': tensor([96, 84])}\n",
      "[2023-08-02 11:10:04,962 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.052638 loss=1.140 weight=1.970 batch={'x': tensor([26, 50]), 'y': tensor([ 52, 100])}\n",
      "[2023-08-02 11:10:04,966 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.047161 loss=5.059 weight=2.178 batch={'x': tensor([17, 40]), 'y': tensor([34, 80])}\n",
      "[2023-08-02 11:10:04,969 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.042384 loss=3.750 weight=2.100 batch={'x': tensor([54, 21]), 'y': tensor([108,  42])}\n",
      "[2023-08-02 11:10:04,973 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.038203 loss=1.998 weight=2.118 batch={'x': tensor([27,  7]), 'y': tensor([54, 14])}\n",
      "[2023-08-02 11:10:04,977 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.033834 loss=1.105 weight=1.958 batch={'x': tensor([19, 33]), 'y': tensor([38, 66])}\n",
      "[2023-08-02 11:10:04,980 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.029291 loss=0.556 weight=1.988 batch={'x': tensor([55, 34]), 'y': tensor([110,  68])}\n",
      "[2023-08-02 11:10:04,982 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9z1rxd0g/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-02 11:10:04,985 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 11:10:04,988 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.027172 loss=1.305 weight=2.045 batch={'x': tensor([35, 23]), 'y': tensor([70, 46])}\n",
      "[2023-08-02 11:10:04,991 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.022546 loss=0.821 weight=2.023 batch={'x': tensor([21, 52]), 'y': tensor([ 42, 104])}\n",
      "[2023-08-02 11:10:04,994 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.018036 loss=7.154 weight=1.853 batch={'x': tensor([39, 58]), 'y': tensor([ 78, 116])}\n",
      "[2023-08-02 11:10:04,997 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.013677 loss=6.986 weight=2.203 batch={'x': tensor([19, 50]), 'y': tensor([ 38, 100])}\n",
      "[2023-08-02 11:10:05,000 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.009537 loss=3.442 weight=1.915 batch={'x': tensor([43, 38]), 'y': tensor([86, 76])}\n",
      "[2023-08-02 11:10:05,003 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.005486 loss=3.135 weight=2.143 batch={'x': tensor([34, 10]), 'y': tensor([68, 20])}\n",
      "[2023-08-02 11:10:05,006 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001556 loss=0.880 weight=2.080 batch={'x': tensor([ 9, 13]), 'y': tensor([18, 26])}\n",
      "[2023-08-02 11:10:05,008 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9z1rxd0g/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-02 11:10:05,011 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9z1rxd0g/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-02 11:10:05,402 13384:140704362395200][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-02 11:10:05,403 13384:140704362395200][base.py:64 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9z1rxd0g/custom_epoch_based_trainer/checkpoints/epoch_2/model.pth\n",
      "[2023-08-02 11:10:05,406 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-02 11:10:05,411 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.084002 loss=7.999 weight=2.203 batch={'x': tensor([50, 29]), 'y': tensor([100,  58])}\n",
      "[2023-08-02 11:10:05,414 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.070748 loss=2.130 weight=2.060 batch={'x': tensor([24, 47]), 'y': tensor([48, 94])}\n",
      "[2023-08-02 11:10:05,417 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.063058 loss=0.875 weight=1.965 batch={'x': tensor([49,  1]), 'y': tensor([98,  2])}\n",
      "[2023-08-02 11:10:05,421 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.058175 loss=5.228 weight=2.128 batch={'x': tensor([38, 44]), 'y': tensor([76, 88])}\n",
      "[2023-08-02 11:10:05,424 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.054202 loss=1.295 weight=2.093 batch={'x': tensor([17, 11]), 'y': tensor([34, 22])}\n",
      "[2023-08-02 11:10:05,427 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.049975 loss=6.184 weight=1.873 batch={'x': tensor([30, 67]), 'y': tensor([ 60, 134])}\n",
      "[2023-08-02 11:10:05,430 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9z1rxd0g/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-02 11:10:05,432 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-02 11:10:05,435 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.054174 loss=8.125 weight=1.675 batch={'x': tensor([35, 15]), 'y': tensor([70, 30])}\n",
      "[2023-08-02 11:10:05,438 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.049287 loss=8.196 weight=1.803 batch={'x': tensor([24, 59]), 'y': tensor([ 48, 118])}\n",
      "[2023-08-02 11:10:05,441 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.044349 loss=1.237 weight=1.955 batch={'x': tensor([52,  3]), 'y': tensor([104,   6])}\n",
      "[2023-08-02 11:10:05,445 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.039713 loss=0.125 weight=2.005 batch={'x': tensor([41,  9]), 'y': tensor([82, 18])}\n",
      "[2023-08-02 11:10:05,448 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.035267 loss=5.234 weight=1.868 batch={'x': tensor([19, 60]), 'y': tensor([ 38, 120])}\n",
      "[2023-08-02 11:10:05,451 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.031028 loss=1.721 weight=2.043 batch={'x': tensor([25, 56]), 'y': tensor([ 50, 112])}\n",
      "[2023-08-02 11:10:05,454 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.027024 loss=3.713 weight=2.135 batch={'x': tensor([ 4, 51]), 'y': tensor([  8, 102])}\n",
      "[2023-08-02 11:10:05,457 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9z1rxd0g/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-02 11:10:05,459 13384:140704362395200][log.py:76 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-02 11:10:05,462 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.025232 loss=2.668 weight=2.055 batch={'x': tensor([39, 58]), 'y': tensor([ 78, 116])}\n",
      "[2023-08-02 11:10:05,465 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.020933 loss=3.200 weight=1.920 batch={'x': tensor([49, 31]), 'y': tensor([98, 62])}\n",
      "[2023-08-02 11:10:05,467 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.016783 loss=10.120 weight=1.770 batch={'x': tensor([55, 33]), 'y': tensor([110,  66])}\n",
      "[2023-08-02 11:10:05,470 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.012776 loss=2.890 weight=1.915 batch={'x': tensor([11, 57]), 'y': tensor([ 22, 114])}\n",
      "[2023-08-02 11:10:05,474 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.008955 loss=0.699 weight=2.033 batch={'x': tensor([17, 26]), 'y': tensor([34, 52])}\n",
      "[2023-08-02 11:10:05,477 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.005203 loss=1.845 weight=1.955 batch={'x': tensor([35, 47]), 'y': tensor([70, 94])}\n",
      "[2023-08-02 11:10:05,508 13384:140704362395200][log.py:70 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.002031 loss=5.088 weight=2.138 batch={'x': tensor([66,  8]), 'y': tensor([132,  16])}\n",
      "[2023-08-02 11:10:05,510 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9z1rxd0g/custom_epoch_based_trainer/checkpoints/epoch_3\n",
      "[2023-08-02 11:10:05,513 13384:140704362395200][checkpoint.py:59 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp9z1rxd0g/custom_epoch_based_trainer/checkpoints/latest\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        strategy_load_model_from_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        strategy_load_model_from_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.load_model_from(os.path.join(work_dirs, 'custom_epoch_based_trainer', 'checkpoints', 'epoch_2', 'model.pth'))\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
