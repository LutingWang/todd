{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install --no-build-isolation --extra-index-url https://pypi.org/simple .. > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-08 02:29:39,446 88602:140704380690048][patches.py:9 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n",
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import todd\n",
    "from todd.runners import Memo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register_()\n",
    "class RunnerModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def _forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        runner: todd.runners.BaseRunner,\n",
    "        batch,\n",
    "        memo: Memo,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> Memo:\n",
    "        log: dict[str, Any] | None = memo.get(\"log\")\n",
    "        y = self._forward(batch[\"x\"])\n",
    "        loss = F.l1_loss(y, batch[\"y\"])\n",
    "        memo[\"loss\"] = loss\n",
    "        if log is not None:\n",
    "            log[\"batch\"] = str(batch)\n",
    "            log[\"weight\"] = f\"{self._weight.item():.3f}\"\n",
    "            log[\"loss\"] = f\"{loss:.3f}\"\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register_()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:29:41,509 88602:140704380690048][base.py:55 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7a4uee5t\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:29:41,865 88602:140704380690048][base.py:55 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:29:41,872 88602:140704380690048][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-08 02:29:41,881 88602:140704380690048][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-08 02:29:41,891 88602:140704380690048][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-08 02:29:41,924 88602:140704380690048][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpr4b_9xdt\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[dict(type='LogCallback', interval=5)],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:29:42,236 88602:140704380690048][base.py:55 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:29:42,242 88602:140704380690048][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [1/8] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.000 loss=14.000\n",
      "[2024-02-08 02:29:42,243 88602:140704380690048][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [2/8] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.000 loss=11.000\n",
      "[2024-02-08 02:29:42,245 88602:140704380690048][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [3/8] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.000 loss=5.000\n",
      "[2024-02-08 02:29:42,247 88602:140704380690048][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [4/8] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.000 loss=14.000\n",
      "[2024-02-08 02:29:42,249 88602:140704380690048][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [5/8] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.000 loss=11.000\n",
      "[2024-02-08 02:29:42,250 88602:140704380690048][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [6/8] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.000 loss=14.000\n",
      "[2024-02-08 02:29:42,252 88602:140704380690048][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [7/8] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.000 loss=11.000\n",
      "[2024-02-08 02:29:42,253 88602:140704380690048][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [8/8] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.000 loss=5.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"iter_based_trainer\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:29:42,266 88602:140704380690048][base.py:55 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:29:42,268 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-08 02:29:42,270 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [1/15] batch={'x': tensor([6, 4]), 'y': tensor([12,  8])} weight=0.000 loss=10.000\n",
      "[2024-02-08 02:29:42,272 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [2/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.000 loss=12.000\n",
      "[2024-02-08 02:29:42,274 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [3/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.000 loss=11.000\n",
      "[2024-02-08 02:29:42,277 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [4/15] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.000 loss=11.000\n",
      "[2024-02-08 02:29:42,278 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [5/15] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.000 loss=11.000\n",
      "[2024-02-08 02:29:42,279 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-08 02:29:42,281 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [6/15] batch={'x': tensor([10,  5]), 'y': tensor([20, 10])} weight=0.000 loss=15.000\n",
      "[2024-02-08 02:29:42,283 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [7/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.000 loss=3.000\n",
      "[2024-02-08 02:29:42,285 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [8/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.000 loss=12.000\n",
      "[2024-02-08 02:29:42,286 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [9/15] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.000 loss=15.000\n",
      "[2024-02-08 02:29:42,288 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [10/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.000 loss=10.000\n",
      "[2024-02-08 02:29:42,289 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-08 02:29:42,291 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [11/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.000 loss=4.000\n",
      "[2024-02-08 02:29:42,292 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [12/15] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.000 loss=13.000\n",
      "[2024-02-08 02:29:42,294 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [13/15] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.000 loss=14.000\n",
      "[2024-02-08 02:29:42,295 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [14/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.000 loss=12.000\n",
      "[2024-02-08 02:29:42,297 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [15/15] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.000 loss=12.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"epoch_based_trainer\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-08 02:29:42,435 88602:140704380690048][log.py:55 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.7.0\n",
      "todd_version: 0.4.0\n",
      "cuda_home: None\n",
      "git_commit_id: 315b3b2\n",
      "git_status: \n",
      "M todd/runners/callbacks/composed.py\n",
      " M todd/utils/misc.py\n",
      " M todd/utils/torch.py\n",
      " M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-02-08 02:29:42,437 88602:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:29:42,442 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-08 02:29:42,446 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-08 02:29:42,449 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-08 02:29:42,453 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:29:42,478 88602:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:29:42,484 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-08 02:29:42,488 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-08 02:29:42,492 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-08 02:29:42,496 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpqfpbk8sm\u001b[0m\n",
      "└── \u001b[1;36mlog_callback\u001b[0m\n",
      "    └── 2024-02-08T02-29-42_477193-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2024-02-08 02:29:42,478 88602:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2024-02-08 02:29:42,484 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-08 02:29:42,488 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-08 02:29:42,492 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-08 02:29:42,496 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='log_callback',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type='LogCallback',\n",
    "            interval=5,\n",
    "            with_file_handler=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/log_callback/*.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:29:43,110 88602:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:29:43,649 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:01 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-08 02:29:44,167 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:01 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-08 02:29:44,687 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-08 02:29:45,208 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:29:45,223 88602:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:29:46,740 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:03 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-08 02:29:50,759 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:04 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-08 02:29:55,780 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:03 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-08 02:30:00,796 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"EMA_ETA\", decay=0.2),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1 * min(10, runner.iter_))\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-08 02:30:00,948 88602:140704380690048][log.py:55 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.7.0\n",
      "todd_version: 0.4.0\n",
      "cuda_home: None\n",
      "git_commit_id: 315b3b2\n",
      "git_status: \n",
      "M todd/runners/callbacks/composed.py\n",
      " M todd/utils/misc.py\n",
      " M todd/utils/torch.py\n",
      " M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-02-08 02:30:00,950 88602:140704380690048][base.py:55 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:00,955 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:00 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-08 02:30:00,958 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:00 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-08 02:30:00,962 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-08 02:30:00,966 88602:140704380690048][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "            with_file_handler=True,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-08 02:30:01,045 88602:140704380690048][git.py:41 todd.Validator.git_callback init] INFO: Saving git diff to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfz__u5vg/git_callback/git_diff_2024-02-08T02-30-01_045098-08-00.log\n",
      "\u001b[2m[2024-02-08 02:30:01,048 88602:140704380690048][base.py:55 todd.Validator.git_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff --git a/todd/runners/callbacks/composed.py b/todd/runners/callbacks/composed.py\n",
      "index 772fb59..cb5285a 100644\n",
      "--- a/todd/runners/callbacks/composed.py\n",
      "+++ b/todd/runners/callbacks/composed.py\n",
      "@@ -2,139 +2,109 @@ __all__ = [\n",
      "     'ComposedCallback',\n",
      " ]\n",
      " \n",
      "-from collections import UserList\n",
      "-from typing import Any, Iterable, Literal, Mapping, TypedDict\n",
      "+from typing import Any, Iterable, Literal, Mapping\n",
      "+\n",
      "+from ...utils import PriorityQueue\n",
      " \n",
      " from ...base import CallbackRegistry, Config\n",
      " from .base import BaseCallback\n",
      " \n",
      "-\n",
      "-class Priority(TypedDict, total=False):\n",
      "-    init: int\n",
      "-    should_break: int\n",
      "-    should_continue: int\n",
      "-    before_run_iter: int\n",
      "-    run_iter_context: int\n",
      "-    after_run_iter: int\n",
      "-    should_break_epoch: int\n",
      "-    should_continue_epoch: int\n",
      "-    before_run_epoch: int\n",
      "-    run_epoch_context: int\n",
      "-    after_run_epoch: int\n",
      "-    before_run: int\n",
      "-    after_run: int\n",
      "-\n",
      "-\n",
      "-CallbackNames = Literal['init', 'should_break', 'should_continue',\n",
      "-                        'before_run_iter', 'run_iter_context',\n",
      "-                        'after_run_iter', 'should_break_epoch',\n",
      "-                        'should_continue_epoch', 'before_run_epoch',\n",
      "-                        'run_epoch_context', 'after_run_epoch', 'before_run',\n",
      "-                        'after_run']\n",
      "+KT = Literal['init', 'should_break', 'should_continue', 'before_run_iter',\n",
      "+             'run_iter_context', 'after_run_iter', 'should_break_epoch',\n",
      "+             'should_continue_epoch', 'before_run_epoch', 'run_epoch_context',\n",
      "+             'after_run_epoch', 'before_run', 'after_run']\n",
      " \n",
      " \n",
      " @CallbackRegistry.register_()\n",
      "-class ComposedCallback(BaseCallback, UserList[BaseCallback]):\n",
      "+class ComposedCallback(BaseCallback):\n",
      " \n",
      "     def __init__(self, *args, callbacks: Iterable[Config], **kwargs) -> None:\n",
      "-        self._priorities: list[Priority] = [\n",
      "-            c.pop('priority', dict()) for c in callbacks\n",
      "-        ]\n",
      "         super().__init__(*args, **kwargs)\n",
      "-        self.extend(\n",
      "+        priorities = [c.pop('priority', dict()) for c in callbacks]\n",
      "+        queue = [\n",
      "             CallbackRegistry.build(c, runner=self._runner) for c in callbacks\n",
      "-        )\n",
      "-\n",
      "-    @property\n",
      "-    def priorities(self) -> list[Priority]:\n",
      "-        return self._priorities\n",
      "-\n",
      "-    def _callbacks(self, name: CallbackNames) -> list[BaseCallback]:\n",
      "-        assert len(self) == len(self._priorities)\n",
      "-        if len(self) == 0:\n",
      "-            return []\n",
      "-        priorities = [p.get(name, 0) for p in self._priorities]\n",
      "-        priority_index = [(p, i) for i, p in enumerate(priorities)]\n",
      "-        priority_index = sorted(priority_index)\n",
      "-        _, indices = zip(*priority_index)\n",
      "-        return [self[i] for i in indices]\n",
      "+        ]\n",
      "+        self._priority_queue: PriorityQueue[KT, BaseCallback] = \\\n",
      "+            PriorityQueue(priorities, queue)\n",
      " \n",
      "     def init(self) -> None:\n",
      "         super().init()\n",
      "-        for c in self._callbacks('init'):\n",
      "+        for c in self._priority_queue('init'):\n",
      "             c.init()\n",
      " \n",
      "     def should_break(self, *args, **kwargs) -> bool:\n",
      "         super().should_break(*args, **kwargs)\n",
      "         return any(\n",
      "             c.should_break(*args, **kwargs)\n",
      "-            for c in self._callbacks('should_break')\n",
      "+            for c in self._priority_queue('should_break')\n",
      "         )\n",
      " \n",
      "     def should_continue(self, *args, **kwargs) -> bool:\n",
      "         super().should_continue(*args, **kwargs)\n",
      "         return any(\n",
      "             c.should_continue(*args, **kwargs)\n",
      "-            for c in self._callbacks('should_continue')\n",
      "+            for c in self._priority_queue('should_continue')\n",
      "         )\n",
      " \n",
      "     def before_run_iter(self, *args, **kwargs) -> None:\n",
      "         super().before_run_iter(*args, **kwargs)\n",
      "-        for c in self._callbacks('before_run_iter'):\n",
      "+        for c in self._priority_queue('before_run_iter'):\n",
      "             c.before_run_iter(*args, **kwargs)\n",
      " \n",
      "     def run_iter_context(self, *args, **kwargs) -> None:\n",
      "         super().run_iter_context(*args, **kwargs)\n",
      "-        for c in self._callbacks('run_iter_context'):\n",
      "+        for c in self._priority_queue('run_iter_context'):\n",
      "             c.run_iter_context(*args, **kwargs)\n",
      " \n",
      "     def after_run_iter(self, *args, **kwargs) -> None:\n",
      "         super().after_run_iter(*args, **kwargs)\n",
      "-        for c in self._callbacks('after_run_iter'):\n",
      "+        for c in self._priority_queue('after_run_iter'):\n",
      "             c.after_run_iter(*args, **kwargs)\n",
      " \n",
      "     def should_break_epoch(self, *args, **kwargs) -> bool:\n",
      "         super().should_break_epoch(*args, **kwargs)\n",
      "         return any(\n",
      "             c.should_break_epoch(*args, **kwargs)\n",
      "-            for c in self._callbacks('should_break_epoch')\n",
      "+            for c in self._priority_queue('should_break_epoch')\n",
      "         )\n",
      " \n",
      "     def should_continue_epoch(self, *args, **kwargs) -> bool:\n",
      "         super().should_continue_epoch(*args, **kwargs)\n",
      "         return any(\n",
      "             c.should_continue_epoch(*args, **kwargs)\n",
      "-            for c in self._callbacks('should_continue_epoch')\n",
      "+            for c in self._priority_queue('should_continue_epoch')\n",
      "         )\n",
      " \n",
      "     def before_run_epoch(self, *args, **kwargs) -> None:\n",
      "         super().before_run_epoch(*args, **kwargs)\n",
      "-        for c in self._callbacks('before_run_epoch'):\n",
      "+        for c in self._priority_queue('before_run_epoch'):\n",
      "             c.before_run_epoch(*args, **kwargs)\n",
      " \n",
      "     def run_epoch_context(self, *args, **kwargs) -> None:\n",
      "         super().run_epoch_context(*args, **kwargs)\n",
      "-        for c in self._callbacks('run_epoch_context'):\n",
      "+        for c in self._priority_queue('run_epoch_context'):\n",
      "             c.run_epoch_context(*args, **kwargs)\n",
      " \n",
      "     def after_run_epoch(self, *args, **kwargs) -> None:\n",
      "         super().after_run_epoch(*args, **kwargs)\n",
      "-        for c in self._callbacks('after_run_epoch'):\n",
      "+        for c in self._priority_queue('after_run_epoch'):\n",
      "             c.after_run_epoch(*args, **kwargs)\n",
      " \n",
      "     def before_run(self, *args, **kwargs) -> None:\n",
      "         super().before_run(*args, **kwargs)\n",
      "-        for c in self._callbacks('before_run'):\n",
      "+        for c in self._priority_queue('before_run'):\n",
      "             c.before_run(*args, **kwargs)\n",
      " \n",
      "     def after_run(self, *args, **kwargs) -> None:\n",
      "         super().after_run(*args, **kwargs)\n",
      "-        for c in self._callbacks('after_run'):\n",
      "+        for c in self._priority_queue('after_run'):\n",
      "             c.after_run(*args, **kwargs)\n",
      " \n",
      "     def state_dict(self, *args, **kwargs) -> dict[str, Any]:\n",
      "         state_dict = super().state_dict(*args, **kwargs)\n",
      "-        state_dict['callbacks'] = [c.state_dict(*args, **kwargs) for c in self]\n",
      "+        state_dict['callbacks'] = [\n",
      "+            c.state_dict(*args, **kwargs) for c in self._priority_queue.queue\n",
      "+        ]\n",
      "         return state_dict\n",
      " \n",
      "     def load_state_dict(\n",
      "@@ -144,5 +114,9 @@ class ComposedCallback(BaseCallback, UserList[BaseCallback]):\n",
      "         **kwargs,\n",
      "     ) -> None:\n",
      "         super().load_state_dict(state_dict, *args, **kwargs)\n",
      "-        for c, s in zip(self, state_dict['callbacks'], strict=True):\n",
      "+        for c, s in zip(\n",
      "+            self._priority_queue.queue,\n",
      "+            state_dict['callbacks'],\n",
      "+            strict=True,\n",
      "+        ):\n",
      "             c.load_state_dict(s, *args, **kwargs)\n",
      "diff --git a/todd/utils/misc.py b/todd/utils/misc.py\n",
      "index ef7475d..1e85994 100644\n",
      "--- a/todd/utils/misc.py\n",
      "+++ b/todd/utils/misc.py\n",
      "@@ -2,12 +2,14 @@ __all__ = [\n",
      "     'get_timestamp',\n",
      "     'Status',\n",
      "     'subprocess_run',\n",
      "+    'PriorityQueue',\n",
      " ]\n",
      " \n",
      "+from collections import UserList\n",
      " import enum\n",
      " import subprocess  # nosec B404\n",
      " from datetime import datetime\n",
      "-from typing import Generic, Mapping, TypeVar\n",
      "+from typing import Generic, Iterable, Mapping, TypeVar\n",
      " \n",
      " T = TypeVar('T', bound=enum.Enum)\n",
      " \n",
      "@@ -44,3 +46,37 @@ def subprocess_run(args: str) -> str:\n",
      "         shell=True,  # nosec B602\n",
      "         text=True,\n",
      "     ).stdout\n",
      "+\n",
      "+\n",
      "+KT = TypeVar('KT')\n",
      "+VT = TypeVar('VT')\n",
      "+\n",
      "+\n",
      "+class PriorityQueue(UserList[tuple[Mapping[KT, int], VT]]):\n",
      "+\n",
      "+    def __init__(\n",
      "+        self,\n",
      "+        priorities: Iterable[Mapping[KT, int]],\n",
      "+        queue: Iterable[VT],\n",
      "+    ) -> None:\n",
      "+        priorities = list(priorities)\n",
      "+        queue = list(queue)\n",
      "+        super().__init__(zip(priorities, queue))\n",
      "+\n",
      "+    @property\n",
      "+    def priorities(self) -> list[Mapping[KT, int]]:\n",
      "+        return [p for p, _ in self]\n",
      "+\n",
      "+    @property\n",
      "+    def queue(self) -> list[VT]:\n",
      "+        return [q for _, q in self]\n",
      "+\n",
      "+    def __call__(self, key: KT) -> list[VT]:\n",
      "+        if len(self) == 0:\n",
      "+            return []\n",
      "+        priorities = [p.get(key, 0) for p in self.priorities]\n",
      "+        priority_index = [(p, i) for i, p in enumerate(priorities)]\n",
      "+        priority_index = sorted(priority_index)\n",
      "+        _, indices = zip(*priority_index)\n",
      "+        queue = [self.queue[i] for i in indices]\n",
      "+        return queue\n",
      "diff --git a/todd/utils/torch.py b/todd/utils/torch.py\n",
      "index 356f68b..42eda22 100644\n",
      "--- a/todd/utils/torch.py\n",
      "+++ b/todd/utils/torch.py\n",
      "@@ -5,12 +5,15 @@ __all__ = [\n",
      "     'all_gather',\n",
      "     'all_gather_',\n",
      "     'Shape',\n",
      "+    'ModuleList',\n",
      "+    'ModuleDict',\n",
      " ]\n",
      " \n",
      " import functools\n",
      " import itertools\n",
      " import operator\n",
      " import os\n",
      "+from typing import Any\n",
      " \n",
      " import torch\n",
      " import torch.distributed as dist\n",
      "@@ -127,3 +130,15 @@ class Shape:\n",
      "                 module.stride,\n",
      "             ),\n",
      "         )\n",
      "+\n",
      "+\n",
      "+class ModuleList(nn.ModuleList):\n",
      "+\n",
      "+    def forward(self, *args, **kwargs) -> list:\n",
      "+        return [m(*args, **kwargs) for m in self]\n",
      "+\n",
      "+\n",
      "+class ModuleDict(nn.ModuleDict):\n",
      "+\n",
      "+    def forward(self, *args, **kwargs) -> dict[str, Any]:\n",
      "+        return {k: m(*args, **kwargs) for k, m in self.items()}\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"git_callback\",\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type=\"RunnerDataset\", n=20)),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"GitCallback\", diff='HEAD -- \":(exclude)*.ipynb\"'),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/git_callback/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:30:01,373 88602:140704380690048][base.py:55 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:01,394 88602:140704380690048][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([10,  6]), 'y': tensor([20, 12])} weight=0.000 loss=16.000\n",
      "[2024-02-08 02:30:01,396 88602:140704380690048][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.040 loss=10.780\n",
      "[2024-02-08 02:30:01,398 88602:140704380690048][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.067 loss=9.663\n",
      "[2024-02-08 02:30:01,403 88602:140704380690048][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.092 loss=12.399\n",
      "[2024-02-08 02:30:01,406 88602:140704380690048][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.125 loss=4.688\n",
      "[2024-02-08 02:30:01,411 88602:140704380690048][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([10,  6]), 'y': tensor([20, 12])} weight=0.137 loss=14.900\n",
      "[2024-02-08 02:30:01,413 88602:140704380690048][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.177 loss=10.024\n",
      "[2024-02-08 02:30:01,415 88602:140704380690048][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.205 loss=8.975\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"optimize_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:30:01,431 88602:140704380690048][base.py:55 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:01,438 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.000 loss=11.000 lr=['1.667e-03']\n",
      "[2024-02-08 02:30:01,446 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.009 loss=6.968 lr=['2.333e-03']\n",
      "[2024-02-08 02:30:01,448 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.017 loss=13.879 lr=['3.000e-03']\n",
      "[2024-02-08 02:30:01,450 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.038 loss=4.904 lr=['3.667e-03']\n",
      "[2024-02-08 02:30:01,453 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.047 loss=17.573 lr=['4.333e-03']\n",
      "[2024-02-08 02:30:01,458 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.086 loss=10.524 lr=['5.000e-03']\n",
      "[2024-02-08 02:30:01,460 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.114 loss=6.601 lr=['5.000e-03']\n",
      "[2024-02-08 02:30:01,462 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.131 loss=13.080 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=5),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:30:01,493 88602:140704380690048][base.py:55 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:01,494 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "[2024-02-08 02:30:01,497 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/10] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.000 loss=7.000 lr=['1.667e-03']\n",
      "[2024-02-08 02:30:01,500 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/10] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.006 loss=2.991 lr=['1.667e-03']\n",
      "[2024-02-08 02:30:01,502 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [2/5]\n",
      "[2024-02-08 02:30:01,507 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/10] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.008 loss=4.979 lr=['2.778e-03']\n",
      "[2024-02-08 02:30:01,511 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/10] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.015 loss=4.962 lr=['2.778e-03']\n",
      "[2024-02-08 02:30:01,512 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [3/5]\n",
      "[2024-02-08 02:30:01,516 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/10] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.022 loss=3.956 lr=['3.889e-03']\n",
      "[2024-02-08 02:30:01,520 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/10] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.030 loss=5.910 lr=['3.889e-03']\n",
      "[2024-02-08 02:30:01,522 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [4/5]\n",
      "[2024-02-08 02:30:01,525 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/10] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.042 loss=5.875 lr=['5.000e-03']\n",
      "[2024-02-08 02:30:01,528 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/10] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.057 loss=3.887 lr=['5.000e-03']\n",
      "[2024-02-08 02:30:01,529 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [5/5]\n",
      "[2024-02-08 02:30:01,532 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [9/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.067 loss=2.900 lr=['5.000e-03']\n",
      "[2024-02-08 02:30:01,534 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [10/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.074 loss=6.740 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=4),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=3),\n",
    "            by_epoch=True,\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=5,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-08 02:30:01,569 88602:140704380690048][lr.py:93 todd.IterBasedTrainer.lr_scale_callback _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-02-08 02:30:01,570 88602:140704380690048][base.py:55 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:01,574 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([8, 6]), 'y': tensor([16, 12])} weight=0.000 loss=14.000\n",
      "[2024-02-08 02:30:01,576 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.070 loss=12.545\n",
      "[2024-02-08 02:30:01,578 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([1, 5]), 'y': tensor([ 2, 10])} weight=0.135 loss=5.595\n",
      "[2024-02-08 02:30:01,580 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([2, 7]), 'y': tensor([ 4, 14])} weight=0.165 loss=8.257\n",
      "[2024-02-08 02:30:01,584 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.210 loss=11.635\n",
      "[2024-02-08 02:30:01,586 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([8, 6]), 'y': tensor([16, 12])} weight=0.275 loss=12.075\n",
      "[2024-02-08 02:30:01,589 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.345 loss=10.757\n",
      "[2024-02-08 02:30:01,591 88602:140704380690048][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([1, 5]), 'y': tensor([ 2, 10])} weight=0.410 loss=4.770\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_scale_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScaleCallback\",\n",
    "            lr_scaler=dict(base_batch_size=1),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:30:01,617 88602:140704380690048][base.py:55 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:01,623 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([3, 6]), 'y': tensor([ 6, 12])} weight=0.000 loss=9.000\n",
      "[2024-02-08 02:30:01,625 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_1\n",
      "[2024-02-08 02:30:01,633 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.022 loss=9.888\n",
      "[2024-02-08 02:30:01,636 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-02-08 02:30:01,640 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.047 loss=10.739\n",
      "[2024-02-08 02:30:01,641 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_3\n",
      "[2024-02-08 02:30:01,646 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.075 loss=12.512\n",
      "[2024-02-08 02:30:01,647 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-02-08 02:30:01,652 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.108 loss=11.355\n",
      "[2024-02-08 02:30:01,653 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-02-08 02:30:01,658 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([3, 6]), 'y': tensor([ 6, 12])} weight=0.138 loss=8.381\n",
      "[2024-02-08 02:30:01,659 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-08 02:30:01,663 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.160 loss=9.200\n",
      "[2024-02-08 02:30:01,665 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-02-08 02:30:01,669 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.185 loss=9.983\n",
      "[2024-02-08 02:30:01,671 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_5\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_7\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_8\u001b[0m\n",
      "\n",
      "12 directories, 40 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-08 02:30:02,164 88602:140704380690048][checkpoint.py:54 todd.IterBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-02-08 02:30:02,169 88602:140704380690048][base.py:82 todd.IterBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-08 02:30:02,170 88602:140704380690048][base.py:55 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:02,174 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.138 loss=16.763\n",
      "[2024-02-08 02:30:02,176 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-08 02:30:02,181 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([2, 5]), 'y': tensor([ 4, 10])} weight=0.183 loss=6.361\n",
      "[2024-02-08 02:30:02,182 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-02-08 02:30:02,187 88602:140704380690048][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.200 loss=11.700\n",
      "[2024-02-08 02:30:02,189 88602:140704380690048][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpubd4uemq/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {0: {'momentum_buffer': None}}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 5}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.1375))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_5 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_5'\n",
    "    for f in iter_5.glob('*.pth'):\n",
    "        print(f'{f.name}:')\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()\n",
    "\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_5),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:30:02,241 88602:140704380690048][base.py:55 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:02,242 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-08 02:30:02,245 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.000 loss=17.000\n",
      "[2024-02-08 02:30:02,248 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([1, 5]), 'y': tensor([ 2, 10])} weight=0.043 loss=5.873\n",
      "[2024-02-08 02:30:02,249 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-02-08 02:30:02,254 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([6, 4]), 'y': tensor([12,  8])} weight=0.058 loss=9.712\n",
      "[2024-02-08 02:30:02,256 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([2, 7]), 'y': tensor([ 4, 14])} weight=0.082 loss=8.629\n",
      "[2024-02-08 02:30:02,258 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-02-08 02:30:02,262 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([10,  3]), 'y': tensor([20,  6])} weight=0.105 loss=12.318\n",
      "[2024-02-08 02:30:02,263 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-08 02:30:02,265 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.137 loss=10.244\n",
      "[2024-02-08 02:30:02,267 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-08 02:30:02,272 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.165 loss=12.845\n",
      "[2024-02-08 02:30:02,275 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.200 loss=15.300\n",
      "[2024-02-08 02:30:02,277 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-02-08 02:30:02,281 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.242 loss=7.030\n",
      "[2024-02-08 02:30:02,284 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.262 loss=4.344\n",
      "[2024-02-08 02:30:02,285 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-08 02:30:02,289 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-08 02:30:02,292 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.275 loss=14.663\n",
      "[2024-02-08 02:30:02,295 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.317 loss=6.730\n",
      "[2024-02-08 02:30:02,296 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-08 02:30:02,301 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.337 loss=9.144\n",
      "[2024-02-08 02:30:02,304 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.365 loss=5.722\n",
      "[2024-02-08 02:30:02,305 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-08 02:30:02,312 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.382 loss=9.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_12\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_14\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_14\u001b[0m\n",
      "\n",
      "11 directories, 35 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-08 02:30:02,794 88602:140704380690048][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-02-08 02:30:02,798 88602:140704380690048][base.py:82 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-08 02:30:02,799 88602:140704380690048][base.py:55 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:02,801 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-08 02:30:02,806 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.242 loss=3.515\n",
      "[2024-02-08 02:30:02,809 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.252 loss=7.864\n",
      "[2024-02-08 02:30:02,810 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-08 02:30:02,815 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-08 02:30:02,818 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.275 loss=16.388\n",
      "[2024-02-08 02:30:02,821 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.322 loss=6.710\n",
      "[2024-02-08 02:30:02,822 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-08 02:30:02,827 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.343 loss=3.315\n",
      "[2024-02-08 02:30:02,829 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.352 loss=9.885\n",
      "[2024-02-08 02:30:02,830 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-08 02:30:02,836 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.382 loss=9.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-08 02:30:03,274 88602:140704380690048][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-08 02:30:03,278 88602:140704380690048][base.py:82 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-08 02:30:03,279 88602:140704380690048][base.py:55 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:03,280 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-08 02:30:03,284 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.275 loss=10.350\n",
      "[2024-02-08 02:30:03,287 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([7, 3]), 'y': tensor([14,  6])} weight=0.305 loss=8.475\n",
      "[2024-02-08 02:30:03,288 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-08 02:30:03,295 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.330 loss=12.525\n",
      "[2024-02-08 02:30:03,297 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.368 loss=2.449\n",
      "[2024-02-08 02:30:03,298 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp_wqnky9z/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-08 02:30:03,303 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.375 loss=12.188\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=2),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_8 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_8'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_8),\n",
    "        )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    iter_10 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_10'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_10),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:30:03,338 88602:140704380690048][base.py:55 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:03,340 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-08 02:30:03,343 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([1, 5]), 'y': tensor([ 2, 10])} weight=0.000 loss=6.000\n",
      "[2024-02-08 02:30:03,346 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.015 loss=9.925\n",
      "[2024-02-08 02:30:03,349 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.040 loss=16.660\n",
      "[2024-02-08 02:30:03,352 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.082 loss=9.587\n",
      "[2024-02-08 02:30:03,355 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.107 loss=11.355\n",
      "[2024-02-08 02:30:03,356 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpb9u7duhc/checkpoint_callback/checkpoints/epoch_1\n",
      "[2024-02-08 02:30:03,360 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-08 02:30:03,363 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.137 loss=7.450\n",
      "[2024-02-08 02:30:03,365 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([10,  4]), 'y': tensor([20,  8])} weight=0.157 loss=12.897\n",
      "[2024-02-08 02:30:03,369 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.192 loss=9.941\n",
      "[2024-02-08 02:30:03,372 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.220 loss=12.460\n",
      "[2024-02-08 02:30:03,375 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.255 loss=6.980\n",
      "[2024-02-08 02:30:03,377 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpb9u7duhc/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-02-08 02:30:03,381 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-08 02:30:03,383 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.275 loss=6.038\n",
      "[2024-02-08 02:30:03,387 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.293 loss=12.806\n",
      "[2024-02-08 02:30:03,390 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.330 loss=5.845\n",
      "[2024-02-08 02:30:03,393 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([7, 8]), 'y': tensor([14, 16])} weight=0.348 loss=12.394\n",
      "[2024-02-08 02:30:03,395 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.385 loss=8.882\n",
      "[2024-02-08 02:30:03,396 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpb9u7duhc/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpb9u7duhc\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36mepoch_3\u001b[0m\n",
      "\n",
      "7 directories, 15 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-08 02:30:03,851 88602:140704380690048][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpb9u7duhc/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-02-08 02:30:03,857 88602:140704380690048][base.py:82 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-08 02:30:03,859 88602:140704380690048][base.py:55 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:03,865 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-08 02:30:03,872 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.275 loss=9.488\n",
      "[2024-02-08 02:30:03,875 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.303 loss=14.429\n",
      "[2024-02-08 02:30:03,887 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.345 loss=10.757\n",
      "[2024-02-08 02:30:03,912 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.377 loss=5.679\n",
      "[2024-02-08 02:30:03,916 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.395 loss=5.617\n",
      "[2024-02-08 02:30:03,920 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpb9u7duhc/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'epoch_2'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(epoch_2),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register_()\n",
    "class FaultyValidator(todd.runners.Validator):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError(\"faulty runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:30:04,052 88602:140704380690048][base.py:55 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-02-08 02:30:04,055 88602:140704380690048][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x14e916c50>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 223, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_88602/1715875531.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2024-02-08 02:30:04,052 88602:140704380690048][base.py:55 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2024-02-08 02:30:04,055 88602:140704380690048][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x14e916c50>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 223, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_88602/1715875531.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='FaultyValidator',\n",
    "    name='monitor_callback',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=[\n",
    "        dict(type='MonitorCallback'),\n",
    "        dict(type='LogCallback', interval=5, with_file_handler=True),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/monitor_callback/*.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:30:04,397 88602:140704380690048][base.py:55 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:04,398 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-08 02:30:04,405 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([7, 8]), 'y': tensor([14, 16])} weight=0.000 loss=15.000\n",
      "[2024-02-08 02:30:04,409 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.037 loss=14.719\n",
      "[2024-02-08 02:30:04,412 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.075 loss=4.812\n",
      "[2024-02-08 02:30:04,416 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.087 loss=4.781\n",
      "[2024-02-08 02:30:04,419 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([10,  5]), 'y': tensor([20, 10])} weight=0.100 loss=14.250\n",
      "[2024-02-08 02:30:04,420 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpk2owwwsa/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-02-08 02:30:04,424 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-08 02:30:04,427 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.137 loss=10.244\n",
      "[2024-02-08 02:30:04,429 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.165 loss=15.597\n",
      "[2024-02-08 02:30:04,434 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.207 loss=8.962\n",
      "[2024-02-08 02:30:04,437 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([8, 3]), 'y': tensor([16,  6])} weight=0.232 loss=9.721\n",
      "[2024-02-08 02:30:04,440 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.260 loss=5.220\n",
      "[2024-02-08 02:30:04,441 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpk2owwwsa/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-02-08 02:30:04,446 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-08 02:30:04,448 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.275 loss=6.900\n",
      "[2024-02-08 02:30:04,451 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.295 loss=5.967\n",
      "[2024-02-08 02:30:04,454 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.312 loss=14.344\n",
      "[2024-02-08 02:30:04,457 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.355 loss=9.048\n",
      "[2024-02-08 02:30:04,462 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.382 loss=9.705\n",
      "[2024-02-08 02:30:04,463 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpk2owwwsa/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-08 02:30:04,958 88602:140704380690048][base.py:55 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-08 02:30:04,959 88602:140704380690048][base.py:97 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpk2owwwsa/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-02-08 02:30:04,963 88602:140704380690048][base.py:82 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "[2024-02-08 02:30:04,965 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-08 02:30:04,969 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.275 loss=9.488\n",
      "[2024-02-08 02:30:04,972 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.302 loss=7.639\n",
      "[2024-02-08 02:30:04,975 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.325 loss=6.700\n",
      "[2024-02-08 02:30:04,979 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.345 loss=13.240\n",
      "[2024-02-08 02:30:04,982 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([8, 3]), 'y': tensor([16,  6])} weight=0.385 loss=8.882\n",
      "[2024-02-08 02:30:04,985 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpk2owwwsa/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-02-08 02:30:04,992 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-08 02:30:04,994 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.412 loss=7.144\n",
      "[2024-02-08 02:30:04,996 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.435 loss=10.955\n",
      "[2024-02-08 02:30:04,999 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([9, 7]), 'y': tensor([18, 14])} weight=0.470 loss=12.240\n",
      "[2024-02-08 02:30:05,002 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.510 loss=9.685\n",
      "[2024-02-08 02:30:05,004 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.543 loss=2.186\n",
      "[2024-02-08 02:30:05,006 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpk2owwwsa/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-02-08 02:30:05,010 88602:140704380690048][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-08 02:30:05,015 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.550 loss=5.075\n",
      "[2024-02-08 02:30:05,019 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.567 loss=7.879\n",
      "[2024-02-08 02:30:05,023 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.595 loss=5.620\n",
      "[2024-02-08 02:30:05,026 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.615 loss=11.773\n",
      "[2024-02-08 02:30:05,030 88602:140704380690048][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.657 loss=8.055\n",
      "[2024-02-08 02:30:05,032 88602:140704380690048][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpk2owwwsa/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"strategy_load_model_from\",\n",
    "    dataloader=dict(\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        dataset=dict(type=\"RunnerDataset\", n=10),\n",
    "    ),\n",
    "    strategy=dict(type=\"BaseStrategy\", model=dict(type=\"RunnerModel\")),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = (pathlib.Path(work_dirs) / 'strategy_load_model_from' / 'checkpoints' / 'epoch_2' / 'model.pth')\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.strategy.load_model_from(epoch_2)\n",
    "    runner.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
