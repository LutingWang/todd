{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.3.0\n",
      "Uninstalling todd-ai-0.3.0:\n",
      "  Successfully uninstalled todd-ai-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing /Users/bytedance/Developer/todd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (0.6.1)\n",
      "Requirement already satisfied: lmdb in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (1.4.1)\n",
      "Requirement already satisfied: opencv-python in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (4.7.0.72)\n",
      "Requirement already satisfied: pandas in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (2.0.1)\n",
      "Requirement already satisfied: python-pptx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (0.6.21)\n",
      "Requirement already satisfied: timm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (0.9.2)\n",
      "Requirement already satisfied: toml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (4.5.0)\n",
      "Requirement already satisfied: yapf in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.3.0) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from opencv-python->todd-ai==0.3.0) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.3.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.3.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.3.0) (2023.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.3.0) (4.9.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.3.0) (9.5.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.3.0) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.3.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.3.0) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.3.0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.3.0) (0.14.1)\n",
      "Requirement already satisfied: safetensors in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.3.0) (0.3.1)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from yapf->todd-ai==0.3.0) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->todd-ai==0.3.0) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.3.0) (3.12.0)\n",
      "Requirement already satisfied: sympy in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.3.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.3.0) (2023.5.0)\n",
      "Requirement already satisfied: requests in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.3.0) (2.30.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.3.0) (23.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from jinja2->torch>=1.7->timm->todd-ai==0.3.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.3.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.3.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.3.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.3.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from sympy->torch>=1.7->timm->todd-ai==0.3.0) (1.3.0)\n",
      "Building wheels for collected packages: todd-ai\n",
      "  Building wheel for todd-ai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for todd-ai: filename=todd_ai-0.3.0-py3-none-any.whl size=94840 sha256=2f00fdee81befbef715c24c668fa2c52ef5e61b4452e0df40ea980620c5dc838\n",
      "  Stored in directory: /private/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/pip-ephem-wheel-cache-5etlvmme/wheels/15/ef/5a/9fc12e257ce5cef16b333a2ed6c992ff9cbcc9167f7199e6ac\n",
      "Successfully built todd-ai\n",
      "Installing collected packages: todd-ai\n",
      "Successfully installed todd-ai-0.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "[2023-07-13 13:45:00,583 45313:140704293135936][patches.py:72 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import Any, TypedDict, cast\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "import todd\n",
    "\n",
    "Memo = dict[str, Any]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register()\n",
    "class RunnerModel(todd.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> None:\n",
    "        y: torch.Tensor = self._strategy.model(batch['x'])\n",
    "        loss = F.l1_loss(y, batch['y'])\n",
    "        memo['loss'] = loss\n",
    "        if 'log' in memo:\n",
    "            memo['log']['loss'] = f'{loss.item():.3f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerMixin(RunnerMixin):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> None:\n",
    "        super()._run_iter(batch, memo)\n",
    "        if 'log' in memo:\n",
    "            model = cast(RunnerModel, self._strategy.model)\n",
    "            memo['log']['weight'] = f'{model.weight.item():.3f}'\n",
    "            memo['log']['batch'] = str(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomValidator(RunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomIterBasedTrainer(TrainerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomEpochBasedTrainer(TrainerMixin, todd.runners.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_demo = todd.Config(\n",
    "    type='CustomValidator',\n",
    "    name='custom_validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='VanillaStrategy', model=dict(type='RunnerModel')),\n",
    "    callbacks=dict(type='LogCallback', interval=5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-13 13:45:01,166 45313:140704293135936][log.py:37 todd.CustomValidator.custom_validator connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-13 13:45:01,169 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] loss=10.000\n",
      "[2023-07-13 13:45:01,171 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] loss=20.000\n",
      "[2023-07-13 13:45:01,172 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] loss=30.000\n",
      "[2023-07-13 13:45:01,174 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpt_pf_hxe\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_demo, \n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_demo = validator_demo.copy()\n",
    "trainer_demo.pop('type')\n",
    "trainer_demo.dataloader = todd.Config(\n",
    "    batch_size=2, shuffle=True, dataset=dict(type='RunnerDataset', n=67),\n",
    ")\n",
    "trainer_demo.optimizer = todd.Config(type='SGD', lr=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_based_trainer_demo = trainer_demo.copy()\n",
    "iter_based_trainer_demo.type = 'CustomIterBasedTrainer'\n",
    "iter_based_trainer_demo.name = 'custom_iter_based_trainer'\n",
    "iter_based_trainer_demo.iters = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-13 13:45:01,478 45313:140704293135936][log.py:37 todd.CustomIterBasedTrainer.custom_iter_based_trainer connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-13 13:45:01,482 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=91.000 weight=0.000 batch={'x': tensor([31, 60]), 'y': tensor([ 62, 120])}\n",
      "[2023-07-13 13:45:01,484 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=86.000 weight=0.000 batch={'x': tensor([65, 21]), 'y': tensor([130,  42])}\n",
      "[2023-07-13 13:45:01,486 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=17.000 weight=0.000 batch={'x': tensor([ 2, 15]), 'y': tensor([ 4, 30])}\n",
      "[2023-07-13 13:45:01,488 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=22.000 weight=0.000 batch={'x': tensor([13,  9]), 'y': tensor([26, 18])}\n",
      "[2023-07-13 13:45:01,490 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=51.000 weight=0.000 batch={'x': tensor([32, 19]), 'y': tensor([64, 38])}\n",
      "[2023-07-13 13:45:01,493 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=95.000 weight=0.000 batch={'x': tensor([29, 66]), 'y': tensor([ 58, 132])}\n",
      "[2023-07-13 13:45:01,495 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=36.000 weight=0.000 batch={'x': tensor([ 3, 33]), 'y': tensor([ 6, 66])}\n",
      "[2023-07-13 13:45:01,496 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=85.000 weight=0.000 batch={'x': tensor([57, 28]), 'y': tensor([114,  56])}\n",
      "[2023-07-13 13:45:01,498 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=45.000 weight=0.000 batch={'x': tensor([38,  7]), 'y': tensor([76, 14])}\n",
      "[2023-07-13 13:45:01,499 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=90.000 weight=0.000 batch={'x': tensor([27, 63]), 'y': tensor([ 54, 126])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        iter_based_trainer_demo,\n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_based_trainer_demo = trainer_demo.copy()\n",
    "epoch_based_trainer_demo.type = 'CustomEpochBasedTrainer'\n",
    "epoch_based_trainer_demo.name = 'custom_epoch_based_trainer'\n",
    "epoch_based_trainer_demo.epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-13 13:45:01,514 45313:140704293135936][log.py:37 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-13 13:45:01,515 45313:140704293135936][log.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-13 13:45:01,518 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=125.000 weight=0.000 batch={'x': tensor([61, 64]), 'y': tensor([122, 128])}\n",
      "[2023-07-13 13:45:01,519 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=80.000 weight=0.000 batch={'x': tensor([50, 30]), 'y': tensor([100,  60])}\n",
      "[2023-07-13 13:45:01,521 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=53.000 weight=0.000 batch={'x': tensor([49,  4]), 'y': tensor([98,  8])}\n",
      "[2023-07-13 13:45:01,523 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=79.000 weight=0.000 batch={'x': tensor([51, 28]), 'y': tensor([102,  56])}\n",
      "[2023-07-13 13:45:01,525 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=52.000 weight=0.000 batch={'x': tensor([35, 17]), 'y': tensor([70, 34])}\n",
      "[2023-07-13 13:45:01,527 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=28.000 weight=0.000 batch={'x': tensor([25,  3]), 'y': tensor([50,  6])}\n",
      "[2023-07-13 13:45:01,529 45313:140704293135936][log.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-13 13:45:01,531 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=57.000 weight=0.000 batch={'x': tensor([47, 10]), 'y': tensor([94, 20])}\n",
      "[2023-07-13 13:45:01,533 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=33.000 weight=0.000 batch={'x': tensor([ 3, 30]), 'y': tensor([ 6, 60])}\n",
      "[2023-07-13 13:45:01,535 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=68.000 weight=0.000 batch={'x': tensor([67,  1]), 'y': tensor([134,   2])}\n",
      "[2023-07-13 13:45:01,537 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=68.000 weight=0.000 batch={'x': tensor([49, 19]), 'y': tensor([98, 38])}\n",
      "[2023-07-13 13:45:01,538 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=52.000 weight=0.000 batch={'x': tensor([29, 23]), 'y': tensor([58, 46])}\n",
      "[2023-07-13 13:45:01,541 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=96.000 weight=0.000 batch={'x': tensor([43, 53]), 'y': tensor([ 86, 106])}\n",
      "[2023-07-13 13:45:01,543 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=62.000 weight=0.000 batch={'x': tensor([40, 22]), 'y': tensor([80, 44])}\n",
      "[2023-07-13 13:45:01,544 45313:140704293135936][log.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-13 13:45:01,546 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=61.000 weight=0.000 batch={'x': tensor([24, 37]), 'y': tensor([48, 74])}\n",
      "[2023-07-13 13:45:01,548 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=106.000 weight=0.000 batch={'x': tensor([48, 58]), 'y': tensor([ 96, 116])}\n",
      "[2023-07-13 13:45:01,549 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=62.000 weight=0.000 batch={'x': tensor([21, 41]), 'y': tensor([42, 82])}\n",
      "[2023-07-13 13:45:01,551 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=82.000 weight=0.000 batch={'x': tensor([18, 64]), 'y': tensor([ 36, 128])}\n",
      "[2023-07-13 13:45:01,553 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=93.000 weight=0.000 batch={'x': tensor([57, 36]), 'y': tensor([114,  72])}\n",
      "[2023-07-13 13:45:01,554 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=76.000 weight=0.000 batch={'x': tensor([ 9, 67]), 'y': tensor([ 18, 134])}\n",
      "[2023-07-13 13:45:01,556 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=19.000 weight=0.000 batch={'x': tensor([ 6, 13]), 'y': tensor([12, 26])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        epoch_based_trainer_demo,\n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_callback_demo = validator_demo.copy()\n",
    "log_callback = log_callback_demo.callbacks\n",
    "log_callback.with_file_handler = True\n",
    "log_callback_demo.callbacks = [log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-13 13:45:01,577 45313:140704293135936][log.py:37 todd.CustomValidator.custom_validator connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-13 13:45:01,580 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] loss=10.000\n",
      "[2023-07-13 13:45:01,582 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] loss=20.000\n",
      "[2023-07-13 13:45:01,584 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] loss=30.000\n",
      "[2023-07-13 13:45:01,587 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpp6ovipjj\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "    └── 2023-07-13T13-45-01_576398-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2023-07-13 13:45:01,577 45313:140704293135936][log.py:37 todd.CustomValidator.custom_validator connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-07-13 13:45:01,580 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] loss=10.000\n",
      "[2023-07-13 13:45:01,582 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] loss=20.000\n",
      "[2023-07-13 13:45:01,584 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] loss=30.000\n",
      "[2023-07-13 13:45:01,587 45313:140704293135936][log.py:52 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] loss=40.000\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        log_callback_demo, \n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_callback_demo = iter_based_trainer_demo.copy()\n",
    "optimize_callback = todd.Config(type='OptimizeCallback')\n",
    "optimize_callback_demo.callbacks = [optimize_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-13 13:45:02,195 45313:140704293135936][log.py:37 todd.CustomIterBasedTrainer.custom_iter_based_trainer connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-13 13:45:02,199 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=33.994 weight=0.612 batch={'x': tensor([37, 12]), 'y': tensor([74, 24])}\n",
      "[2023-07-13 13:45:02,205 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=26.705 weight=1.510 batch={'x': tensor([56, 53]), 'y': tensor([112, 106])}\n",
      "[2023-07-13 13:45:02,208 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=2.827 weight=1.805 batch={'x': tensor([ 8, 21]), 'y': tensor([16, 42])}\n",
      "[2023-07-13 13:45:02,212 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=2.805 weight=2.110 batch={'x': tensor([10, 41]), 'y': tensor([20, 82])}\n",
      "[2023-07-13 13:45:02,215 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=0.106 weight=2.013 batch={'x': tensor([ 6, 11]), 'y': tensor([12, 22])}\n",
      "[2023-07-13 13:45:02,218 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=0.750 weight=1.980 batch={'x': tensor([49, 26]), 'y': tensor([98, 52])}\n",
      "[2023-07-13 13:45:02,221 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=7.080 weight=1.760 batch={'x': tensor([57,  2]), 'y': tensor([114,   4])}\n",
      "[2023-07-13 13:45:02,224 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=0.698 weight=2.045 batch={'x': tensor([ 4, 27]), 'y': tensor([ 8, 54])}\n",
      "[2023-07-13 13:45:02,227 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=1.856 weight=2.038 batch={'x': tensor([47, 52]), 'y': tensor([ 94, 104])}\n",
      "[2023-07-13 13:45:02,230 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=0.719 weight=1.943 batch={'x': tensor([18,  7]), 'y': tensor([36, 14])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        optimize_callback_demo,\n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_schedule_callback = todd.Config(\n",
    "    type='LrScheduleCallback',\n",
    "    lr_scheduler=dict(type='LinearLR', total_iters=10),\n",
    ")\n",
    "lr_schedule_callback_demo.callbacks = [optimize_callback, lr_schedule_callback, log_callback,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-13 13:45:02,266 45313:140704293135936][log.py:37 todd.CustomIterBasedTrainer.custom_iter_based_trainer connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-13 13:45:02,270 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=108.950 weight=0.184 batch={'x': tensor([53, 67]), 'y': tensor([106, 134])} lr=['3.333e-03']\n",
      "[2023-07-13 13:45:02,273 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=43.540 weight=0.951 batch={'x': tensor([46, 37]), 'y': tensor([92, 74])} lr=['5.000e-03']\n",
      "[2023-07-13 13:45:02,277 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=0.226 weight=1.850 batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} lr=['5.000e-03']\n",
      "[2023-07-13 13:45:02,280 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=5.037 weight=2.109 batch={'x': tensor([52, 40]), 'y': tensor([104,  80])} lr=['5.000e-03']\n",
      "[2023-07-13 13:45:02,283 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=3.910 weight=1.932 batch={'x': tensor([60, 55]), 'y': tensor([120, 110])} lr=['5.000e-03']\n",
      "[2023-07-13 13:45:02,286 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=4.696 weight=2.144 batch={'x': tensor([11, 54]), 'y': tensor([ 22, 108])} lr=['5.000e-03']\n",
      "[2023-07-13 13:45:02,289 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=2.461 weight=2.089 batch={'x': tensor([29, 26]), 'y': tensor([58, 52])} lr=['5.000e-03']\n",
      "[2023-07-13 13:45:02,292 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=6.993 weight=2.197 batch={'x': tensor([32, 39]), 'y': tensor([64, 78])} lr=['5.000e-03']\n",
      "[2023-07-13 13:45:02,294 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=5.488 weight=1.865 batch={'x': tensor([18, 63]), 'y': tensor([ 36, 126])} lr=['5.000e-03']\n",
      "[2023-07-13 13:45:02,297 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=1.483 weight=1.957 batch={'x': tensor([ 8, 61]), 'y': tensor([ 16, 122])} lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_callback_demo,\n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "lr_schedule_by_epoch_callback = lr_schedule_callback.copy()\n",
    "lr_schedule_by_epoch_callback.by_epoch = True\n",
    "lr_schedule_by_epoch_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_by_epoch_callback, log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-13 13:45:02,315 45313:140704293135936][log.py:37 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-13 13:45:02,316 45313:140704293135936][log.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-13 13:45:02,321 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=42.940 weight=0.211 batch={'x': tensor([38, 10]), 'y': tensor([76, 20])} lr=['1.667e-03']\n",
      "[2023-07-13 13:45:02,324 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=54.719 weight=0.541 batch={'x': tensor([36, 39]), 'y': tensor([72, 78])} lr=['1.667e-03']\n",
      "[2023-07-13 13:45:02,327 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=53.474 weight=0.798 batch={'x': tensor([57, 32]), 'y': tensor([114,  64])} lr=['1.667e-03']\n",
      "[2023-07-13 13:45:02,331 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=53.713 weight=1.041 batch={'x': tensor([59, 53]), 'y': tensor([118, 106])} lr=['1.667e-03']\n",
      "[2023-07-13 13:45:02,334 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=22.619 weight=1.413 batch={'x': tensor([42, 35]), 'y': tensor([84, 70])} lr=['1.667e-03']\n",
      "[2023-07-13 13:45:02,337 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=5.249 weight=1.731 batch={'x': tensor([14, 25]), 'y': tensor([28, 50])} lr=['1.667e-03']\n",
      "[2023-07-13 13:45:02,340 45313:140704293135936][log.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-13 13:45:02,343 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=2.123 weight=1.924 batch={'x': tensor([41, 15]), 'y': tensor([82, 30])} lr=['2.000e-03']\n",
      "[2023-07-13 13:45:02,347 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=1.120 weight=1.976 batch={'x': tensor([27, 67]), 'y': tensor([ 54, 134])} lr=['2.000e-03']\n",
      "[2023-07-13 13:45:02,350 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=0.228 weight=1.986 batch={'x': tensor([ 1, 32]), 'y': tensor([ 2, 64])} lr=['2.000e-03']\n",
      "[2023-07-13 13:45:02,353 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=0.200 weight=1.985 batch={'x': tensor([22,  5]), 'y': tensor([44, 10])} lr=['2.000e-03']\n",
      "[2023-07-13 13:45:02,356 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=2.921 weight=1.942 batch={'x': tensor([57, 44]), 'y': tensor([114,  88])} lr=['2.000e-03']\n",
      "[2023-07-13 13:45:02,361 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=5.462 weight=2.108 batch={'x': tensor([47, 54]), 'y': tensor([ 94, 108])} lr=['2.000e-03']\n",
      "[2023-07-13 13:45:02,366 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=1.569 weight=1.955 batch={'x': tensor([25, 45]), 'y': tensor([50, 90])} lr=['2.000e-03']\n",
      "[2023-07-13 13:45:02,368 45313:140704293135936][log.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-13 13:45:02,371 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=1.764 weight=2.048 batch={'x': tensor([ 9, 65]), 'y': tensor([ 18, 130])} lr=['2.333e-03']\n",
      "[2023-07-13 13:45:02,375 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=0.565 weight=1.982 batch={'x': tensor([23, 41]), 'y': tensor([46, 82])} lr=['2.333e-03']\n",
      "[2023-07-13 13:45:02,378 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=1.740 weight=2.076 batch={'x': tensor([20, 26]), 'y': tensor([40, 52])} lr=['2.333e-03']\n",
      "[2023-07-13 13:45:02,381 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=2.459 weight=2.076 batch={'x': tensor([54, 11]), 'y': tensor([108,  22])} lr=['2.333e-03']\n",
      "[2023-07-13 13:45:02,392 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=0.666 weight=2.019 batch={'x': tensor([33, 39]), 'y': tensor([66, 78])} lr=['2.333e-03']\n",
      "[2023-07-13 13:45:02,402 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=1.463 weight=2.035 batch={'x': tensor([57, 27]), 'y': tensor([114,  54])} lr=['2.333e-03']\n",
      "[2023-07-13 13:45:02,407 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=2.364 weight=2.115 batch={'x': tensor([12, 29]), 'y': tensor([24, 58])} lr=['2.333e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_by_epoch_callback_demo,\n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_with_scaler_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_schedule_with_scaler_callback = lr_schedule_callback.copy()\n",
    "lr_schedule_with_scaler_callback.lr_scaler = dict(base_batch_size=1)\n",
    "lr_schedule_with_scaler_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_with_scaler_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-13 13:45:02,427 45313:140704293135936][lr_schedule.py:54 todd.CustomIterBasedTrainer.custom_iter_based_trainer _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.0\n",
      "\u001b[2m[2023-07-13 13:45:02,428 45313:140704293135936][log.py:37 todd.CustomIterBasedTrainer.custom_iter_based_trainer connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-13 13:45:02,432 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=30.421 weight=0.677 batch={'x': tensor([16, 30]), 'y': tensor([32, 60])} lr=['6.667e-03']\n",
      "[2023-07-13 13:45:02,436 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=3.282 weight=2.068 batch={'x': tensor([49, 48]), 'y': tensor([98, 96])} lr=['1.000e-02']\n",
      "[2023-07-13 13:45:02,439 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=1.320 weight=1.940 batch={'x': tensor([40,  4]), 'y': tensor([80,  8])} lr=['1.000e-02']\n",
      "[2023-07-13 13:45:02,442 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=9.488 weight=1.655 batch={'x': tensor([19, 36]), 'y': tensor([38, 72])} lr=['1.000e-02']\n",
      "[2023-07-13 13:45:02,445 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=1.890 weight=1.955 batch={'x': tensor([21, 63]), 'y': tensor([ 42, 126])} lr=['1.000e-02']\n",
      "[2023-07-13 13:45:02,449 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=3.188 weight=2.075 batch={'x': tensor([57, 28]), 'y': tensor([114,  56])} lr=['1.000e-02']\n",
      "[2023-07-13 13:45:02,451 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=0.135 weight=2.090 batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} lr=['1.000e-02']\n",
      "[2023-07-13 13:45:02,454 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=3.780 weight=2.140 batch={'x': tensor([46,  8]), 'y': tensor([92, 16])} lr=['1.000e-02']\n",
      "[2023-07-13 13:45:02,457 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=1.400 weight=1.930 batch={'x': tensor([11, 29]), 'y': tensor([22, 58])} lr=['1.000e-02']\n",
      "[2023-07-13 13:45:02,460 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=4.837 weight=1.785 batch={'x': tensor([31, 14]), 'y': tensor([62, 28])} lr=['1.000e-02']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_with_scaler_callback_demo,\n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback_demo = iter_based_trainer_demo.copy()\n",
    "checkpoint_callback = todd.Config(type='CheckpointCallback', interval=10)\n",
    "checkpoint_callback_demo.callbacks = [checkpoint_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-13 13:45:02,481 45313:140704293135936][log.py:37 todd.CustomIterBasedTrainer.custom_iter_based_trainer connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-13 13:45:02,484 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] loss=97.000 weight=0.000 batch={'x': tensor([45, 52]), 'y': tensor([ 90, 104])}\n",
      "[2023-07-13 13:45:02,486 45313:140704293135936][checkpoint.py:34 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpew7rag0_/custom_iter_based_trainer/iter_10.pth\n",
      "[2023-07-13 13:45:02,488 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] loss=98.000 weight=0.000 batch={'x': tensor([56, 42]), 'y': tensor([112,  84])}\n",
      "[2023-07-13 13:45:02,491 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] loss=65.000 weight=0.000 batch={'x': tensor([ 1, 64]), 'y': tensor([  2, 128])}\n",
      "[2023-07-13 13:45:02,494 45313:140704293135936][checkpoint.py:34 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpew7rag0_/custom_iter_based_trainer/iter_20.pth\n",
      "[2023-07-13 13:45:02,495 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] loss=28.000 weight=0.000 batch={'x': tensor([23,  5]), 'y': tensor([46, 10])}\n",
      "[2023-07-13 13:45:02,498 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] loss=102.000 weight=0.000 batch={'x': tensor([41, 61]), 'y': tensor([ 82, 122])}\n",
      "[2023-07-13 13:45:02,500 45313:140704293135936][checkpoint.py:34 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpew7rag0_/custom_iter_based_trainer/iter_30.pth\n",
      "[2023-07-13 13:45:02,501 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] loss=57.000 weight=0.000 batch={'x': tensor([14, 43]), 'y': tensor([28, 86])}\n",
      "[2023-07-13 13:45:02,504 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] loss=89.000 weight=0.000 batch={'x': tensor([53, 36]), 'y': tensor([106,  72])}\n",
      "[2023-07-13 13:45:02,506 45313:140704293135936][checkpoint.py:34 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpew7rag0_/custom_iter_based_trainer/iter_40.pth\n",
      "[2023-07-13 13:45:02,507 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] loss=52.000 weight=0.000 batch={'x': tensor([25, 27]), 'y': tensor([50, 54])}\n",
      "[2023-07-13 13:45:02,521 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] loss=68.000 weight=0.000 batch={'x': tensor([65,  3]), 'y': tensor([130,   6])}\n",
      "[2023-07-13 13:45:02,523 45313:140704293135936][checkpoint.py:34 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpew7rag0_/custom_iter_based_trainer/iter_50.pth\n",
      "[2023-07-13 13:45:02,524 45313:140704293135936][log.py:52 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] loss=34.000 weight=0.000 batch={'x': tensor([ 6, 28]), 'y': tensor([12, 56])}\n",
      "[2023-07-13 13:45:02,527 45313:140704293135936][checkpoint.py:34 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpew7rag0_/custom_iter_based_trainer/latest.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpew7rag0_\u001b[0m\n",
      "└── \u001b[1;36mcustom_iter_based_trainer\u001b[0m\n",
      "    ├── 2023-07-13T13-45-02_480950-08-00.log\n",
      "    ├── iter_10.pth\n",
      "    ├── iter_20.pth\n",
      "    ├── iter_30.pth\n",
      "    ├── iter_40.pth\n",
      "    ├── iter_50.pth\n",
      "    └── latest.pth\n",
      "\n",
      "2 directories, 7 files\n",
      "\n",
      "dict_keys(['meta', 'strategy', 'optimizer'])\n",
      "{'iter_': 50}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_callback_demo, \n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    checkpoint_path = os.path.join(work_dirs, 'custom_iter_based_trainer', 'iter_50.pth')\n",
    "    checkpoint: dict[str, Any] = torch.load(checkpoint_path, 'cpu')\n",
    "    print(checkpoint.keys())\n",
    "    print(checkpoint['meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "checkpoint_by_epoch_callback = checkpoint_callback.copy()\n",
    "checkpoint_by_epoch_callback.update(interval=1, by_epoch=True)\n",
    "checkpoint_by_epoch_callback_demo.callbacks = [\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-13 13:45:02,965 45313:140704293135936][log.py:37 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-07-13 13:45:02,966 45313:140704293135936][log.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-07-13 13:45:02,969 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] loss=90.000 weight=0.000 batch={'x': tensor([36, 54]), 'y': tensor([ 72, 108])}\n",
      "[2023-07-13 13:45:02,972 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] loss=74.000 weight=0.000 batch={'x': tensor([67,  7]), 'y': tensor([134,  14])}\n",
      "[2023-07-13 13:45:02,974 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] loss=36.000 weight=0.000 batch={'x': tensor([12, 24]), 'y': tensor([24, 48])}\n",
      "[2023-07-13 13:45:02,977 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] loss=58.000 weight=0.000 batch={'x': tensor([56,  2]), 'y': tensor([112,   4])}\n",
      "[2023-07-13 13:45:02,980 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] loss=82.000 weight=0.000 batch={'x': tensor([49, 33]), 'y': tensor([98, 66])}\n",
      "[2023-07-13 13:45:02,982 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] loss=69.000 weight=0.000 batch={'x': tensor([28, 41]), 'y': tensor([56, 82])}\n",
      "[2023-07-13 13:45:02,984 45313:140704293135936][checkpoint.py:34 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpwjl6ntis/custom_epoch_based_trainer/epoch_1.pth\n",
      "[2023-07-13 13:45:02,986 45313:140704293135936][log.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-07-13 13:45:02,988 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] loss=52.000 weight=0.000 batch={'x': tensor([32, 20]), 'y': tensor([64, 40])}\n",
      "[2023-07-13 13:45:02,990 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] loss=100.000 weight=0.000 batch={'x': tensor([36, 64]), 'y': tensor([ 72, 128])}\n",
      "[2023-07-13 13:45:02,992 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] loss=50.000 weight=0.000 batch={'x': tensor([12, 38]), 'y': tensor([24, 76])}\n",
      "[2023-07-13 13:45:02,994 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] loss=69.000 weight=0.000 batch={'x': tensor([ 2, 67]), 'y': tensor([  4, 134])}\n",
      "[2023-07-13 13:45:02,996 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] loss=51.000 weight=0.000 batch={'x': tensor([14, 37]), 'y': tensor([28, 74])}\n",
      "[2023-07-13 13:45:02,999 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] loss=50.000 weight=0.000 batch={'x': tensor([11, 39]), 'y': tensor([22, 78])}\n",
      "[2023-07-13 13:45:03,001 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] loss=49.000 weight=0.000 batch={'x': tensor([43,  6]), 'y': tensor([86, 12])}\n",
      "[2023-07-13 13:45:03,002 45313:140704293135936][checkpoint.py:34 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpwjl6ntis/custom_epoch_based_trainer/epoch_2.pth\n",
      "[2023-07-13 13:45:03,003 45313:140704293135936][log.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-07-13 13:45:03,005 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] loss=72.000 weight=0.000 batch={'x': tensor([62, 10]), 'y': tensor([124,  20])}\n",
      "[2023-07-13 13:45:03,007 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] loss=25.000 weight=0.000 batch={'x': tensor([ 5, 20]), 'y': tensor([10, 40])}\n",
      "[2023-07-13 13:45:03,010 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] loss=82.000 weight=0.000 batch={'x': tensor([21, 61]), 'y': tensor([ 42, 122])}\n",
      "[2023-07-13 13:45:03,012 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] loss=93.000 weight=0.000 batch={'x': tensor([39, 54]), 'y': tensor([ 78, 108])}\n",
      "[2023-07-13 13:45:03,014 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] loss=38.000 weight=0.000 batch={'x': tensor([30,  8]), 'y': tensor([60, 16])}\n",
      "[2023-07-13 13:45:03,016 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] loss=47.000 weight=0.000 batch={'x': tensor([11, 36]), 'y': tensor([22, 72])}\n",
      "[2023-07-13 13:45:03,018 45313:140704293135936][log.py:52 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] loss=16.000 weight=0.000 batch={'x': tensor([12,  4]), 'y': tensor([24,  8])}\n",
      "[2023-07-13 13:45:03,019 45313:140704293135936][checkpoint.py:34 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpwjl6ntis/custom_epoch_based_trainer/epoch_3.pth\n",
      "[2023-07-13 13:45:03,021 45313:140704293135936][checkpoint.py:34 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpwjl6ntis/custom_epoch_based_trainer/latest.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpwjl6ntis\u001b[0m\n",
      "└── \u001b[1;36mcustom_epoch_based_trainer\u001b[0m\n",
      "    ├── 2023-07-13T13-45-02_965224-08-00.log\n",
      "    ├── epoch_1.pth\n",
      "    ├── epoch_2.pth\n",
      "    ├── epoch_3.pth\n",
      "    └── latest.pth\n",
      "\n",
      "2 directories, 5 files\n",
      "\n",
      "dict_keys(['meta', 'strategy', 'optimizer'])\n",
      "{'iter_': 68, 'epoch': 2}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_by_epoch_callback_demo, \n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    checkpoint_path = os.path.join(work_dirs, 'custom_epoch_based_trainer', 'epoch_2.pth')\n",
    "    checkpoint: dict[str, Any] = torch.load(checkpoint_path, 'cpu')\n",
    "    print(checkpoint.keys())\n",
    "    print(checkpoint['meta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaultyRunnerMixin(todd.runners.BaseRunner):\n",
    "    \n",
    "    def _run_iter(self, *args, **kwargs) -> None:\n",
    "        raise CustomError('faulty runner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyValidator(FaultyRunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyIterBasedTrainer(FaultyRunnerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyEpochBasedTrainer(FaultyRunnerMixin, todd.runners.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_callback_demo = validator_demo.copy()\n",
    "monitor_callback_demo.type = 'FaultyValidator'\n",
    "monitor_callback = todd.Config(type='MonitorCallback')\n",
    "monitor_callback_demo.callbacks = [monitor_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-07-13 13:45:03,460 45313:140704293135936][log.py:37 todd.FaultyValidator.custom_validator connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2023-07-13 13:45:03,462 45313:140704293135936][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x14e4cda50>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/runners.py\", line 169, in _run\n",
      "    self._run_iter(batch, memo)\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_45313/3253115116.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2023-07-13 13:45:03,460 45313:140704293135936][log.py:37 todd.FaultyValidator.custom_validator connect] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-07-13 13:45:03,462 45313:140704293135936][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x14e4cda50>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/runners.py\", line 169, in _run\n",
      "    self._run_iter(batch, memo)\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_45313/3253115116.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        monitor_callback_demo, \n",
    "        todd.Config(work_dir=dict(root=work_dirs)),\n",
    "    )\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
