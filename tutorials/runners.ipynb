{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing /Users/bytedance/Developer/todd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.6.1)\n",
      "Requirement already satisfied: lmdb in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (1.4.1)\n",
      "Requirement already satisfied: opencv-python in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.7.0.72)\n",
      "Requirement already satisfied: pandas in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: python-pptx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.6.21)\n",
      "Requirement already satisfied: timm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.9.2)\n",
      "Requirement already satisfied: toml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (4.5.0)\n",
      "Requirement already satisfied: yapf in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from todd-ai==0.4.0) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from opencv-python->todd-ai==0.4.0) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from pandas->todd-ai==0.4.0) (2023.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (4.9.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (9.5.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-pptx->todd-ai==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.7 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.14.1)\n",
      "Requirement already satisfied: safetensors in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from timm->todd-ai==0.4.0) (0.3.1)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from yapf->todd-ai==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->todd-ai==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.12.0)\n",
      "Requirement already satisfied: sympy in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from torch>=1.7->timm->todd-ai==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (2023.5.0)\n",
      "Requirement already satisfied: requests in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (2.30.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from huggingface-hub->timm->todd-ai==0.4.0) (23.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from jinja2->torch>=1.7->timm->todd-ai==0.4.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from requests->huggingface-hub->timm->todd-ai==0.4.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages (from sympy->torch>=1.7->timm->todd-ai==0.4.0) (1.3.0)\n",
      "Building wheels for collected packages: todd-ai\n",
      "  Building wheel for todd-ai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for todd-ai: filename=todd_ai-0.4.0-py3-none-any.whl size=108366 sha256=2fec75da0858253d8a6f2ac57730117869162ec01295bebb7baa551526073a5c\n",
      "  Stored in directory: /private/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/pip-ephem-wheel-cache-_biha_6c/wheels/15/ef/5a/9fc12e257ce5cef16b333a2ed6c992ff9cbcc9167f7199e6ac\n",
      "Successfully built todd-ai\n",
      "Installing collected packages: todd-ai\n",
      "Successfully installed todd-ai-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "[2023-08-03 16:16:32,183 45971:140704487245376][patches.py:14 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tempfile\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict, cast\n",
    "\n",
    "import todd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "Memo = dict[str, Any]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register()\n",
    "class RunnerModel(todd.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> Memo:\n",
    "        y: torch.Tensor = self._strategy.model(batch['x'])\n",
    "        loss = F.l1_loss(y, batch['y'])\n",
    "        memo['loss'] = loss\n",
    "        if 'log' in memo:\n",
    "            memo['log']['loss'] = f'{loss.item():.3f}'\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerMixin(RunnerMixin):\n",
    "\n",
    "    def _run_iter(self, batch: Batch, memo: Memo) -> Memo:\n",
    "        memo = super()._run_iter(batch, memo)\n",
    "        if 'log' in memo:\n",
    "            model = cast(RunnerModel, self._strategy.module)\n",
    "            memo['log']['weight'] = f'{model.weight.item():.3f}'\n",
    "            memo['log']['batch'] = str(batch)\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomValidator(RunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomIterBasedTrainer(TrainerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class CustomEpochBasedTrainer(TrainerMixin, todd.runners.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_demo = todd.Config(\n",
    "    type='CustomValidator',\n",
    "    name='custom_validator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(type='RunnerDataset', n=20)),\n",
    "    strategy=dict(type='BaseStrategy', model=dict(type='RunnerModel')),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-03 16:16:32,830 45971:140704487245376][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpo1m3sdin\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_demo.callbacks=dict(type='LogCallback', interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-03 16:16:33,129 45971:140704487245376][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:33,132 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.006384 loss=10.000\n",
      "[2023-08-03 16:16:33,134 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.003968 loss=20.000\n",
      "[2023-08-03 16:16:33,135 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.001882 loss=30.000\n",
      "[2023-08-03 16:16:33,137 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpzk9x8eqd\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        validator_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "    \n",
    "    !echo\n",
    "    !tree $work_dirs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_demo = validator_demo.copy()\n",
    "trainer_demo.pop('type')\n",
    "trainer_demo.dataloader = todd.Config(\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    dataset=dict(type='RunnerDataset', n=67),\n",
    ")\n",
    "trainer_demo.optimizer = todd.Config(type='SGD', lr=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_based_trainer_demo = trainer_demo.copy()\n",
    "iter_based_trainer_demo.type = 'CustomIterBasedTrainer'\n",
    "iter_based_trainer_demo.name = 'custom_iter_based_trainer'\n",
    "iter_based_trainer_demo.iters = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-03 16:16:33,432 45971:140704487245376][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:33,435 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.022387 loss=68.000 weight=0.000 batch={'x': tensor([66,  2]), 'y': tensor([132,   4])}\n",
      "[2023-08-03 16:16:33,438 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.020752 loss=73.000 weight=0.000 batch={'x': tensor([52, 21]), 'y': tensor([104,  42])}\n",
      "[2023-08-03 16:16:33,440 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.017726 loss=77.000 weight=0.000 batch={'x': tensor([44, 33]), 'y': tensor([88, 66])}\n",
      "[2023-08-03 16:16:33,442 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.014815 loss=47.000 weight=0.000 batch={'x': tensor([ 6, 41]), 'y': tensor([12, 82])}\n",
      "[2023-08-03 16:16:33,444 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.012470 loss=72.000 weight=0.000 batch={'x': tensor([43, 29]), 'y': tensor([86, 58])}\n",
      "[2023-08-03 16:16:33,446 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.010137 loss=51.000 weight=0.000 batch={'x': tensor([34, 17]), 'y': tensor([68, 34])}\n",
      "[2023-08-03 16:16:33,448 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.007798 loss=79.000 weight=0.000 batch={'x': tensor([31, 48]), 'y': tensor([62, 96])}\n",
      "[2023-08-03 16:16:33,450 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.005421 loss=67.000 weight=0.000 batch={'x': tensor([54, 13]), 'y': tensor([108,  26])}\n",
      "[2023-08-03 16:16:33,451 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.003234 loss=29.000 weight=0.000 batch={'x': tensor([25,  4]), 'y': tensor([50,  8])}\n",
      "[2023-08-03 16:16:33,453 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.001193 loss=97.000 weight=0.000 batch={'x': tensor([32, 65]), 'y': tensor([ 64, 130])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        iter_based_trainer_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_based_trainer_demo = trainer_demo.copy()\n",
    "epoch_based_trainer_demo.type = 'CustomEpochBasedTrainer'\n",
    "epoch_based_trainer_demo.name = 'custom_epoch_based_trainer'\n",
    "epoch_based_trainer_demo.epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-08-03 16:16:33,469 45971:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:33,470 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:16:33,473 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.052225 loss=92.000 weight=0.000 batch={'x': tensor([64, 28]), 'y': tensor([128,  56])}\n",
      "[2023-08-03 16:16:33,475 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.042614 loss=40.000 weight=0.000 batch={'x': tensor([34,  6]), 'y': tensor([68, 12])}\n",
      "[2023-08-03 16:16:33,477 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.037781 loss=109.000 weight=0.000 batch={'x': tensor([66, 43]), 'y': tensor([132,  86])}\n",
      "[2023-08-03 16:16:33,479 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.034624 loss=76.000 weight=0.000 batch={'x': tensor([11, 65]), 'y': tensor([ 22, 130])}\n",
      "[2023-08-03 16:16:33,480 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.031588 loss=52.000 weight=0.000 batch={'x': tensor([47,  5]), 'y': tensor([94, 10])}\n",
      "[2023-08-03 16:16:33,482 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.029527 loss=83.000 weight=0.000 batch={'x': tensor([22, 61]), 'y': tensor([ 44, 122])}\n",
      "[2023-08-03 16:16:33,484 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:16:33,486 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.029403 loss=23.000 weight=0.000 batch={'x': tensor([ 3, 20]), 'y': tensor([ 6, 40])}\n",
      "[2023-08-03 16:16:33,488 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.027370 loss=108.000 weight=0.000 batch={'x': tensor([50, 58]), 'y': tensor([100, 116])}\n",
      "[2023-08-03 16:16:33,490 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.024858 loss=106.000 weight=0.000 batch={'x': tensor([52, 54]), 'y': tensor([104, 108])}\n",
      "[2023-08-03 16:16:33,492 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.022475 loss=90.000 weight=0.000 batch={'x': tensor([29, 61]), 'y': tensor([ 58, 122])}\n",
      "[2023-08-03 16:16:33,494 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.020128 loss=11.000 weight=0.000 batch={'x': tensor([6, 5]), 'y': tensor([12, 10])}\n",
      "[2023-08-03 16:16:33,496 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.017800 loss=42.000 weight=0.000 batch={'x': tensor([12, 30]), 'y': tensor([24, 60])}\n",
      "[2023-08-03 16:16:33,497 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.015529 loss=38.000 weight=0.000 batch={'x': tensor([25, 13]), 'y': tensor([50, 26])}\n",
      "[2023-08-03 16:16:33,499 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:16:33,501 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.013890 loss=91.000 weight=0.000 batch={'x': tensor([61, 30]), 'y': tensor([122,  60])}\n",
      "[2023-08-03 16:16:33,503 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.011645 loss=57.000 weight=0.000 batch={'x': tensor([26, 31]), 'y': tensor([52, 62])}\n",
      "[2023-08-03 16:16:33,505 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.009492 loss=30.000 weight=0.000 batch={'x': tensor([18, 12]), 'y': tensor([36, 24])}\n",
      "[2023-08-03 16:16:33,507 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.007284 loss=95.000 weight=0.000 batch={'x': tensor([67, 28]), 'y': tensor([134,  56])}\n",
      "[2023-08-03 16:16:33,508 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.005102 loss=22.000 weight=0.000 batch={'x': tensor([ 8, 14]), 'y': tensor([16, 28])}\n",
      "[2023-08-03 16:16:33,510 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.002956 loss=36.000 weight=0.000 batch={'x': tensor([ 9, 27]), 'y': tensor([18, 54])}\n",
      "[2023-08-03 16:16:33,512 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.000838 loss=47.000 weight=0.000 batch={'x': tensor([32, 15]), 'y': tensor([64, 30])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        epoch_based_trainer_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_callback_demo = validator_demo.copy()\n",
    "log_callback = log_callback_demo.callbacks\n",
    "log_callback.collect_env = todd.Config(verbose=False)\n",
    "log_callback.with_file_handler = True\n",
    "log_callback_demo.callbacks = [log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:33,648 45971:140704487245376][log.py:50 todd.CustomValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:33,653 45971:140704487245376][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:33,658 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.011835 loss=10.000\n",
      "[2023-08-03 16:16:33,661 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.006097 loss=20.000\n",
      "[2023-08-03 16:16:33,663 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.002794 loss=30.000\n",
      "[2023-08-03 16:16:33,665 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp609c9fi8\u001b[0m\n",
      "└── \u001b[1;36mcustom_validator\u001b[0m\n",
      "    └── 2023-08-03T16-16-33_531839-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2023-08-03 16:16:33,648 45971:140704487245376][log.py:50 todd.CustomValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "[2023-08-03 16:16:33,653 45971:140704487245376][base.py:53 todd.CustomValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-08-03 16:16:33,658 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [5/20] ETA 0:00:00.011835 loss=10.000\n",
      "[2023-08-03 16:16:33,661 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [10/20] ETA 0:00:00.006097 loss=20.000\n",
      "[2023-08-03 16:16:33,663 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [15/20] ETA 0:00:00.002794 loss=30.000\n",
      "[2023-08-03 16:16:33,665 45971:140704487245376][log.py:83 todd.CustomValidator.custom_validator after_run_iter] INFO: Iter [20/20] ETA 0:00:00 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        log_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_callback_demo = iter_based_trainer_demo.copy()\n",
    "optimize_callback = todd.Config(type='OptimizeCallback')\n",
    "optimize_callback_demo.callbacks = [optimize_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:34,324 45971:140704487245376][log.py:50 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:34,326 45971:140704487245376][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:34,332 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.038784 loss=25.113 weight=0.775 batch={'x': tensor([29, 12]), 'y': tensor([58, 24])}\n",
      "[2023-08-03 16:16:34,335 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.031609 loss=17.440 weight=1.455 batch={'x': tensor([34, 30]), 'y': tensor([68, 60])}\n",
      "[2023-08-03 16:16:34,338 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.027160 loss=3.988 weight=2.073 batch={'x': tensor([64, 46]), 'y': tensor([128,  92])}\n",
      "[2023-08-03 16:16:34,341 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.022791 loss=1.691 weight=1.918 batch={'x': tensor([36,  5]), 'y': tensor([72, 10])}\n",
      "[2023-08-03 16:16:34,344 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.018545 loss=5.591 weight=1.823 batch={'x': tensor([32, 31]), 'y': tensor([64, 62])}\n",
      "[2023-08-03 16:16:34,347 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.014760 loss=8.375 weight=1.833 batch={'x': tensor([63, 37]), 'y': tensor([126,  74])}\n",
      "[2023-08-03 16:16:34,350 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.011493 loss=3.769 weight=1.888 batch={'x': tensor([24, 43]), 'y': tensor([48, 86])}\n",
      "[2023-08-03 16:16:34,353 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.008193 loss=2.400 weight=1.920 batch={'x': tensor([ 4, 56]), 'y': tensor([  8, 112])}\n",
      "[2023-08-03 16:16:34,355 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.004935 loss=2.593 weight=2.043 batch={'x': tensor([62, 60]), 'y': tensor([124, 120])}\n",
      "[2023-08-03 16:16:34,358 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.001816 loss=6.200 weight=2.200 batch={'x': tensor([22, 40]), 'y': tensor([44, 80])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        optimize_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_schedule_callback = todd.Config(\n",
    "    type='LRScheduleCallback',\n",
    "    lr_scheduler=dict(type='LinearLR', total_iters=10),\n",
    ")\n",
    "lr_schedule_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_callback,\n",
    "    log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:34,465 45971:140704487245376][log.py:50 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:34,467 45971:140704487245376][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:34,478 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.087533 loss=60.542 weight=0.245 batch={'x': tensor([36, 33]), 'y': tensor([72, 66])} lr=['3.333e-03']\n",
      "[2023-08-03 16:16:34,513 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.193560 loss=27.083 weight=0.796 batch={'x': tensor([25, 20]), 'y': tensor([50, 40])} lr=['5.000e-03']\n",
      "[2023-08-03 16:16:34,517 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.123875 loss=1.762 weight=1.706 batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} lr=['5.000e-03']\n",
      "[2023-08-03 16:16:34,522 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.087646 loss=7.947 weight=2.279 batch={'x': tensor([19, 38]), 'y': tensor([38, 76])} lr=['5.000e-03']\n",
      "[2023-08-03 16:16:34,525 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.063600 loss=2.456 weight=2.046 batch={'x': tensor([57, 49]), 'y': tensor([114,  98])} lr=['5.000e-03']\n",
      "[2023-08-03 16:16:34,528 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.045998 loss=0.588 weight=2.014 batch={'x': tensor([64, 21]), 'y': tensor([128,  42])} lr=['5.000e-03']\n",
      "[2023-08-03 16:16:34,532 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.032585 loss=4.046 weight=2.076 batch={'x': tensor([61, 45]), 'y': tensor([122,  90])} lr=['5.000e-03']\n",
      "[2023-08-03 16:16:34,535 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.021544 loss=0.330 weight=1.971 batch={'x': tensor([ 7, 16]), 'y': tensor([14, 32])} lr=['5.000e-03']\n",
      "[2023-08-03 16:16:34,539 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.012597 loss=2.564 weight=2.044 batch={'x': tensor([55, 62]), 'y': tensor([110, 124])} lr=['5.000e-03']\n",
      "[2023-08-03 16:16:34,542 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.004435 loss=12.042 weight=2.234 batch={'x': tensor([51, 52]), 'y': tensor([102, 104])} lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "lr_schedule_by_epoch_callback = lr_schedule_callback.copy()\n",
    "lr_schedule_by_epoch_callback.by_epoch = True\n",
    "lr_schedule_by_epoch_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_schedule_by_epoch_callback,\n",
    "    log_callback,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:34,676 45971:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:34,678 45971:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:34,680 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:16:34,685 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.101229 loss=99.167 weight=0.229 batch={'x': tensor([54, 58]), 'y': tensor([108, 116])} lr=['1.667e-03']\n",
      "[2023-08-03 16:16:34,689 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.087115 loss=65.230 weight=0.534 batch={'x': tensor([56, 33]), 'y': tensor([112,  66])} lr=['1.667e-03']\n",
      "[2023-08-03 16:16:34,693 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.076293 loss=35.495 weight=0.855 batch={'x': tensor([21, 41]), 'y': tensor([42, 82])} lr=['1.667e-03']\n",
      "[2023-08-03 16:16:34,696 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.069725 loss=15.602 weight=1.157 batch={'x': tensor([ 2, 35]), 'y': tensor([ 4, 70])} lr=['1.667e-03']\n",
      "[2023-08-03 16:16:34,701 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.065964 loss=11.547 weight=1.376 batch={'x': tensor([13, 24]), 'y': tensor([26, 48])} lr=['1.667e-03']\n",
      "[2023-08-03 16:16:34,708 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.069547 loss=4.973 weight=1.689 batch={'x': tensor([18, 14]), 'y': tensor([36, 28])} lr=['1.667e-03']\n",
      "[2023-08-03 16:16:34,713 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:16:34,715 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.067362 loss=2.297 weight=1.912 batch={'x': tensor([19, 33]), 'y': tensor([38, 66])} lr=['2.000e-03']\n",
      "[2023-08-03 16:16:34,720 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.062144 loss=1.463 weight=1.949 batch={'x': tensor([20, 37]), 'y': tensor([40, 74])} lr=['2.000e-03']\n",
      "[2023-08-03 16:16:34,726 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.058734 loss=2.025 weight=1.928 batch={'x': tensor([ 5, 51]), 'y': tensor([ 10, 102])} lr=['2.000e-03']\n",
      "[2023-08-03 16:16:34,730 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.052358 loss=2.320 weight=1.952 batch={'x': tensor([38, 58]), 'y': tensor([ 76, 116])} lr=['2.000e-03']\n",
      "[2023-08-03 16:16:34,736 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.048606 loss=3.689 weight=1.928 batch={'x': tensor([36, 66]), 'y': tensor([ 72, 132])} lr=['2.000e-03']\n",
      "[2023-08-03 16:16:34,742 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.043465 loss=3.411 weight=1.943 batch={'x': tensor([55, 64]), 'y': tensor([110, 128])} lr=['2.000e-03']\n",
      "[2023-08-03 16:16:34,745 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.037348 loss=1.405 weight=1.935 batch={'x': tensor([34,  9]), 'y': tensor([68, 18])} lr=['2.000e-03']\n",
      "[2023-08-03 16:16:34,747 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:16:34,751 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.032502 loss=1.022 weight=1.969 batch={'x': tensor([12, 55]), 'y': tensor([ 24, 110])} lr=['2.333e-03']\n",
      "[2023-08-03 16:16:34,755 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.027165 loss=2.000 weight=2.050 batch={'x': tensor([19, 61]), 'y': tensor([ 38, 122])} lr=['2.333e-03']\n",
      "[2023-08-03 16:16:34,760 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.022090 loss=2.429 weight=2.113 batch={'x': tensor([20, 23]), 'y': tensor([40, 46])} lr=['2.333e-03']\n",
      "[2023-08-03 16:16:34,763 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.016752 loss=2.537 weight=2.043 batch={'x': tensor([53, 65]), 'y': tensor([106, 130])} lr=['2.333e-03']\n",
      "[2023-08-03 16:16:34,768 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.011866 loss=1.217 weight=2.024 batch={'x': tensor([48, 52]), 'y': tensor([ 96, 104])} lr=['2.333e-03']\n",
      "[2023-08-03 16:16:34,774 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.006943 loss=1.113 weight=1.972 batch={'x': tensor([29, 50]), 'y': tensor([ 58, 100])} lr=['2.333e-03']\n",
      "[2023-08-03 16:16:34,777 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001952 loss=1.334 weight=2.052 batch={'x': tensor([44,  7]), 'y': tensor([88, 14])} lr=['2.333e-03']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_schedule_by_epoch_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scaler_callback_demo = iter_based_trainer_demo.copy()\n",
    "lr_scaler_callback = todd.Config(\n",
    "    type='LRScaleCallback',\n",
    "    lr_scaler=dict(base_batch_size=1),\n",
    ")\n",
    "lr_scaler_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    lr_scaler_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:34,806 45971:140704487245376][lr.py:92 todd.CustomIterBasedTrainer.custom_iter_based_trainer _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "[2023-08-03 16:16:34,918 45971:140704487245376][log.py:50 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:34,920 45971:140704487245376][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:34,927 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.032832 loss=48.620 weight=0.895 batch={'x': tensor([26, 62]), 'y': tensor([ 52, 124])}\n",
      "[2023-08-03 16:16:34,930 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.029464 loss=0.495 weight=2.090 batch={'x': tensor([8, 3]), 'y': tensor([16,  6])}\n",
      "[2023-08-03 16:16:34,935 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.029318 loss=10.657 weight=2.245 batch={'x': tensor([31, 56]), 'y': tensor([ 62, 112])}\n",
      "[2023-08-03 16:16:34,940 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.027108 loss=13.650 weight=2.300 batch={'x': tensor([38, 53]), 'y': tensor([ 76, 106])}\n",
      "[2023-08-03 16:16:34,944 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.022878 loss=2.125 weight=1.950 batch={'x': tensor([21, 64]), 'y': tensor([ 42, 128])}\n",
      "[2023-08-03 16:16:34,948 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.018743 loss=9.120 weight=1.620 batch={'x': tensor([43,  5]), 'y': tensor([86, 10])}\n",
      "[2023-08-03 16:16:34,951 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.014460 loss=7.262 weight=1.825 batch={'x': tensor([47, 36]), 'y': tensor([94, 72])}\n",
      "[2023-08-03 16:16:34,955 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.010397 loss=12.245 weight=1.690 batch={'x': tensor([25, 54]), 'y': tensor([ 50, 108])}\n",
      "[2023-08-03 16:16:34,960 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.006488 loss=7.260 weight=1.780 batch={'x': tensor([48, 18]), 'y': tensor([96, 36])}\n",
      "[2023-08-03 16:16:34,964 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.002425 loss=0.140 weight=2.005 batch={'x': tensor([ 7, 49]), 'y': tensor([14, 98])}\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        lr_scaler_callback_demo,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback_demo = iter_based_trainer_demo.copy()\n",
    "checkpoint_callback = todd.Config(type='CheckpointCallback', interval=10)\n",
    "checkpoint_callback_demo.callbacks = [checkpoint_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:35,141 45971:140704487245376][log.py:50 todd.CustomIterBasedTrainer.custom_iter_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:35,143 45971:140704487245376][base.py:53 todd.CustomIterBasedTrainer.custom_iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:35,147 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [5/53] ETA 0:00:00.016829 loss=68.000 weight=0.000 batch={'x': tensor([17, 51]), 'y': tensor([ 34, 102])}\n",
      "[2023-08-03 16:16:35,151 45971:140704487245376][checkpoint.py:61 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmptgrxdjgh/custom_iter_based_trainer/checkpoints/iter_10\n",
      "[2023-08-03 16:16:35,155 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [10/53] ETA 0:00:00.039655 loss=80.000 weight=0.000 batch={'x': tensor([14, 66]), 'y': tensor([ 28, 132])}\n",
      "[2023-08-03 16:16:35,158 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [15/53] ETA 0:00:00.032835 loss=87.000 weight=0.000 batch={'x': tensor([67, 20]), 'y': tensor([134,  40])}\n",
      "[2023-08-03 16:16:35,161 45971:140704487245376][checkpoint.py:61 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmptgrxdjgh/custom_iter_based_trainer/checkpoints/iter_20\n",
      "[2023-08-03 16:16:35,164 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [20/53] ETA 0:00:00.030395 loss=58.000 weight=0.000 batch={'x': tensor([15, 43]), 'y': tensor([30, 86])}\n",
      "[2023-08-03 16:16:35,166 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [25/53] ETA 0:00:00.023595 loss=61.000 weight=0.000 batch={'x': tensor([11, 50]), 'y': tensor([ 22, 100])}\n",
      "[2023-08-03 16:16:35,169 45971:140704487245376][checkpoint.py:61 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmptgrxdjgh/custom_iter_based_trainer/checkpoints/iter_30\n",
      "[2023-08-03 16:16:35,171 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [30/53] ETA 0:00:00.019952 loss=62.000 weight=0.000 batch={'x': tensor([23, 39]), 'y': tensor([46, 78])}\n",
      "[2023-08-03 16:16:35,174 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [35/53] ETA 0:00:00.014575 loss=101.000 weight=0.000 batch={'x': tensor([36, 65]), 'y': tensor([ 72, 130])}\n",
      "[2023-08-03 16:16:35,176 45971:140704487245376][checkpoint.py:61 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmptgrxdjgh/custom_iter_based_trainer/checkpoints/iter_40\n",
      "[2023-08-03 16:16:35,178 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [40/53] ETA 0:00:00.010677 loss=98.000 weight=0.000 batch={'x': tensor([35, 63]), 'y': tensor([ 70, 126])}\n",
      "[2023-08-03 16:16:35,180 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [45/53] ETA 0:00:00.006217 loss=89.000 weight=0.000 batch={'x': tensor([62, 27]), 'y': tensor([124,  54])}\n",
      "[2023-08-03 16:16:35,182 45971:140704487245376][checkpoint.py:61 todd.CustomIterBasedTrainer.custom_iter_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmptgrxdjgh/custom_iter_based_trainer/checkpoints/iter_50\n",
      "[2023-08-03 16:16:35,185 45971:140704487245376][log.py:83 todd.CustomIterBasedTrainer.custom_iter_based_trainer after_run_iter] INFO: Iter [50/53] ETA 0:00:00.002365 loss=61.000 weight=0.000 batch={'x': tensor([53,  8]), 'y': tensor([106,  16])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmptgrxdjgh\u001b[0m\n",
      "└── \u001b[1;36mcustom_iter_based_trainer\u001b[0m\n",
      "    ├── 2023-08-03T16-16-35_023548-08-00.log\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_20\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_30\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_40\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_50\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmptgrxdjgh/custom_iter_based_trainer/checkpoints/iter_50\u001b[0m\n",
      "\n",
      "9 directories, 26 files\n",
      "\n",
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 50}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomIterBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_50 = pathlib.Path(work_dirs) / 'custom_iter_based_trainer' / 'checkpoints' / 'iter_50'\n",
    "    for f in iter_50.glob('*.pth'):\n",
    "        print(f\"{f.name}:\")\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_by_epoch_callback_demo = epoch_based_trainer_demo.copy()\n",
    "checkpoint_by_epoch_callback = checkpoint_callback.copy()\n",
    "checkpoint_by_epoch_callback.update(interval=1, by_epoch=True)\n",
    "checkpoint_by_epoch_callback_demo.callbacks = [\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:35,737 45971:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:35,739 45971:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:35,741 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:16:35,744 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.077076 loss=44.000 weight=0.000 batch={'x': tensor([15, 29]), 'y': tensor([30, 58])}\n",
      "[2023-08-03 16:16:35,747 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.062063 loss=85.000 weight=0.000 batch={'x': tensor([60, 25]), 'y': tensor([120,  50])}\n",
      "[2023-08-03 16:16:35,750 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.054607 loss=70.000 weight=0.000 batch={'x': tensor([ 5, 65]), 'y': tensor([ 10, 130])}\n",
      "[2023-08-03 16:16:35,753 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.050725 loss=80.000 weight=0.000 batch={'x': tensor([47, 33]), 'y': tensor([94, 66])}\n",
      "[2023-08-03 16:16:35,755 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.046440 loss=67.000 weight=0.000 batch={'x': tensor([50, 17]), 'y': tensor([100,  34])}\n",
      "[2023-08-03 16:16:35,758 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.041609 loss=23.000 weight=0.000 batch={'x': tensor([21,  2]), 'y': tensor([42,  4])}\n",
      "[2023-08-03 16:16:35,760 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp53_sjt06/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-03 16:16:35,763 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:16:35,766 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.048355 loss=45.000 weight=0.000 batch={'x': tensor([19, 26]), 'y': tensor([38, 52])}\n",
      "[2023-08-03 16:16:35,768 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.042823 loss=73.000 weight=0.000 batch={'x': tensor([40, 33]), 'y': tensor([80, 66])}\n",
      "[2023-08-03 16:16:35,771 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.038490 loss=88.000 weight=0.000 batch={'x': tensor([49, 39]), 'y': tensor([98, 78])}\n",
      "[2023-08-03 16:16:35,773 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.034014 loss=64.000 weight=0.000 batch={'x': tensor([47, 17]), 'y': tensor([94, 34])}\n",
      "[2023-08-03 16:16:35,776 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.030135 loss=75.000 weight=0.000 batch={'x': tensor([61, 14]), 'y': tensor([122,  28])}\n",
      "[2023-08-03 16:16:35,778 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.026297 loss=132.000 weight=0.000 batch={'x': tensor([65, 67]), 'y': tensor([130, 134])}\n",
      "[2023-08-03 16:16:35,780 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.022700 loss=88.000 weight=0.000 batch={'x': tensor([66, 22]), 'y': tensor([132,  44])}\n",
      "[2023-08-03 16:16:35,783 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp53_sjt06/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-03 16:16:35,786 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:16:35,788 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.021936 loss=56.000 weight=0.000 batch={'x': tensor([17, 39]), 'y': tensor([34, 78])}\n",
      "[2023-08-03 16:16:35,791 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.018200 loss=26.000 weight=0.000 batch={'x': tensor([22,  4]), 'y': tensor([44,  8])}\n",
      "[2023-08-03 16:16:35,794 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.014683 loss=72.000 weight=0.000 batch={'x': tensor([31, 41]), 'y': tensor([62, 82])}\n",
      "[2023-08-03 16:16:35,796 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.011206 loss=47.000 weight=0.000 batch={'x': tensor([18, 29]), 'y': tensor([36, 58])}\n",
      "[2023-08-03 16:16:35,799 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.007853 loss=79.000 weight=0.000 batch={'x': tensor([26, 53]), 'y': tensor([ 52, 106])}\n",
      "[2023-08-03 16:16:35,802 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.004545 loss=111.000 weight=0.000 batch={'x': tensor([49, 62]), 'y': tensor([ 98, 124])}\n",
      "[2023-08-03 16:16:35,805 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001296 loss=39.000 weight=0.000 batch={'x': tensor([ 9, 30]), 'y': tensor([18, 60])}\n",
      "[2023-08-03 16:16:35,807 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp53_sjt06/custom_epoch_based_trainer/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp53_sjt06\u001b[0m\n",
      "└── \u001b[1;36mcustom_epoch_based_trainer\u001b[0m\n",
      "    ├── 2023-08-03T16-16-35_597787-08-00.log\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp53_sjt06/custom_epoch_based_trainer/checkpoints/epoch_3\u001b[0m\n",
      "\n",
      "7 directories, 16 files\n",
      "\n",
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {}}\n",
      "\n",
      "meta.pth:\n",
      "{'epoch': 2, 'iter_': 68}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_by_epoch_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'custom_epoch_based_trainer' / 'checkpoints' / 'epoch_2'\n",
    "    for f in epoch_2.glob('*.pth'):\n",
    "        print(f\"{f.name}:\")\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_load_from_callback_demo = checkpoint_by_epoch_callback_demo.copy()\n",
    "checkpoint_load_from_callback_demo.callbacks = [\n",
    "    optimize_callback,\n",
    "    checkpoint_by_epoch_callback,\n",
    "    log_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:36,332 45971:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:36,333 45971:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:36,335 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:16:36,342 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.138167 loss=49.853 weight=0.555 batch={'x': tensor([10, 59]), 'y': tensor([ 20, 118])}\n",
      "[2023-08-03 16:16:36,345 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.094907 loss=26.910 weight=1.415 batch={'x': tensor([65, 27]), 'y': tensor([130,  54])}\n",
      "[2023-08-03 16:16:36,348 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.078561 loss=0.113 weight=1.985 batch={'x': tensor([9, 6]), 'y': tensor([18, 12])}\n",
      "[2023-08-03 16:16:36,351 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.069216 loss=1.075 weight=2.108 batch={'x': tensor([19,  1]), 'y': tensor([38,  2])}\n",
      "[2023-08-03 16:16:36,355 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.062478 loss=0.950 weight=2.020 batch={'x': tensor([38, 57]), 'y': tensor([ 76, 114])}\n",
      "[2023-08-03 16:16:36,358 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.056342 loss=6.600 weight=1.760 batch={'x': tensor([23, 32]), 'y': tensor([46, 64])}\n",
      "[2023-08-03 16:16:36,361 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjixqamyw/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-03 16:16:36,365 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:16:36,368 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.064029 loss=2.960 weight=1.920 batch={'x': tensor([ 8, 66]), 'y': tensor([ 16, 132])}\n",
      "[2023-08-03 16:16:36,372 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.057476 loss=6.720 weight=2.140 batch={'x': tensor([49, 47]), 'y': tensor([98, 94])}\n",
      "[2023-08-03 16:16:36,375 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.050931 loss=4.230 weight=1.820 batch={'x': tensor([38,  9]), 'y': tensor([76, 18])}\n",
      "[2023-08-03 16:16:36,378 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.044897 loss=1.050 weight=2.075 batch={'x': tensor([10, 18]), 'y': tensor([20, 36])}\n",
      "[2023-08-03 16:16:36,381 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.039484 loss=0.935 weight=2.028 batch={'x': tensor([28, 40]), 'y': tensor([56, 80])}\n",
      "[2023-08-03 16:16:36,413 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.055296 loss=0.413 weight=2.015 batch={'x': tensor([30, 25]), 'y': tensor([60, 50])}\n",
      "[2023-08-03 16:16:36,417 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.047128 loss=1.283 weight=2.095 batch={'x': tensor([21,  6]), 'y': tensor([42, 12])}\n",
      "[2023-08-03 16:16:36,424 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjixqamyw/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-03 16:16:36,426 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:16:36,429 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.043154 loss=4.104 weight=1.833 batch={'x': tensor([12, 37]), 'y': tensor([24, 74])}\n",
      "[2023-08-03 16:16:36,432 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.035067 loss=0.525 weight=2.018 batch={'x': tensor([44, 16]), 'y': tensor([88, 32])}\n",
      "[2023-08-03 16:16:36,435 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.027564 loss=9.844 weight=1.813 batch={'x': tensor([55, 50]), 'y': tensor([110, 100])}\n",
      "[2023-08-03 16:16:36,438 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.020721 loss=0.510 weight=2.020 batch={'x': tensor([41, 10]), 'y': tensor([82, 20])}\n",
      "[2023-08-03 16:16:36,441 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.014229 loss=4.500 weight=1.850 batch={'x': tensor([54,  6]), 'y': tensor([108,  12])}\n",
      "[2023-08-03 16:16:36,444 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.008084 loss=4.379 weight=2.078 batch={'x': tensor([61, 52]), 'y': tensor([122, 104])}\n",
      "[2023-08-03 16:16:36,447 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.002249 loss=2.531 weight=2.068 batch={'x': tensor([36, 39]), 'y': tensor([72, 78])}\n",
      "[2023-08-03 16:16:36,449 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjixqamyw/custom_epoch_based_trainer/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:36,841 45971:140704487245376][checkpoint.py:46 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjixqamyw/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-03 16:16:36,922 45971:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:36,924 45971:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:36,925 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:16:36,929 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.057728 loss=4.030 weight=1.845 batch={'x': tensor([16, 36]), 'y': tensor([32, 72])}\n",
      "[2023-08-03 16:16:36,932 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.026097 loss=7.695 weight=1.730 batch={'x': tensor([46, 11]), 'y': tensor([92, 22])}\n",
      "[2023-08-03 16:16:36,935 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.018364 loss=0.923 weight=2.023 batch={'x': tensor([31, 51]), 'y': tensor([ 62, 102])}\n",
      "[2023-08-03 16:16:36,939 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.013460 loss=2.682 weight=1.908 batch={'x': tensor([41, 17]), 'y': tensor([82, 34])}\n",
      "[2023-08-03 16:16:36,942 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.009115 loss=3.797 weight=1.923 batch={'x': tensor([48, 50]), 'y': tensor([ 96, 100])}\n",
      "[2023-08-03 16:16:36,945 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.005101 loss=7.350 weight=1.825 batch={'x': tensor([54, 30]), 'y': tensor([108,  60])}\n",
      "[2023-08-03 16:16:36,948 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001418 loss=6.773 weight=2.158 batch={'x': tensor([27, 59]), 'y': tensor([ 54, 118])}\n",
      "[2023-08-03 16:16:36,950 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpjixqamyw/custom_epoch_based_trainer/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_load_from_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        checkpoint_load_from_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "        load_from=os.path.join(work_dirs, 'custom_epoch_based_trainer', 'checkpoints', 'epoch_2')\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaultyRunnerMixin(todd.runners.BaseRunner):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError('faulty runner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyValidator(FaultyRunnerMixin, todd.runners.Validator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyIterBasedTrainer(FaultyRunnerMixin, todd.runners.IterBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register()\n",
    "class FaultyEpochBasedTrainer(\n",
    "    FaultyRunnerMixin,\n",
    "    todd.runners.EpochBasedTrainer,\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_callback_demo = validator_demo.copy()\n",
    "monitor_callback_demo.type = 'FaultyValidator'\n",
    "monitor_callback = todd.Config(type='MonitorCallback')\n",
    "monitor_callback_demo.callbacks = [monitor_callback, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:37,105 45971:140704487245376][log.py:50 todd.FaultyValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:37,106 45971:140704487245376][base.py:53 todd.FaultyValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2023-08-03 16:16:37,110 45971:140704487245376][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x153736a90>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 193, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_45971/2137902126.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:37,105 45971:140704487245376][log.py:50 todd.FaultyValidator.custom_validator init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "[2023-08-03 16:16:37,106 45971:140704487245376][base.py:53 todd.FaultyValidator.custom_validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2023-08-03 16:16:37,110 45971:140704487245376][monitor.py:28 todd.FaultyValidator.custom_validator __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x153736a90>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 193, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_45971/2137902126.py\", line 4, in _run_iter\n",
      "    raise CustomError('faulty runner')\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomValidator = todd.RunnerRegistry.build(\n",
    "        monitor_callback_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/custom_validator/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_load_model_from_demo = checkpoint_load_from_callback_demo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:37,490 45971:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:37,492 45971:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:37,494 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:16:37,499 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.099367 loss=30.300 weight=0.738 batch={'x': tensor([46,  2]), 'y': tensor([92,  4])}\n",
      "[2023-08-03 16:16:37,503 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.080776 loss=17.940 weight=1.610 batch={'x': tensor([30, 62]), 'y': tensor([ 60, 124])}\n",
      "[2023-08-03 16:16:37,507 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.074141 loss=2.475 weight=2.050 batch={'x': tensor([45, 54]), 'y': tensor([ 90, 108])}\n",
      "[2023-08-03 16:16:37,511 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.068458 loss=1.125 weight=2.030 batch={'x': tensor([51, 24]), 'y': tensor([102,  48])}\n",
      "[2023-08-03 16:16:37,514 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.061674 loss=1.594 weight=1.957 batch={'x': tensor([53, 22]), 'y': tensor([106,  44])}\n",
      "[2023-08-03 16:16:37,518 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.056930 loss=2.185 weight=2.047 batch={'x': tensor([26, 66]), 'y': tensor([ 52, 132])}\n",
      "[2023-08-03 16:16:37,522 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7qhgjhft/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-03 16:16:37,525 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:16:37,527 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.063393 loss=6.050 weight=2.220 batch={'x': tensor([20, 35]), 'y': tensor([40, 70])}\n",
      "[2023-08-03 16:16:37,530 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.056598 loss=2.210 weight=1.957 batch={'x': tensor([43, 61]), 'y': tensor([ 86, 122])}\n",
      "[2023-08-03 16:16:37,534 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.050847 loss=3.870 weight=2.180 batch={'x': tensor([ 9, 34]), 'y': tensor([18, 68])}\n",
      "[2023-08-03 16:16:37,538 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.045804 loss=0.225 weight=1.990 batch={'x': tensor([31, 14]), 'y': tensor([62, 28])}\n",
      "[2023-08-03 16:16:37,541 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.040399 loss=1.705 weight=2.110 batch={'x': tensor([26,  5]), 'y': tensor([52, 10])}\n",
      "[2023-08-03 16:16:37,544 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.035373 loss=7.380 weight=1.820 batch={'x': tensor([28, 54]), 'y': tensor([ 56, 108])}\n",
      "[2023-08-03 16:16:37,548 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.030668 loss=0.540 weight=1.990 batch={'x': tensor([58, 50]), 'y': tensor([116, 100])}\n",
      "[2023-08-03 16:16:37,551 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7qhgjhft/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-03 16:16:37,554 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:16:37,557 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.028663 loss=0.731 weight=2.037 batch={'x': tensor([ 6, 33]), 'y': tensor([12, 66])}\n",
      "[2023-08-03 16:16:37,560 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.023722 loss=4.461 weight=2.107 batch={'x': tensor([32, 51]), 'y': tensor([ 64, 102])}\n",
      "[2023-08-03 16:16:37,563 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.019078 loss=2.415 weight=1.947 batch={'x': tensor([49, 43]), 'y': tensor([98, 86])}\n",
      "[2023-08-03 16:16:37,567 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.014555 loss=3.263 weight=1.887 batch={'x': tensor([34, 24]), 'y': tensor([68, 48])}\n",
      "[2023-08-03 16:16:37,570 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.010186 loss=2.535 weight=2.097 batch={'x': tensor([41, 11]), 'y': tensor([82, 22])}\n",
      "[2023-08-03 16:16:37,574 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.005881 loss=0.130 weight=1.997 batch={'x': tensor([67, 37]), 'y': tensor([134,  74])}\n",
      "[2023-08-03 16:16:37,577 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001662 loss=6.756 weight=1.882 batch={'x': tensor([62, 53]), 'y': tensor([124, 106])}\n",
      "[2023-08-03 16:16:37,579 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7qhgjhft/custom_epoch_based_trainer/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-03 16:16:38,067 45971:140704487245376][log.py:50 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer init] INFO: \n",
      "Platform: macOS-13.4.1\n",
      "NVIDIA SMI: None\n",
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "PyTorch version: 2.0.1\n",
      "TorchVision version: 0.15.2\n",
      "OpenCV version: 4.7.0\n",
      "Todd version: 0.4.0\n",
      "CUDA_HOME: None\n",
      "Git commit ID: 3997d99\n",
      "Git status: M todd/runners/callbacks/checkpoint.py\n",
      "\u001b[2m[2023-08-03 16:16:38,069 45971:140704487245376][base.py:53 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2023-08-03 16:16:38,070 45971:140704487245376][base.py:60 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7qhgjhft/custom_epoch_based_trainer/checkpoints/epoch_2/model.pth\n",
      "[2023-08-03 16:16:38,073 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2023-08-03 16:16:38,077 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [5/102] ETA 0:00:00.076048 loss=0.281 weight=2.007 batch={'x': tensor([ 8, 67]), 'y': tensor([ 16, 134])}\n",
      "[2023-08-03 16:16:38,081 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [10/102] ETA 0:00:00.065614 loss=4.219 weight=2.112 batch={'x': tensor([18, 57]), 'y': tensor([ 36, 114])}\n",
      "[2023-08-03 16:16:38,084 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [15/102] ETA 0:00:00.063794 loss=2.940 weight=1.860 batch={'x': tensor([ 7, 35]), 'y': tensor([14, 70])}\n",
      "[2023-08-03 16:16:38,088 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [20/102] ETA 0:00:00.059577 loss=5.135 weight=1.870 batch={'x': tensor([21, 58]), 'y': tensor([ 42, 116])}\n",
      "[2023-08-03 16:16:38,091 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [25/102] ETA 0:00:00.054562 loss=2.310 weight=1.895 batch={'x': tensor([25, 19]), 'y': tensor([50, 38])}\n",
      "[2023-08-03 16:16:38,094 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [30/102] ETA 0:00:00.050676 loss=0.184 weight=1.992 batch={'x': tensor([26, 23]), 'y': tensor([52, 46])}\n",
      "[2023-08-03 16:16:38,097 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7qhgjhft/custom_epoch_based_trainer/checkpoints/epoch_1\n",
      "[2023-08-03 16:16:38,100 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2023-08-03 16:16:38,102 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [35/102] ETA 0:00:00.055733 loss=3.045 weight=2.145 batch={'x': tensor([11, 31]), 'y': tensor([22, 62])}\n",
      "[2023-08-03 16:16:38,107 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [40/102] ETA 0:00:00.052207 loss=0.360 weight=1.990 batch={'x': tensor([55, 17]), 'y': tensor([110,  34])}\n",
      "[2023-08-03 16:16:38,110 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [45/102] ETA 0:00:00.046839 loss=3.382 weight=2.082 batch={'x': tensor([36, 46]), 'y': tensor([72, 92])}\n",
      "[2023-08-03 16:16:38,114 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [50/102] ETA 0:00:00.041849 loss=2.990 weight=1.942 batch={'x': tensor([54, 50]), 'y': tensor([108, 100])}\n",
      "[2023-08-03 16:16:38,117 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [55/102] ETA 0:00:00.037196 loss=2.281 weight=1.937 batch={'x': tensor([35, 38]), 'y': tensor([70, 76])}\n",
      "[2023-08-03 16:16:38,120 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [60/102] ETA 0:00:00.032888 loss=0.726 weight=2.017 batch={'x': tensor([19, 64]), 'y': tensor([ 38, 128])}\n",
      "[2023-08-03 16:16:38,125 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [65/102] ETA 0:00:00.029445 loss=4.900 weight=2.245 batch={'x': tensor([15, 25]), 'y': tensor([30, 50])}\n",
      "[2023-08-03 16:16:38,128 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7qhgjhft/custom_epoch_based_trainer/checkpoints/epoch_2\n",
      "[2023-08-03 16:16:38,131 45971:140704487245376][log.py:89 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2023-08-03 16:16:38,134 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [70/102] ETA 0:00:00.027722 loss=0.900 weight=1.962 batch={'x': tensor([28, 20]), 'y': tensor([56, 40])}\n",
      "[2023-08-03 16:16:38,137 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [75/102] ETA 0:00:00.023037 loss=0.675 weight=1.865 batch={'x': tensor([9, 1]), 'y': tensor([18,  2])}\n",
      "[2023-08-03 16:16:38,141 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [80/102] ETA 0:00:00.018669 loss=2.437 weight=2.162 batch={'x': tensor([ 3, 27]), 'y': tensor([ 6, 54])}\n",
      "[2023-08-03 16:16:38,146 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [85/102] ETA 0:00:00.014557 loss=1.989 weight=1.907 batch={'x': tensor([17, 26]), 'y': tensor([34, 52])}\n",
      "[2023-08-03 16:16:38,151 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [90/102] ETA 0:00:00.010367 loss=4.550 weight=2.100 batch={'x': tensor([42, 49]), 'y': tensor([84, 98])}\n",
      "[2023-08-03 16:16:38,155 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [95/102] ETA 0:00:00.006020 loss=3.000 weight=2.050 batch={'x': tensor([64, 56]), 'y': tensor([128, 112])}\n",
      "[2023-08-03 16:16:38,160 45971:140704487245376][log.py:83 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer after_run_iter] INFO: Iter [100/102] ETA 0:00:00.001731 loss=2.200 weight=1.945 batch={'x': tensor([62, 18]), 'y': tensor([124,  36])}\n",
      "[2023-08-03 16:16:38,163 45971:140704487245376][checkpoint.py:61 todd.CustomEpochBasedTrainer.custom_epoch_based_trainer _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp7qhgjhft/custom_epoch_based_trainer/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        strategy_load_model_from_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    runner: CustomEpochBasedTrainer = todd.RunnerRegistry.build(\n",
    "        strategy_load_model_from_demo, \n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.load_model_from(os.path.join(work_dirs, 'custom_epoch_based_trainer', 'checkpoints', 'epoch_2', 'model.pth'))\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
