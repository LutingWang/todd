{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd-ai 0.4.0\n",
      "Uninstalling todd-ai-0.4.0:\n",
      "  Successfully uninstalled todd-ai-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install --no-build-isolation --extra-index-url https://pypi.org/simple .. > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-23 18:26:49,338 35174:140704541179520][patches.py:9 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n",
      "/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import todd\n",
    "from todd.runners import Memo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.ModelRegistry.register_()\n",
    "class RunnerModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def _forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        runner: todd.runners.BaseRunner,\n",
    "        batch,\n",
    "        memo: Memo,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> Memo:\n",
    "        log: dict[str, Any] | None = memo.get(\"log\")\n",
    "        y = self._forward(batch[\"x\"])\n",
    "        loss = F.l1_loss(y, batch[\"y\"])\n",
    "        memo[\"loss\"] = loss\n",
    "        if log is not None:\n",
    "            log[\"batch\"] = str(batch)\n",
    "            log[\"weight\"] = f\"{self._weight.item():.3f}\"\n",
    "            log[\"loss\"] = f\"{loss:.3f}\"\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.DatasetRegistry.register_()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:26:50,988 35174:140704541179520][base.py:57 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpl9zy4w3o\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:26:51,346 35174:140704541179520][base.py:57 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:26:51,388 35174:140704541179520][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-23 18:26:51,392 35174:140704541179520][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-23 18:26:51,395 35174:140704541179520][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-23 18:26:51,398 35174:140704541179520][log.py:93 todd.Validator.validator after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpy_xrnx2v\u001b[0m\n",
      "└── \u001b[1;36mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[dict(type='LogCallback', interval=5)],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:26:51,739 35174:140704541179520][base.py:57 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:26:51,743 35174:140704541179520][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [1/8] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.000 loss=4.000\n",
      "[2024-02-23 18:26:51,744 35174:140704541179520][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [2/8] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.000 loss=13.000\n",
      "[2024-02-23 18:26:51,745 35174:140704541179520][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [3/8] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.000 loss=6.000\n",
      "[2024-02-23 18:26:51,748 35174:140704541179520][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [4/8] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.000 loss=16.000\n",
      "[2024-02-23 18:26:51,749 35174:140704541179520][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [5/8] batch={'x': tensor([7, 9]), 'y': tensor([14, 18])} weight=0.000 loss=16.000\n",
      "[2024-02-23 18:26:51,751 35174:140704541179520][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [6/8] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.000 loss=12.000\n",
      "[2024-02-23 18:26:51,753 35174:140704541179520][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [7/8] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.000 loss=7.000\n",
      "[2024-02-23 18:26:51,755 35174:140704541179520][log.py:93 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [8/8] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.000 loss=8.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"iter_based_trainer\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:26:51,767 35174:140704541179520][base.py:57 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:26:51,768 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-23 18:26:51,771 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [1/15] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.000 loss=13.000\n",
      "[2024-02-23 18:26:51,772 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [2/15] batch={'x': tensor([7, 3]), 'y': tensor([14,  6])} weight=0.000 loss=10.000\n",
      "[2024-02-23 18:26:51,774 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [3/15] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.000 loss=12.000\n",
      "[2024-02-23 18:26:51,775 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [4/15] batch={'x': tensor([8, 6]), 'y': tensor([16, 12])} weight=0.000 loss=14.000\n",
      "[2024-02-23 18:26:51,777 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [5/15] batch={'x': tensor([1, 5]), 'y': tensor([ 2, 10])} weight=0.000 loss=6.000\n",
      "[2024-02-23 18:26:51,777 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-23 18:26:51,779 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [6/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.000 loss=12.000\n",
      "[2024-02-23 18:26:51,780 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [7/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.000 loss=5.000\n",
      "[2024-02-23 18:26:51,782 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [8/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.000 loss=12.000\n",
      "[2024-02-23 18:26:51,783 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [9/15] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.000 loss=10.000\n",
      "[2024-02-23 18:26:51,785 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [10/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.000 loss=16.000\n",
      "[2024-02-23 18:26:51,786 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-23 18:26:51,788 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [11/15] batch={'x': tensor([10,  9]), 'y': tensor([20, 18])} weight=0.000 loss=19.000\n",
      "[2024-02-23 18:26:51,790 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [12/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.000 loss=12.000\n",
      "[2024-02-23 18:26:51,793 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [13/15] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.000 loss=12.000\n",
      "[2024-02-23 18:26:51,794 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 6]), 'y': tensor([ 6, 12])} weight=0.000 loss=9.000\n",
      "[2024-02-23 18:26:51,796 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [15/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.000 loss=3.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"epoch_based_trainer\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-23 18:26:51,926 35174:140704541179520][log.py:55 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.7.0\n",
      "todd_version: 0.4.0\n",
      "cuda_home: None\n",
      "git_commit_id: 8dccf62\n",
      "git_status: \n",
      "M pyproject.toml\n",
      " M todd/runners/base.py\n",
      " M todd/runners/epoch_based_trainer.py\n",
      " M todd/runners/iter_based_trainer.py\n",
      " M todd/runners/trainer.py\n",
      " M todd/runners/validator.py\n",
      " M todd/utils/torch.py\n",
      "\u001b[2m[2024-02-23 18:26:51,927 35174:140704541179520][base.py:57 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:26:51,931 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-23 18:26:51,934 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-23 18:26:51,937 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-23 18:26:51,939 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:26:51,979 35174:140704541179520][base.py:57 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:26:51,987 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-23 18:26:51,991 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-23 18:26:52,005 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-23 18:26:52,032 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe_f9kq5u\u001b[0m\n",
      "└── \u001b[1;36mlog_callback\u001b[0m\n",
      "    └── 2024-02-23T18-26-51_978897-08-00.log\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "[2024-02-23 18:26:51,979 35174:140704541179520][base.py:57 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2024-02-23 18:26:51,987 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-23 18:26:51,991 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-23 18:26:52,005 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-23 18:26:52,032 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='log_callback',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type='LogCallback',\n",
    "            interval=5,\n",
    "            with_file_handler=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/log_callback/*.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:26:52,618 35174:140704541179520][base.py:57 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:26:53,137 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:01 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-23 18:26:53,652 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:01 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-23 18:26:54,162 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-23 18:26:54,678 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:26:54,690 35174:140704541179520][base.py:57 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:26:56,210 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:04 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-23 18:27:00,228 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:05 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-23 18:27:05,240 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:03 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-23 18:27:10,261 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"EMA_ETA\", ema=dict(decay=0.2)),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1 * min(10, runner.iter_))\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-23 18:27:10,384 35174:140704541179520][log.py:55 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.7.0\n",
      "todd_version: 0.4.0\n",
      "cuda_home: None\n",
      "git_commit_id: 8dccf62\n",
      "git_status: \n",
      "M pyproject.toml\n",
      " M todd/runners/base.py\n",
      " M todd/runners/epoch_based_trainer.py\n",
      " M todd/runners/iter_based_trainer.py\n",
      " M todd/runners/trainer.py\n",
      " M todd/runners/validator.py\n",
      " M todd/utils/torch.py\n",
      "\u001b[2m[2024-02-23 18:27:10,386 35174:140704541179520][base.py:57 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:10,390 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:00 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-02-23 18:27:10,393 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:00 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-02-23 18:27:10,395 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-02-23 18:27:10,398 35174:140704541179520][log.py:93 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "            with_file_handler=True,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-23 18:27:10,464 35174:140704541179520][git.py:41 todd.Validator.git_callback init] INFO: Saving git diff to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpk3uvcez8/git_callback/git_diff_2024-02-23T18-27-10_464496-08-00.log\n",
      "\u001b[2m[2024-02-23 18:27:10,467 35174:140704541179520][base.py:57 todd.Validator.git_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff --git a/pyproject.toml b/pyproject.toml\n",
      "index aa38442..487cb54 100644\n",
      "--- a/pyproject.toml\n",
      "+++ b/pyproject.toml\n",
      "@@ -133,6 +133,7 @@ module = [\n",
      "     'ipdb.*',\n",
      "     'mmcv.*',\n",
      "     'pptx.*',\n",
      "+    'setuptools.*',\n",
      "     'torchvision.*',\n",
      "     'yapf.*',\n",
      " ]\n",
      "diff --git a/todd/runners/base.py b/todd/runners/base.py\n",
      "index fda811f..e16bbfd 100644\n",
      "--- a/todd/runners/base.py\n",
      "+++ b/todd/runners/base.py\n",
      "@@ -2,6 +2,7 @@ __all__ = [\n",
      "     'BaseRunner',\n",
      " ]\n",
      " \n",
      "+from abc import abstractmethod\n",
      " import contextlib\n",
      " import getpass\n",
      " import logging\n",
      "@@ -105,8 +106,9 @@ class BaseRunner(StateDictMixin, Generic[T]):\n",
      "         return self._logger\n",
      " \n",
      "     @property\n",
      "+    @abstractmethod\n",
      "     def iters(self) -> int:\n",
      "-        return len(self._dataloader)\n",
      "+        pass\n",
      " \n",
      "     def _build_strategy(\n",
      "         self,\n",
      "@@ -255,7 +257,7 @@ class BaseRunner(StateDictMixin, Generic[T]):\n",
      "         return memo\n",
      " \n",
      "     def _setup(self) -> Memo:\n",
      "-        return dict(dataloader=iter(self._dataloader))\n",
      "+        return dict()\n",
      " \n",
      "     def _teardown(self, memo: Memo) -> None:\n",
      "         pass\n",
      "diff --git a/todd/runners/epoch_based_trainer.py b/todd/runners/epoch_based_trainer.py\n",
      "index 73ff6b7..8c2a517 100644\n",
      "--- a/todd/runners/epoch_based_trainer.py\n",
      "+++ b/todd/runners/epoch_based_trainer.py\n",
      "@@ -9,6 +9,8 @@ from typing import TypeVar\n",
      " \n",
      " from torch import nn\n",
      " \n",
      "+from ..utils import set_epoch\n",
      "+\n",
      " from ..base import RunnerRegistry\n",
      " from .trainer import Trainer\n",
      " from .types import Memo\n",
      "@@ -23,17 +25,9 @@ class EpochBasedTrainer(Trainer):\n",
      "         super().__init__(*args, **kwargs)\n",
      "         self._epochs = epochs\n",
      " \n",
      "-    @property\n",
      "-    def epoch(self) -> int:\n",
      "-        return self._iter // super().iters\n",
      "-\n",
      "-    @property\n",
      "-    def inner_iter(self) -> int:\n",
      "-        return self._iter % super().iters\n",
      "-\n",
      "     @property\n",
      "     def iters(self) -> int:\n",
      "-        return super().iters * self._epochs\n",
      "+        return self.iters_per_epoch * self._epochs\n",
      " \n",
      "     @property\n",
      "     def epochs(self) -> int:\n",
      "@@ -43,22 +37,13 @@ class EpochBasedTrainer(Trainer):\n",
      "         return super()._run(epoch_memo)\n",
      " \n",
      "     def _setup_epoch(self, memo: Memo) -> Memo:\n",
      "-        samplers = [\n",
      "-            self._dataloader.sampler,\n",
      "-            self._dataloader.batch_sampler,\n",
      "-            getattr(self._dataloader.batch_sampler, 'sampler', None),\n",
      "-        ]\n",
      "-        for sampler in samplers:\n",
      "-            if (set_epoch := getattr(sampler, 'set_epoch', None)) is not None:\n",
      "-                set_epoch(self.epoch)\n",
      "         epoch_memo = super()._setup()\n",
      "-        dataloader = epoch_memo['dataloader']\n",
      "-        dataloader = itertools.islice(\n",
      "-            dataloader,\n",
      "-            super().iters - self.inner_iter,\n",
      "-        )\n",
      "+        set_epoch(self._dataloader, self.epoch)\n",
      "         epoch_memo.update(\n",
      "-            dataloader=dataloader,\n",
      "+            dataloader=(\n",
      "+                itertools.islice(self._dataloader, self.inner_iter, None)\n",
      "+                if self.inner_iter > 0 else self._dataloader\n",
      "+            ),\n",
      "             epoch=defaultdict(list),\n",
      "         )\n",
      "         return epoch_memo\n",
      "diff --git a/todd/runners/iter_based_trainer.py b/todd/runners/iter_based_trainer.py\n",
      "index bc507f1..2f69a9d 100644\n",
      "--- a/todd/runners/iter_based_trainer.py\n",
      "+++ b/todd/runners/iter_based_trainer.py\n",
      "@@ -3,10 +3,12 @@ __all__ = [\n",
      " ]\n",
      " \n",
      " import itertools\n",
      "-from typing import TypeVar\n",
      "+from typing import Any, Generator, TypeVar\n",
      " \n",
      " from torch import nn\n",
      " \n",
      "+from ..utils.torch import set_epoch\n",
      "+\n",
      " from ..base import RunnerRegistry\n",
      " from .trainer import Trainer\n",
      " from .types import Memo\n",
      "@@ -26,10 +28,23 @@ class IterBasedTrainer(Trainer):\n",
      "     def iters(self) -> int:\n",
      "         return self._iters\n",
      " \n",
      "+    def _iterate_dataloader(self) -> Generator[Any, None, None]:\n",
      "+        if self.inner_iter > 0:\n",
      "+            set_epoch(self._dataloader, self.epoch)\n",
      "+            yield from itertools.islice(\n",
      "+                self._dataloader,\n",
      "+                self.inner_iter,\n",
      "+                self.iters - self.iters_per_epoch * self.epoch,\n",
      "+            )\n",
      "+        while self._iter < self.iters:\n",
      "+            assert self.inner_iter == 0\n",
      "+            set_epoch(self._dataloader, self.epoch)\n",
      "+            yield from itertools.islice(\n",
      "+                self._dataloader,\n",
      "+                self.iters - self._iter,\n",
      "+            )\n",
      "+\n",
      "     def _setup(self) -> Memo:\n",
      "         memo = super()._setup()\n",
      "-        dataloader = memo['dataloader']\n",
      "-        dataloader = itertools.cycle(dataloader)\n",
      "-        dataloader = itertools.islice(dataloader, self._iters - self._iter)\n",
      "-        memo['dataloader'] = dataloader\n",
      "+        memo['dataloader'] = self._iterate_dataloader()\n",
      "         return memo\n",
      "diff --git a/todd/runners/trainer.py b/todd/runners/trainer.py\n",
      "index 40ecbe6..449c8b7 100644\n",
      "--- a/todd/runners/trainer.py\n",
      "+++ b/todd/runners/trainer.py\n",
      "@@ -2,6 +2,7 @@ __all__ = [\n",
      "     'Trainer',\n",
      " ]\n",
      " \n",
      "+from abc import ABC\n",
      " from typing import Any, Mapping, TypeVar\n",
      " \n",
      " import torch\n",
      "@@ -15,7 +16,19 @@ T = TypeVar('T', bound=nn.Module)\n",
      " \n",
      " \n",
      " @RunnerRegistry.register_()\n",
      "-class Trainer(BaseRunner[T]):\n",
      "+class Trainer(BaseRunner[T], ABC):\n",
      "+\n",
      "+    @property\n",
      "+    def iters_per_epoch(self) -> int:\n",
      "+        return len(self._dataloader)\n",
      "+\n",
      "+    @property\n",
      "+    def inner_iter(self) -> int:\n",
      "+        return self._iter % self.iters_per_epoch\n",
      "+\n",
      "+    @property\n",
      "+    def epoch(self) -> int:\n",
      "+        return self._iter // self.iters_per_epoch\n",
      " \n",
      "     @property\n",
      "     def optimizer(self) -> torch.optim.Optimizer:\n",
      "diff --git a/todd/runners/validator.py b/todd/runners/validator.py\n",
      "index a70132a..c55f18b 100644\n",
      "--- a/todd/runners/validator.py\n",
      "+++ b/todd/runners/validator.py\n",
      "@@ -16,9 +16,15 @@ T = TypeVar('T', bound=nn.Module)\n",
      " @RunnerRegistry.register_()\n",
      " class Validator(BaseRunner[T]):\n",
      " \n",
      "+    @property\n",
      "+    def iters(self) -> int:\n",
      "+        return len(self._dataloader)\n",
      "+\n",
      "     def _setup(self) -> Memo:\n",
      "         self._model.eval()\n",
      "-        return super()._setup()\n",
      "+        memo = super()._setup()\n",
      "+        memo['dataloader'] = self._dataloader\n",
      "+        return memo\n",
      " \n",
      "     @torch.no_grad()\n",
      "     def run(self) -> Memo:\n",
      "diff --git a/todd/utils/torch.py b/todd/utils/torch.py\n",
      "index 05ec37b..f574616 100644\n",
      "--- a/todd/utils/torch.py\n",
      "+++ b/todd/utils/torch.py\n",
      "@@ -5,6 +5,7 @@ __all__ = [\n",
      "     'all_gather',\n",
      "     'all_gather_',\n",
      "     'all_sync',\n",
      "+    'set_epoch',\n",
      "     'Shape',\n",
      "     'ModuleList',\n",
      "     'ModuleDict',\n",
      "@@ -20,6 +21,7 @@ from typing import TYPE_CHECKING\n",
      " import torch\n",
      " import torch.distributed as dist\n",
      " from torch import nn\n",
      "+from torch.utils.data import DataLoader\n",
      " \n",
      " \n",
      " def get_rank(*args, **kwargs) -> int:\n",
      "@@ -101,6 +103,18 @@ def all_sync(x: torch.Tensor) -> bool:\n",
      "     return torch.allclose(x, x_prime)\n",
      " \n",
      " \n",
      "+def set_epoch(dataloader: DataLoader, epoch: int) -> None:\n",
      "+    samplers = [\n",
      "+        dataloader.sampler,\n",
      "+        dataloader.batch_sampler,\n",
      "+        getattr(dataloader.batch_sampler, 'sampler', None),\n",
      "+    ]\n",
      "+    for sampler in samplers:\n",
      "+        set_epoch_ = getattr(sampler, 'set_epoch', None)\n",
      "+        if set_epoch_ is not None:\n",
      "+            set_epoch_(epoch)\n",
      "+\n",
      "+\n",
      " class Shape:\n",
      " \n",
      "     @classmethod\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"git_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(type=\"GitCallback\", diff='HEAD -- \":(exclude)*.ipynb\"'),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/git_callback/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:27:10,779 35174:140704541179520][base.py:57 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:10,784 35174:140704541179520][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.000 loss=13.000\n",
      "[2024-02-23 18:27:10,786 35174:140704541179520][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.032 loss=9.838\n",
      "[2024-02-23 18:27:10,787 35174:140704541179520][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.057 loss=8.741\n",
      "[2024-02-23 18:27:10,789 35174:140704541179520][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.080 loss=11.520\n",
      "[2024-02-23 18:27:10,791 35174:140704541179520][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.110 loss=10.395\n",
      "[2024-02-23 18:27:10,794 35174:140704541179520][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.138 loss=16.763\n",
      "[2024-02-23 18:27:10,796 35174:140704541179520][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([3, 5]), 'y': tensor([ 6, 10])} weight=0.183 loss=7.270\n",
      "[2024-02-23 18:27:10,798 35174:140704541179520][log.py:93 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.203 loss=2.696\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"optimize_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:27:10,810 35174:140704541179520][base.py:57 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:10,812 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.000 loss=9.000 lr=['1.667e-03']\n",
      "[2024-02-23 18:27:10,814 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.008 loss=10.959 lr=['2.333e-03']\n",
      "[2024-02-23 18:27:10,815 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.020 loss=11.878 lr=['3.000e-03']\n",
      "[2024-02-23 18:27:10,817 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.038 loss=11.770 lr=['3.667e-03']\n",
      "[2024-02-23 18:27:10,819 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.060 loss=10.668 lr=['4.333e-03']\n",
      "[2024-02-23 18:27:10,820 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([10,  6]), 'y': tensor([20, 12])} weight=0.084 loss=15.327 lr=['5.000e-03']\n",
      "[2024-02-23 18:27:10,822 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.124 loss=10.317 lr=['5.000e-03']\n",
      "[2024-02-23 18:27:10,823 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.152 loss=9.242 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=5),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:27:10,836 35174:140704541179520][base.py:57 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:10,836 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "[2024-02-23 18:27:10,839 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.000 loss=3.000 lr=['1.667e-03']\n",
      "[2024-02-23 18:27:10,841 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/10] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.002 loss=6.991 lr=['1.667e-03']\n",
      "[2024-02-23 18:27:10,842 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [2/5]\n",
      "[2024-02-23 18:27:10,844 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/10] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.008 loss=4.979 lr=['2.778e-03']\n",
      "[2024-02-23 18:27:10,846 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/10] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.015 loss=4.962 lr=['2.778e-03']\n",
      "[2024-02-23 18:27:10,847 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [3/5]\n",
      "[2024-02-23 18:27:10,849 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/10] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.022 loss=4.944 lr=['3.889e-03']\n",
      "[2024-02-23 18:27:10,851 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/10] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.032 loss=4.920 lr=['3.889e-03']\n",
      "[2024-02-23 18:27:10,852 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [4/5]\n",
      "[2024-02-23 18:27:10,854 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.042 loss=2.938 lr=['5.000e-03']\n",
      "[2024-02-23 18:27:10,856 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/10] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.049 loss=6.828 lr=['5.000e-03']\n",
      "[2024-02-23 18:27:10,857 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [5/5]\n",
      "[2024-02-23 18:27:10,859 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [9/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.067 loss=2.900 lr=['5.000e-03']\n",
      "[2024-02-23 18:27:10,861 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [10/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.074 loss=6.740 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=4),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=3),\n",
    "            by_epoch=True,\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=5,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-23 18:27:10,874 35174:140704541179520][lr.py:93 todd.IterBasedTrainer.lr_scale_callback _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-02-23 18:27:10,875 35174:140704541179520][base.py:57 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:10,877 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.000 loss=10.000\n",
      "[2024-02-23 18:27:10,879 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.050 loss=12.675\n",
      "[2024-02-23 18:27:10,881 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.115 loss=6.598\n",
      "[2024-02-23 18:27:10,882 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.150 loss=11.100\n",
      "[2024-02-23 18:27:10,884 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.210 loss=11.635\n",
      "[2024-02-23 18:27:10,885 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.275 loss=6.900\n",
      "[2024-02-23 18:27:10,887 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([8, 3]), 'y': tensor([16,  6])} weight=0.315 loss=9.267\n",
      "[2024-02-23 18:27:10,888 35174:140704541179520][log.py:93 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.370 loss=12.225\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_scale_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScaleCallback\",\n",
    "            lr_scaler=dict(base_batch_size=1),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:27:10,910 35174:140704541179520][base.py:57 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:10,912 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([4, 1]), 'y': tensor([8, 2])} weight=0.000 loss=5.000\n",
      "[2024-02-23 18:27:10,913 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_1\n",
      "[2024-02-23 18:27:10,917 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.012 loss=12.919\n",
      "[2024-02-23 18:27:10,918 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-02-23 18:27:10,922 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([ 9, 10]), 'y': tensor([18, 20])} weight=0.045 loss=18.572\n",
      "[2024-02-23 18:27:10,923 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_3\n",
      "[2024-02-23 18:27:10,928 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.093 loss=12.399\n",
      "[2024-02-23 18:27:10,930 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-02-23 18:27:10,936 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.125 loss=4.688\n",
      "[2024-02-23 18:27:10,937 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-02-23 18:27:10,941 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.138 loss=8.381\n",
      "[2024-02-23 18:27:10,942 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-23 18:27:10,946 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.160 loss=10.120\n",
      "[2024-02-23 18:27:10,947 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-02-23 18:27:10,951 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.188 loss=10.875\n",
      "[2024-02-23 18:27:10,952 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_5\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_7\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_8\u001b[0m\n",
      "\n",
      "12 directories, 40 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-23 18:27:11,413 35174:140704541179520][checkpoint.py:54 todd.IterBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-02-23 18:27:11,420 35174:140704541179520][base.py:65 todd.IterBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-23 18:27:11,421 35174:140704541179520][base.py:57 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:11,424 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.138 loss=9.312\n",
      "[2024-02-23 18:27:11,425 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-23 18:27:11,429 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.162 loss=7.350\n",
      "[2024-02-23 18:27:11,430 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-02-23 18:27:11,434 35174:140704541179520][log.py:93 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.182 loss=10.905\n",
      "[2024-02-23 18:27:11,435 35174:140704541179520][checkpoint.py:80 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp5g2sg2lp/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {0: {'momentum_buffer': None}}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 5}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.1375))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_5 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_5'\n",
    "    for f in iter_5.glob('*.pth'):\n",
    "        print(f'{f.name}:')\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()\n",
    "\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_5),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:27:11,475 35174:140704541179520][base.py:57 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:11,476 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-23 18:27:11,478 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.000 loss=11.000\n",
      "[2024-02-23 18:27:11,480 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.027 loss=7.890\n",
      "[2024-02-23 18:27:11,481 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-02-23 18:27:11,484 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.047 loss=10.739\n",
      "[2024-02-23 18:27:11,486 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([9, 2]), 'y': tensor([18,  4])} weight=0.075 loss=10.588\n",
      "[2024-02-23 18:27:11,487 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-02-23 18:27:11,491 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.103 loss=13.283\n",
      "[2024-02-23 18:27:11,492 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-23 18:27:11,494 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.138 loss=8.381\n",
      "[2024-02-23 18:27:11,495 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-02-23 18:27:11,499 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.160 loss=11.960\n",
      "[2024-02-23 18:27:11,500 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.192 loss=9.038\n",
      "[2024-02-23 18:27:11,502 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-02-23 18:27:11,506 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.218 loss=11.586\n",
      "[2024-02-23 18:27:11,508 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.250 loss=8.750\n",
      "[2024-02-23 18:27:11,509 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-23 18:27:11,512 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-23 18:27:11,514 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.275 loss=7.763\n",
      "[2024-02-23 18:27:11,516 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.298 loss=6.810\n",
      "[2024-02-23 18:27:11,516 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-23 18:27:11,521 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.318 loss=10.095\n",
      "[2024-02-23 18:27:11,522 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.348 loss=11.567\n",
      "[2024-02-23 18:27:11,523 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-23 18:27:11,527 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.383 loss=9.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36miter_10\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_12\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_14\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_4\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_6\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36miter_8\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36miter_14\u001b[0m\n",
      "\n",
      "11 directories, 35 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-23 18:27:11,956 35174:140704541179520][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-02-23 18:27:11,959 35174:140704541179520][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-23 18:27:11,960 35174:140704541179520][base.py:57 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:11,961 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-23 18:27:11,964 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([6, 4]), 'y': tensor([12,  8])} weight=0.218 loss=8.913\n",
      "[2024-02-23 18:27:11,966 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.243 loss=9.666\n",
      "[2024-02-23 18:27:11,967 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-23 18:27:11,970 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-23 18:27:11,972 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.270 loss=14.705\n",
      "[2024-02-23 18:27:11,973 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.312 loss=11.812\n",
      "[2024-02-23 18:27:11,974 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-23 18:27:11,978 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.347 loss=8.262\n",
      "[2024-02-23 18:27:11,980 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.373 loss=4.883\n",
      "[2024-02-23 18:27:11,982 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-23 18:27:11,987 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.387 loss=6.450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-23 18:27:12,442 35174:140704541179520][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-02-23 18:27:12,445 35174:140704541179520][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-23 18:27:12,446 35174:140704541179520][base.py:57 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:12,447 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-23 18:27:12,450 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.270 loss=6.920\n",
      "[2024-02-23 18:27:12,452 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.290 loss=10.260\n",
      "[2024-02-23 18:27:12,453 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-02-23 18:27:12,463 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([10,  4]), 'y': tensor([20,  8])} weight=0.320 loss=11.760\n",
      "[2024-02-23 18:27:12,465 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.355 loss=9.047\n",
      "[2024-02-23 18:27:12,466 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp46q6bkgj/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-02-23 18:27:12,470 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.383 loss=8.087\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=2),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_8 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_8'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_8),\n",
    "        )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    iter_10 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_10'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_10),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:27:12,498 35174:140704541179520][base.py:57 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:12,499 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-23 18:27:12,501 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.000 loss=4.000\n",
      "[2024-02-23 18:27:12,503 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.010 loss=12.935\n",
      "[2024-02-23 18:27:12,505 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([2, 7]), 'y': tensor([ 4, 14])} weight=0.042 loss=8.809\n",
      "[2024-02-23 18:27:12,507 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.065 loss=15.480\n",
      "[2024-02-23 18:27:12,509 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.105 loss=12.318\n",
      "[2024-02-23 18:27:12,510 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8jm3x0vu/checkpoint_callback/checkpoints/epoch_1\n",
      "[2024-02-23 18:27:12,513 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-23 18:27:12,515 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([1, 5]), 'y': tensor([ 2, 10])} weight=0.137 loss=5.588\n",
      "[2024-02-23 18:27:12,517 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([10,  2]), 'y': tensor([20,  4])} weight=0.152 loss=11.085\n",
      "[2024-02-23 18:27:12,519 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.182 loss=10.905\n",
      "[2024-02-23 18:27:12,521 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.212 loss=8.938\n",
      "[2024-02-23 18:27:12,523 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.237 loss=13.219\n",
      "[2024-02-23 18:27:12,525 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8jm3x0vu/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-02-23 18:27:12,528 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-23 18:27:12,530 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.275 loss=9.488\n",
      "[2024-02-23 18:27:12,532 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.302 loss=8.488\n",
      "[2024-02-23 18:27:12,534 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.327 loss=14.216\n",
      "[2024-02-23 18:27:12,535 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([7, 5]), 'y': tensor([14, 10])} weight=0.370 loss=9.780\n",
      "[2024-02-23 18:27:12,537 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.400 loss=4.000\n",
      "[2024-02-23 18:27:12,538 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8jm3x0vu/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8jm3x0vu\u001b[0m\n",
      "└── \u001b[1;36mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[1;36mcheckpoints\u001b[0m\n",
      "        ├── \u001b[1;36mepoch_1\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_2\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        ├── \u001b[1;36mepoch_3\u001b[0m\n",
      "        │   ├── callbacks.pth\n",
      "        │   ├── meta.pth\n",
      "        │   ├── model.pth\n",
      "        │   ├── optim.pth\n",
      "        │   └── strategy.pth\n",
      "        └── \u001b[35mlatest\u001b[0m -> \u001b[1;36mepoch_3\u001b[0m\n",
      "\n",
      "7 directories, 15 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-23 18:27:12,968 35174:140704541179520][checkpoint.py:54 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8jm3x0vu/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-02-23 18:27:12,974 35174:140704541179520][base.py:65 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-02-23 18:27:12,976 35174:140704541179520][base.py:57 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:12,978 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-23 18:27:12,982 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([1, 8]), 'y': tensor([ 2, 16])} weight=0.275 loss=7.763\n",
      "[2024-02-23 18:27:12,984 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([2, 7]), 'y': tensor([ 4, 14])} weight=0.297 loss=7.661\n",
      "[2024-02-23 18:27:12,986 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.320 loss=10.080\n",
      "[2024-02-23 18:27:12,989 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.350 loss=9.075\n",
      "[2024-02-23 18:27:12,991 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([10,  4]), 'y': tensor([20,  8])} weight=0.377 loss=11.358\n",
      "[2024-02-23 18:27:12,992 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp8jm3x0vu/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'epoch_2'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(epoch_2),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.RunnerRegistry.register_()\n",
    "class FaultyValidator(todd.runners.Validator):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError(\"faulty runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:27:13,032 35174:140704541179520][base.py:57 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-02-23 18:27:13,035 35174:140704541179520][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x14c1dc050>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 255, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_35174/1715875531.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2024-02-23 18:27:13,032 35174:140704541179520][base.py:57 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\n",
      "[2024-02-23 18:27:13,035 35174:140704541179520][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x14c1dc050>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 255, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_35174/1715875531.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='FaultyValidator',\n",
    "    name='monitor_callback',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(type='MonitorCallback'),\n",
    "        dict(type='LogCallback', interval=5, with_file_handler=True),\n",
    "    ],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/monitor_callback/*.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:27:13,341 35174:140704541179520][base.py:57 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:13,342 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-23 18:27:13,346 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.000 loss=4.000\n",
      "[2024-02-23 18:27:13,347 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([6, 2]), 'y': tensor([12,  4])} weight=0.010 loss=7.960\n",
      "[2024-02-23 18:27:13,349 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.030 loss=17.730\n",
      "[2024-02-23 18:27:13,351 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.075 loss=11.550\n",
      "[2024-02-23 18:27:13,367 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.105 loss=12.318\n",
      "[2024-02-23 18:27:13,385 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp91d3imrz/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-02-23 18:27:13,414 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-23 18:27:13,419 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.137 loss=6.519\n",
      "[2024-02-23 18:27:13,420 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([6, 8]), 'y': tensor([12, 16])} weight=0.155 loss=12.915\n",
      "[2024-02-23 18:27:13,422 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.190 loss=10.860\n",
      "[2024-02-23 18:27:13,424 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.220 loss=7.120\n",
      "[2024-02-23 18:27:13,426 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([10,  4]), 'y': tensor([20,  8])} weight=0.240 loss=12.320\n",
      "[2024-02-23 18:27:13,427 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp91d3imrz/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-02-23 18:27:13,430 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-23 18:27:13,432 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.275 loss=11.212\n",
      "[2024-02-23 18:27:13,434 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.307 loss=9.309\n",
      "[2024-02-23 18:27:13,436 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([2, 5]), 'y': tensor([ 4, 10])} weight=0.335 loss=5.827\n",
      "[2024-02-23 18:27:13,438 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.352 loss=12.356\n",
      "[2024-02-23 18:27:13,440 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([8, 1]), 'y': tensor([16,  2])} weight=0.390 loss=7.245\n",
      "[2024-02-23 18:27:13,441 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp91d3imrz/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2024-02-23 18:27:13,865 35174:140704541179520][base.py:57 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-02-23 18:27:13,866 35174:140704541179520][base.py:80 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp91d3imrz/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-02-23 18:27:13,868 35174:140704541179520][base.py:65 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "[2024-02-23 18:27:13,869 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-02-23 18:27:13,873 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.275 loss=7.762\n",
      "[2024-02-23 18:27:13,875 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.297 loss=13.620\n",
      "[2024-02-23 18:27:13,877 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.337 loss=4.156\n",
      "[2024-02-23 18:27:13,881 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.350 loss=14.025\n",
      "[2024-02-23 18:27:13,883 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.392 loss=6.430\n",
      "[2024-02-23 18:27:13,884 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp91d3imrz/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-02-23 18:27:13,888 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-02-23 18:27:13,890 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.412 loss=8.731\n",
      "[2024-02-23 18:27:13,891 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.440 loss=7.020\n",
      "[2024-02-23 18:27:13,894 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([3, 9]), 'y': tensor([ 6, 18])} weight=0.462 loss=9.225\n",
      "[2024-02-23 18:27:13,896 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.492 loss=9.799\n",
      "[2024-02-23 18:27:13,898 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.525 loss=7.375\n",
      "[2024-02-23 18:27:13,899 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp91d3imrz/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-02-23 18:27:13,902 35174:140704541179520][log.py:99 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-02-23 18:27:13,905 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.550 loss=9.425\n",
      "[2024-02-23 18:27:13,907 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.582 loss=8.505\n",
      "[2024-02-23 18:27:13,910 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([10,  4]), 'y': tensor([20,  8])} weight=0.612 loss=9.712\n",
      "[2024-02-23 18:27:13,912 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([1, 6]), 'y': tensor([ 2, 12])} weight=0.647 loss=4.734\n",
      "[2024-02-23 18:27:13,914 35174:140704541179520][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 7]), 'y': tensor([ 4, 14])} weight=0.665 loss=6.008\n",
      "[2024-02-23 18:27:13,915 35174:140704541179520][checkpoint.py:80 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp91d3imrz/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"strategy_load_model_from\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = (pathlib.Path(work_dirs) / 'strategy_load_model_from' / 'checkpoints' / 'epoch_2' / 'model.pth')\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.strategy.load_model_from(epoch_2)\n",
    "    runner.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
