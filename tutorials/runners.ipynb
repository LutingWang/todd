{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Models and Datasets as Usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-25 00:19:08,089 4861:4384228736][patches.py:82 todd <module>] INFO: `ipdb` is installed. Using it for debugging.\n"
     ]
    }
   ],
   "source": [
    "import todd\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models should be built by users.\n",
    "The same model can be used by multiple runners, such as a trainer and a validator, simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class Model(todd.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(2.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> float:\n",
    "        return self._weight.item()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to models, datasets are built inside runners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> int:\n",
    "        return self._data[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Mixin for All Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerMixin(todd.utils.BaseRunner):\n",
    "\n",
    "    def _build_dataloader(\n",
    "        self,\n",
    "        config: todd.Config,\n",
    "    ) -> torch.utils.data.DataLoader:\n",
    "        dataset = Dataset(**config.pop('dataset'))\n",
    "        return torch.utils.data.DataLoader(dataset, **config)\n",
    "\n",
    "    def _run_iter(self, batch, memo: todd.utils.Memo) -> torch.Tensor:\n",
    "        y: torch.Tensor = self._model(batch)\n",
    "        loss = y.sum().abs()\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DRY_RUN` is turned on by default when CUDA devices are not available.\n",
    "To override this setting, manually set `DRY_RUN` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import cast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and register the validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.utils.RunnerRegistry.register()\n",
    "class CustomValidator(RunnerMixin, todd.utils.Validator):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the validator config. \n",
    "`config` will be reused by trainers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = todd.Config(\n",
    "    model=model,\n",
    "    log=dict(interval=5),\n",
    "    load_state_dict=dict(model=dict(strict=False)),\n",
    "    state_dict=dict(model=dict()),\n",
    ")\n",
    "validator = todd.Config(\n",
    "    type='CustomValidator',\n",
    "    dataloader=dict(batch_size=1, dataset=dict(n=20)),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and run the validator.\n",
    "Logs will be saved to the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-25 00:19:11,334 4861:4384228736][runners.py:71 todd.CustomValidator __init__] DEBUG: Runner initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-25 00:19:11,337 4861:4384228736][runners.py:177 todd.CustomValidator _run] INFO: Iter [5/20] Loss 10.000\n",
      "[2023-01-25 00:19:11,338 4861:4384228736][runners.py:177 todd.CustomValidator _run] INFO: Iter [10/20] Loss 20.000\n",
      "[2023-01-25 00:19:11,341 4861:4384228736][runners.py:177 todd.CustomValidator _run] INFO: Iter [15/20] Loss 30.000\n",
      "[2023-01-25 00:19:11,342 4861:4384228736][runners.py:177 todd.CustomValidator _run] INFO: Iter [20/20] Loss 40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023-01-25T00:19:11.333057+08:00.log']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    validator.name = work_dir\n",
    "    runner = todd.utils.RunnerRegistry.build(validator, config)\n",
    "    cast(CustomValidator, runner).run()\n",
    "    print(os.listdir(work_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = todd.Config(\n",
    "    dataloader=dict(batch_size=2, dataset=dict(n=67)),\n",
    "    optimizer=dict(type='SGD', lr=0.01),\n",
    "    load_state_dict=dict(optimizer=dict()),\n",
    "    state_dict=dict(optimizer=dict()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.utils.RunnerRegistry.register()\n",
    "class CustomIterBasedTrainer(RunnerMixin, todd.utils.IterBasedTrainer):\n",
    "\n",
    "    def _before_run_iter_log(\n",
    "        self,\n",
    "        batch,\n",
    "        memo: todd.utils.Memo,\n",
    "    ) -> str | None:\n",
    "        info = super()._before_run_iter_log(batch, memo)\n",
    "        info = \"\" if info is None else f\" {info} \"\n",
    "        model: Model = self.model\n",
    "        info += f\"Weight {model.weight:.3f} Batch {batch}\"\n",
    "        return info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `_before_run_iter_log` returns `None`, meaning that no message will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-25 00:19:11,764 4861:4384228736][runners.py:71 todd.CustomIterBasedTrainer __init__] DEBUG: Runner initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-25 00:19:11,774 4861:4384228736][runners.py:166 todd.CustomIterBasedTrainer _run] INFO: Weight 1.640 Batch tensor([ 9, 10])\n",
      "[2023-01-25 00:19:11,776 4861:4384228736][runners.py:177 todd.CustomIterBasedTrainer _run] INFO: Iter [5/53] Loss 31.160\n",
      "[2023-01-25 00:19:11,779 4861:4384228736][runners.py:166 todd.CustomIterBasedTrainer _run] INFO: Weight 0.290 Batch tensor([19, 20])\n",
      "[2023-01-25 00:19:11,780 4861:4384228736][runners.py:177 todd.CustomIterBasedTrainer _run] INFO: Iter [10/53] Loss 11.310\n",
      "[2023-01-25 00:19:11,785 4861:4384228736][runners.py:166 todd.CustomIterBasedTrainer _run] INFO: Weight -0.180 Batch tensor([29, 30])\n",
      "[2023-01-25 00:19:11,787 4861:4384228736][runners.py:177 todd.CustomIterBasedTrainer _run] INFO: Iter [15/53] Loss 10.620\n",
      "[2023-01-25 00:19:11,789 4861:4384228736][runners.py:166 todd.CustomIterBasedTrainer _run] INFO: Weight 0.490 Batch tensor([39, 40])\n",
      "[2023-01-25 00:19:11,807 4861:4384228736][runners.py:177 todd.CustomIterBasedTrainer _run] INFO: Iter [20/53] Loss 38.710\n",
      "[2023-01-25 00:19:11,809 4861:4384228736][runners.py:275 todd.CustomIterBasedTrainer validate] INFO: Skipping validation since validator is undefined\n",
      "[2023-01-25 00:19:11,810 4861:4384228736][runners.py:201 todd.CustomIterBasedTrainer write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpy03p_0y3/iter_20.pth\n",
      "[2023-01-25 00:19:11,813 4861:4384228736][runners.py:166 todd.CustomIterBasedTrainer _run] INFO: Weight -0.380 Batch tensor([49, 50])\n",
      "[2023-01-25 00:19:11,815 4861:4384228736][runners.py:177 todd.CustomIterBasedTrainer _run] INFO: Iter [25/53] Loss 37.620\n",
      "[2023-01-25 00:19:11,819 4861:4384228736][runners.py:166 todd.CustomIterBasedTrainer _run] INFO: Weight 0.690 Batch tensor([59, 60])\n",
      "[2023-01-25 00:19:11,821 4861:4384228736][runners.py:177 todd.CustomIterBasedTrainer _run] INFO: Iter [30/53] Loss 82.110\n",
      "[2023-01-25 00:19:11,823 4861:4384228736][runners.py:166 todd.CustomIterBasedTrainer _run] INFO: Weight 0.100 Batch tensor([1, 2])\n",
      "[2023-01-25 00:19:11,825 4861:4384228736][runners.py:177 todd.CustomIterBasedTrainer _run] INFO: Iter [35/53] Loss 0.300\n",
      "[2023-01-25 00:19:11,828 4861:4384228736][runners.py:166 todd.CustomIterBasedTrainer _run] INFO: Weight 0.150 Batch tensor([11, 12])\n",
      "[2023-01-25 00:19:11,831 4861:4384228736][runners.py:177 todd.CustomIterBasedTrainer _run] INFO: Iter [40/53] Loss 3.450\n",
      "[2023-01-25 00:19:11,833 4861:4384228736][runners.py:275 todd.CustomIterBasedTrainer validate] INFO: Skipping validation since validator is undefined\n",
      "[2023-01-25 00:19:11,835 4861:4384228736][runners.py:201 todd.CustomIterBasedTrainer write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpy03p_0y3/iter_40.pth\n",
      "[2023-01-25 00:19:11,839 4861:4384228736][runners.py:166 todd.CustomIterBasedTrainer _run] INFO: Weight -0.160 Batch tensor([21, 22])\n",
      "[2023-01-25 00:19:11,841 4861:4384228736][runners.py:177 todd.CustomIterBasedTrainer _run] INFO: Iter [45/53] Loss 6.880\n",
      "[2023-01-25 00:19:11,844 4861:4384228736][runners.py:166 todd.CustomIterBasedTrainer _run] INFO: Weight 0.350 Batch tensor([31, 32])\n",
      "[2023-01-25 00:19:11,846 4861:4384228736][runners.py:177 todd.CustomIterBasedTrainer _run] INFO: Iter [50/53] Loss 22.050\n",
      "[2023-01-25 00:19:11,849 4861:4384228736][runners.py:201 todd.CustomIterBasedTrainer write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpy03p_0y3/latest.pth\n",
      "[2023-01-25 00:19:11,851 4861:4384228736][runners.py:275 todd.CustomIterBasedTrainer validate] INFO: Skipping validation since validator is undefined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iter_20.pth', 'iter_40.pth', 'latest.pth', '2023-01-25T00:19:11.764042+08:00.log']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    iter_based_trainer = trainer.copy()\n",
    "    iter_based_trainer.update(\n",
    "        type='CustomIterBasedTrainer',\n",
    "        name=work_dir,\n",
    "        iters=53,\n",
    "        state_dict=dict(interval=20),\n",
    "    )\n",
    "    runner = todd.utils.RunnerRegistry.build(iter_based_trainer, config)\n",
    "    cast(CustomIterBasedTrainer, runner).run()\n",
    "    print(os.listdir(work_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainers increment `todd.Store.ITER` to keep track of the training progress.\n",
    "If multiple trainers are to be run, `todd.Store.ITER` must be manually reset to zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.utils.RunnerRegistry.register()\n",
    "class CustomEpochBasedTrainer(RunnerMixin, todd.utils.EpochBasedTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-25 00:19:12,412 4861:4384228736][runners.py:71 todd.CustomEpochBasedTrainer __init__] DEBUG: Runner initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-25 00:19:12,426 4861:4384228736][runners.py:402 todd.CustomEpochBasedTrainer _run] INFO: Epoch [1/3] beginning\n",
      "[2023-01-25 00:19:12,435 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [5/34] Loss 1.330\n",
      "[2023-01-25 00:19:12,441 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [10/34] Loss 7.800\n",
      "[2023-01-25 00:19:12,445 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [15/34] Loss 15.930\n",
      "[2023-01-25 00:19:12,449 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [20/34] Loss 31.600\n",
      "[2023-01-25 00:19:12,455 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [25/34] Loss 46.530\n",
      "[2023-01-25 00:19:12,461 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [30/34] Loss 71.400\n",
      "[2023-01-25 00:19:12,468 4861:4384228736][runners.py:413 todd.CustomEpochBasedTrainer _run] INFO: Epoch [1/3] ended\n",
      "[2023-01-25 00:19:12,470 4861:4384228736][runners.py:275 todd.CustomEpochBasedTrainer validate] INFO: Skipping validation since validator is undefined\n",
      "[2023-01-25 00:19:12,472 4861:4384228736][runners.py:201 todd.CustomEpochBasedTrainer write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpneguxgqz/epoch_1.pth\n",
      "[2023-01-25 00:19:12,474 4861:4384228736][runners.py:402 todd.CustomEpochBasedTrainer _run] INFO: Epoch [2/3] beginning\n",
      "[2023-01-25 00:19:12,478 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [5/34] Loss 1.710\n",
      "[2023-01-25 00:19:12,488 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [10/34] Loss 7.020\n",
      "[2023-01-25 00:19:12,493 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [15/34] Loss 17.110\n",
      "[2023-01-25 00:19:12,497 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [20/34] Loss 30.020\n",
      "[2023-01-25 00:19:12,504 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [25/34] Loss 48.510\n",
      "[2023-01-25 00:19:12,506 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [30/34] Loss 69.020\n",
      "[2023-01-25 00:19:12,509 4861:4384228736][runners.py:413 todd.CustomEpochBasedTrainer _run] INFO: Epoch [2/3] ended\n",
      "[2023-01-25 00:19:12,510 4861:4384228736][runners.py:275 todd.CustomEpochBasedTrainer validate] INFO: Skipping validation since validator is undefined\n",
      "[2023-01-25 00:19:12,511 4861:4384228736][runners.py:201 todd.CustomEpochBasedTrainer write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpneguxgqz/epoch_2.pth\n",
      "[2023-01-25 00:19:12,513 4861:4384228736][runners.py:402 todd.CustomEpochBasedTrainer _run] INFO: Epoch [3/3] beginning\n",
      "[2023-01-25 00:19:12,516 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [5/34] Loss 1.710\n",
      "[2023-01-25 00:19:12,518 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [10/34] Loss 7.020\n",
      "[2023-01-25 00:19:12,521 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [15/34] Loss 17.110\n",
      "[2023-01-25 00:19:12,524 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [20/34] Loss 30.020\n",
      "[2023-01-25 00:19:12,526 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [25/34] Loss 48.510\n",
      "[2023-01-25 00:19:12,529 4861:4384228736][runners.py:177 todd.CustomEpochBasedTrainer _run] INFO: Iter [30/34] Loss 69.020\n",
      "[2023-01-25 00:19:12,531 4861:4384228736][runners.py:413 todd.CustomEpochBasedTrainer _run] INFO: Epoch [3/3] ended\n",
      "[2023-01-25 00:19:12,532 4861:4384228736][runners.py:275 todd.CustomEpochBasedTrainer validate] INFO: Skipping validation since validator is undefined\n",
      "[2023-01-25 00:19:12,533 4861:4384228736][runners.py:201 todd.CustomEpochBasedTrainer write_state_dict] INFO: Writing state dict to /var/folders/xg/wgfj92492d77cdnj5qrrhf380000gp/T/tmpneguxgqz/epoch_3.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['epoch_1.pth', '2023-01-25T00:19:12.412151+08:00.log', 'epoch_2.pth', 'epoch_3.pth']\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    epoch_based_trainer = trainer.copy()\n",
    "    epoch_based_trainer.update(\n",
    "        type='CustomEpochBasedTrainer',\n",
    "        name=work_dir,\n",
    "        epochs=3,\n",
    "    )\n",
    "    runner = todd.utils.RunnerRegistry.build(epoch_based_trainer, config)\n",
    "    cast(CustomEpochBasedTrainer, runner).run()\n",
    "    print(os.listdir(work_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `DRY_RUN` is enabled, the runner will stop upon the first log message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m[2023-01-25 00:19:12,731 4861:4384228736][runners.py:71 todd.CustomValidator __init__] DEBUG: Runner initialized by lutingwang@wangluting.local\u001b[m\n",
      "[2023-01-25 00:19:12,734 4861:4384228736][runners.py:177 todd.CustomValidator _run] INFO: Iter [5/20] Loss 0.050\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "    validator.name = work_dir\n",
    "    runner = todd.utils.RunnerRegistry.build(validator, config)\n",
    "    cast(CustomValidator, runner).run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
