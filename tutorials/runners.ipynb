{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Using Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: todd_ai 0.5.1\n",
      "Uninstalling todd_ai-0.5.1:\n",
      "  Successfully uninstalled todd_ai-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y todd_ai\n",
    "%pip install --extra-index-url https://pypi.org/simple .. > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, NoReturn, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import todd\n",
    "from todd.runners import Memo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.registries.ModelRegistry.register_()\n",
    "class RunnerModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._weight = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.nn.Parameter:\n",
    "        return self._weight\n",
    "\n",
    "    def _forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self._weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        runner: todd.runners.BaseRunner,\n",
    "        batch,\n",
    "        memo: Memo,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> Memo:\n",
    "        log: dict[str, Any] | None = memo.get(\"log\")\n",
    "        y = self._forward(batch[\"x\"])\n",
    "        loss = F.l1_loss(y, batch[\"y\"])\n",
    "        memo[\"loss\"] = loss\n",
    "        if log is not None:\n",
    "            log[\"batch\"] = str(batch)\n",
    "            log[\"weight\"] = f\"{self._weight.item():.3f}\"\n",
    "            log[\"loss\"] = f\"{loss:.3f}\"\n",
    "        return memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(TypedDict):\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.registries.DatasetRegistry.register_()\n",
    "class RunnerDataset(torch.utils.data.Dataset[int]):\n",
    "\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self._data = list(range(1, n + 1))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        x = self._data[index]\n",
    "        return Sample(x=x, y=x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(TypedDict):\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:31,521 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:31,524 34661:140704458489472][base.py:72 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[01;34m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmp0ygk1y8v\u001b[0m\n",
      "└── \u001b[01;34mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    logger=dict(),\n",
    "    callbacks=[],\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:32,735 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:32,737 34661:140704458489472][base.py:72 todd.Validator.validator __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:32,743 34661:140704458489472][log.py:87 todd.Validator.validator after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:32,746 34661:140704458489472][log.py:87 todd.Validator.validator after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-27 09:43:32,749 34661:140704458489472][log.py:87 todd.Validator.validator after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-27 09:43:32,752 34661:140704458489472][log.py:87 todd.Validator.validator after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[01;34m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpwrft9i68\u001b[0m\n",
      "└── \u001b[01;34mvalidator\u001b[0m\n",
      "\n",
      "2 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='validator',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[dict(type='LogCallback', interval=5)],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree $work_dirs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:33,969 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:33,972 34661:140704458489472][base.py:72 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:33,976 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [1/8] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:33,978 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [2/8] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.000 loss=9.000\n",
      "[2024-06-27 09:43:33,979 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [3/8] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.000 loss=14.000\n",
      "\u001b[2m[2024-06-27 09:43:33,972 34661:140704458489472][base.py:72 todd.IterBasedTrainer.iter_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:33,976 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [1/8] batch={'x': tensor([9, 1]), 'y': tensor([18,  2])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:33,978 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [2/8] batch={'x': tensor([7, 2]), 'y': tensor([14,  4])} weight=0.000 loss=9.000\n",
      "[2024-06-27 09:43:33,979 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [3/8] batch={'x': tensor([ 4, 10]), 'y': tensor([ 8, 20])} weight=0.000 loss=14.000\n",
      "[2024-06-27 09:43:33,981 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [4/8] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.000 loss=13.000\n",
      "[2024-06-27 09:43:33,982 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [5/8] batch={'x': tensor([3, 6]), 'y': tensor([ 6, 12])} weight=0.000 loss=9.000\n",
      "[2024-06-27 09:43:33,984 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [6/8] batch={'x': tensor([7, 9]), 'y': tensor([14, 18])} weight=0.000 loss=16.000\n",
      "[2024-06-27 09:43:33,986 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [7/8] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.000 loss=4.000\n",
      "[2024-06-27 09:43:34,028 34661:140704458489472][log.py:87 todd.IterBasedTrainer.iter_based_trainer after_run_iter] INFO: Iter [8/8] batch={'x': tensor([5, 8]), 'y': tensor([10, 16])} weight=0.000 loss=13.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"iter_based_trainer\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:34,049 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:34,051 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:34,052 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-27 09:43:34,057 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [1/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.000 loss=9.000\n",
      "[2024-06-27 09:43:34,058 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [2/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.000 loss=9.000\n",
      "\u001b[2m[2024-06-27 09:43:34,051 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.epoch_based_trainer __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:34,052 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-27 09:43:34,057 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [1/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.000 loss=9.000\n",
      "[2024-06-27 09:43:34,058 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [2/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.000 loss=9.000\n",
      "[2024-06-27 09:43:34,060 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [3/15] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.000 loss=3.000\n",
      "[2024-06-27 09:43:34,062 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [4/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.000 loss=17.000\n",
      "[2024-06-27 09:43:34,064 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [5/15] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.000 loss=17.000\n",
      "[2024-06-27 09:43:34,065 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-27 09:43:34,066 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [6/15] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.000 loss=17.000\n",
      "[2024-06-27 09:43:34,068 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [7/15] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.000 loss=9.000\n",
      "[2024-06-27 09:43:34,069 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [8/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.000 loss=15.000\n",
      "[2024-06-27 09:43:34,071 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [9/15] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.000 loss=3.000\n",
      "[2024-06-27 09:43:34,073 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [10/15] batch={'x': tensor([3, 8]), 'y': tensor([ 6, 16])} weight=0.000 loss=11.000\n",
      "[2024-06-27 09:43:34,074 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.epoch_based_trainer before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-27 09:43:34,076 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [11/15] batch={'x': tensor([3, 7]), 'y': tensor([ 6, 14])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:34,078 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [12/15] batch={'x': tensor([6, 5]), 'y': tensor([12, 10])} weight=0.000 loss=11.000\n",
      "[2024-06-27 09:43:34,079 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [13/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:34,081 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [14/15] batch={'x': tensor([10,  1]), 'y': tensor([20,  2])} weight=0.000 loss=11.000\n",
      "[2024-06-27 09:43:34,082 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.epoch_based_trainer after_run_iter] INFO: Iter [15/15] batch={'x': tensor([4, 9]), 'y': tensor([ 8, 18])} weight=0.000 loss=13.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"epoch_based_trainer\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[dict(type=\"LogCallback\", interval=1)],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:34,094 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-27 09:43:34,558 34661:140704458489472][log.py:49 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.10.0\n",
      "todd_version: 0.5.1\n",
      "cuda_home: None\n",
      "git_commit_id: adccc66\n",
      "git_status: M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-06-27 09:43:34,560 34661:140704458489472][base.py:72 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:34,564 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:34,566 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-27 09:43:34,568 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-27 09:43:34,571 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:34,591 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:34,594 34661:140704458489472][base.py:72 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:34,639 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:34,653 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-27 09:43:34,657 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-27 09:43:34,661 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[01;34m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpc_rzkwwm\u001b[0m\n",
      "└── \u001b[01;34mlog_callback\u001b[0m\n",
      "    └── \u001b[00m2024-06-27T09-43-34_593965-08-00.log\u001b[0m\n",
      "\n",
      "2 directories, 1 file\n",
      "\n",
      "\u001b[2m[2024-06-27 09:43:34,594 34661:140704458489472][base.py:72 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:34,639 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:34,653 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-27 09:43:34,657 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-27 09:43:34,661 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='Validator',\n",
    "    name='log_callback',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type='LogCallback',\n",
    "            interval=5,\n",
    "            with_file_handler=True,\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "    !cat {work_dirs}/log_callback/*.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:37,021 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:37,024 34661:140704458489472][base.py:72 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-06-27 09:43:37,024 34661:140704458489472][base.py:72 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:37,538 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:01 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:38,057 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:01 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-27 09:43:38,576 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-27 09:43:39,088 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1)\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:39,103 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:39,105 34661:140704458489472][base.py:72 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-06-27 09:43:39,105 34661:140704458489472][base.py:72 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:40,617 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:04 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:44,631 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:05 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-27 09:43:49,648 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:03 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-27 09:43:54,671 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            eta=dict(type=\"EMA_ETA\", ema=dict(decay=0.2)),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.strategy.module.register_forward_hook(\n",
    "        lambda *args, **kwargs: time.sleep(0.1 * min(10, runner.iter_))\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:54,686 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-27 09:43:54,817 34661:140704458489472][log.py:49 todd.Validator.log_callback init] INFO: \n",
      "platform: macOS-14.0\n",
      "nvidia_smi: None\n",
      "python_version: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "pytorch_version: 2.0.1\n",
      "torchvision_version: 0.15.2\n",
      "opencv_version: 4.10.0\n",
      "todd_version: 0.5.1\n",
      "cuda_home: None\n",
      "git_commit_id: adccc66\n",
      "git_status: M tutorials/runners.ipynb\n",
      "\u001b[2m[2024-06-27 09:43:54,820 34661:140704458489472][base.py:72 todd.Validator.log_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:54,825 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [5/20] ETA 0:00:00 batch={'x': tensor([5]), 'y': tensor([10])} weight=0.000 loss=10.000\n",
      "[2024-06-27 09:43:54,830 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [10/20] ETA 0:00:00 batch={'x': tensor([10]), 'y': tensor([20])} weight=0.000 loss=20.000\n",
      "[2024-06-27 09:43:54,833 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [15/20] ETA 0:00:00 batch={'x': tensor([15]), 'y': tensor([30])} weight=0.000 loss=30.000\n",
      "[2024-06-27 09:43:54,837 34661:140704458489472][log.py:87 todd.Validator.log_callback after_run_iter] INFO: Iter [20/20] ETA 0:00:00 batch={'x': tensor([20]), 'y': tensor([40])} weight=0.000 loss=40.000\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"log_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(\n",
    "            type=\"LogCallback\",\n",
    "            interval=5,\n",
    "            collect_env=dict(verbose=False),\n",
    "            with_file_handler=True,\n",
    "            eta=dict(type=\"AverageETA\"),\n",
    "        ),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:54,858 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-27 09:43:54,911 34661:140704458489472][git.py:43 todd.Validator.git_callback init] INFO: Saving git diff to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpeuttyanf/git_callback/git_diff_2024-06-27T09-43-54_911157-08-00.log\n",
      "\u001b[2m[2024-06-27 09:43:54,914 34661:140704458489472][base.py:72 todd.Validator.git_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"Validator\",\n",
    "    name=\"git_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(type=\"GitCallback\", diff='HEAD -- \":(exclude)*.ipynb\"'),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/git_callback/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:56,238 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:56,242 34661:140704458489472][base.py:72 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[2m[2024-06-27 09:43:56,242 34661:140704458489472][base.py:72 todd.IterBasedTrainer.optimize_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:56,252 34661:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.000 loss=18.000\n",
      "[2024-06-27 09:43:56,254 34661:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([5, 2]), 'y': tensor([10,  4])} weight=0.045 loss=6.842\n",
      "[2024-06-27 09:43:56,257 34661:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.062 loss=6.781\n",
      "[2024-06-27 09:43:56,260 34661:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.080 loss=7.680\n",
      "[2024-06-27 09:43:56,261 34661:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.100 loss=14.250\n",
      "[2024-06-27 09:43:56,264 34661:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([ 1, 10]), 'y': tensor([ 2, 20])} weight=0.137 loss=10.244\n",
      "[2024-06-27 09:43:56,266 34661:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.165 loss=8.257\n",
      "[2024-06-27 09:43:56,269 34661:140704458489472][log.py:87 todd.IterBasedTrainer.optimize_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.187 loss=9.062\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"optimize_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:56,285 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:56,290 34661:140704458489472][base.py:72 todd.IterBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:56,351 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.000 loss=8.000 lr=['1.667e-03']\n",
      "[2024-06-27 09:43:56,368 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([10,  6]), 'y': tensor([20, 12])} weight=0.007 loss=15.947 lr=['2.333e-03']\n",
      "[2024-06-27 09:43:56,371 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.025 loss=11.848 lr=['3.000e-03']\n",
      "[2024-06-27 09:43:56,374 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([2, 5]), 'y': tensor([ 4, 10])} weight=0.043 loss=6.848 lr=['3.667e-03']\n",
      "[2024-06-27 09:43:56,376 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.056 loss=11.663 lr=['4.333e-03']\n",
      "[2024-06-27 09:43:56,380 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([8, 1]), 'y': tensor([16,  2])} weight=0.082 loss=8.630 lr=['5.000e-03']\n",
      "[2024-06-27 09:43:56,382 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([4, 6]), 'y': tensor([ 8, 12])} weight=0.105 loss=9.477 lr=['5.000e-03']\n",
      "[2024-06-27 09:43:56,386 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.130 loss=13.092 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=5),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:56,404 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:56,408 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:56,410 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "\u001b[2m[2024-06-27 09:43:56,408 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.lr_schedule_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:56,410 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [1/5]\n",
      "[2024-06-27 09:43:56,413 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [1/10] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.000 loss=5.000 lr=['1.667e-03']\n",
      "[2024-06-27 09:43:56,416 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [2/10] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.004 loss=4.990 lr=['1.667e-03']\n",
      "[2024-06-27 09:43:56,417 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [2/5]\n",
      "[2024-06-27 09:43:56,422 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [3/10] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.008 loss=2.987 lr=['2.778e-03']\n",
      "[2024-06-27 09:43:56,425 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [4/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.013 loss=6.956 lr=['2.778e-03']\n",
      "[2024-06-27 09:43:56,426 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [3/5]\n",
      "[2024-06-27 09:43:56,430 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [5/10] batch={'x': tensor([2, 4]), 'y': tensor([4, 8])} weight=0.022 loss=5.933 lr=['3.889e-03']\n",
      "[2024-06-27 09:43:56,432 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [6/10] batch={'x': tensor([3, 1]), 'y': tensor([6, 2])} weight=0.034 loss=3.932 lr=['3.889e-03']\n",
      "[2024-06-27 09:43:56,433 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [4/5]\n",
      "[2024-06-27 09:43:56,438 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [7/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.042 loss=6.854 lr=['5.000e-03']\n",
      "[2024-06-27 09:43:56,440 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [8/10] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.059 loss=2.911 lr=['5.000e-03']\n",
      "[2024-06-27 09:43:56,441 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.lr_schedule_callback before_run_epoch] INFO: Epoch [5/5]\n",
      "[2024-06-27 09:43:56,445 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [9/10] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.067 loss=2.900 lr=['5.000e-03']\n",
      "[2024-06-27 09:43:56,447 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.lr_schedule_callback after_run_iter] INFO: Iter [10/10] batch={'x': tensor([3, 4]), 'y': tensor([6, 8])} weight=0.074 loss=6.740 lr=['5.000e-03']\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"lr_schedule_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=4),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScheduleCallback\",\n",
    "            lr_scheduler=dict(type=\"LinearLR\", total_iters=3),\n",
    "            by_epoch=True,\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=5,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:56,464 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-27 09:43:56,466 34661:140704458489472][lr.py:96 todd.IterBasedTrainer.lr_scale_callback _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-06-27 09:43:56,470 34661:140704458489472][base.py:72 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:56,466 34661:140704458489472][lr.py:96 todd.IterBasedTrainer.lr_scale_callback _scale_lr] INFO: base_batch_size=1 batch_size=2 lr_scaler=2.000\n",
      "\u001b[2m[2024-06-27 09:43:56,470 34661:140704458489472][base.py:72 todd.IterBasedTrainer.lr_scale_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:56,532 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.000 loss=6.000\n",
      "[2024-06-27 09:43:56,535 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.030 loss=5.910\n",
      "[2024-06-27 09:43:56,550 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.060 loss=12.610\n",
      "[2024-06-27 09:43:56,553 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.125 loss=11.250\n",
      "[2024-06-27 09:43:56,556 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.185 loss=16.335\n",
      "[2024-06-27 09:43:56,559 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.275 loss=2.588\n",
      "[2024-06-27 09:43:56,561 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.290 loss=11.970\n",
      "[2024-06-27 09:43:56,562 34661:140704458489472][log.py:87 todd.IterBasedTrainer.lr_scale_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([10,  7]), 'y': tensor([20, 14])} weight=0.360 loss=13.940\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"lr_scale_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type=\"OptimizeCallback\"),\n",
    "        dict(\n",
    "            type=\"LRScaleCallback\",\n",
    "            lr_scaler=dict(base_batch_size=1),\n",
    "        ),\n",
    "        dict(type=\"LogCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = todd.registries.RunnerRegistry.build(\n",
    "        config,\n",
    "        work_dir=dict(root=work_dirs),\n",
    "    )\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:56,587 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:56,593 34661:140704458489472][base.py:72 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:56,597 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/8] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.000 loss=11.000\n",
      "[2024-06-27 09:43:56,599 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_1\n",
      "[2024-06-27 09:43:56,605 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/8] batch={'x': tensor([1, 5]), 'y': tensor([ 2, 10])} weight=0.027 loss=5.918\n",
      "[2024-06-27 09:43:56,607 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-06-27 09:43:56,612 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/8] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.043 loss=9.788\n",
      "[2024-06-27 09:43:56,613 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_3\n",
      "[2024-06-27 09:43:56,618 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/8] batch={'x': tensor([6, 9]), 'y': tensor([12, 18])} weight=0.067 loss=14.494\n",
      "[2024-06-27 09:43:56,619 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-06-27 09:43:56,625 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/8] batch={'x': tensor([10,  3]), 'y': tensor([20,  6])} weight=0.105 loss=12.318\n",
      "[2024-06-27 09:43:56,626 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-06-27 09:43:56,630 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([6, 7]), 'y': tensor([12, 14])} weight=0.137 loss=12.106\n",
      "[2024-06-27 09:43:56,632 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-06-27 09:43:56,636 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.170 loss=10.980\n",
      "[2024-06-27 09:43:56,637 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-06-27 09:43:56,641 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([1, 2]), 'y': tensor([2, 4])} weight=0.200 loss=2.700\n",
      "[2024-06-27 09:43:56,642 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[01;34m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3\u001b[0m\n",
      "└── \u001b[01;34mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[01;34mcheckpoints\u001b[0m\n",
      "        ├── \u001b[01;34miter_1\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_2\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_3\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_4\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_5\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_6\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_7\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_8\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        └── \u001b[01;36mlatest\u001b[0m -> \u001b[01;34miter_8\u001b[0m\n",
      "\n",
      "12 directories, 40 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:58,677 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-27 09:43:58,678 34661:140704458489472][checkpoint.py:56 todd.IterBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_5\n",
      "[2024-06-27 09:43:58,681 34661:140704458489472][base.py:69 todd.IterBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-27 09:43:58,682 34661:140704458489472][base.py:72 todd.IterBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:58,687 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/8] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.137 loss=8.381\n",
      "[2024-06-27 09:43:58,687 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-06-27 09:43:58,693 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/8] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.160 loss=10.120\n",
      "[2024-06-27 09:43:58,694 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_7\n",
      "[2024-06-27 09:43:58,698 34661:140704458489472][log.py:87 todd.IterBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/8] batch={'x': tensor([7, 8]), 'y': tensor([14, 16])} weight=0.187 loss=13.594\n",
      "[2024-06-27 09:43:58,699 34661:140704458489472][checkpoint.py:82 todd.IterBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmplrmazqm3/checkpoint_callback/checkpoints/iter_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy.pth:\n",
      "{}\n",
      "\n",
      "optim.pth:\n",
      "{'param_groups': [{'dampening': 0,\n",
      "                   'differentiable': False,\n",
      "                   'foreach': None,\n",
      "                   'lr': 0.005,\n",
      "                   'maximize': False,\n",
      "                   'momentum': 0,\n",
      "                   'nesterov': False,\n",
      "                   'params': [0],\n",
      "                   'weight_decay': 0}],\n",
      " 'state': {0: {'momentum_buffer': None}}}\n",
      "\n",
      "meta.pth:\n",
      "{'iter_': 5}\n",
      "\n",
      "model.pth:\n",
      "OrderedDict([('_weight', tensor(0.1375))])\n",
      "\n",
      "callbacks.pth:\n",
      "{'callbacks': [{}, {}, {}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"IterBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    iters=8,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_5 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_5'\n",
    "    for f in iter_5.glob('*.pth'):\n",
    "        print(f'{f.name}:')\n",
    "        pprint(torch.load(f, 'cpu'))\n",
    "        print()\n",
    "\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_5),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:43:58,739 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:43:58,742 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:43:58,743 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-27 09:43:58,745 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.000 loss=13.000\n",
      "[2024-06-27 09:43:58,747 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([4, 8]), 'y': tensor([ 8, 16])} weight=0.032 loss=11.805\n",
      "[2024-06-27 09:43:58,748 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_2\n",
      "[2024-06-27 09:43:58,752 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([5, 7]), 'y': tensor([10, 14])} weight=0.062 loss=11.625\n",
      "[2024-06-27 09:43:58,754 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([2, 9]), 'y': tensor([ 4, 18])} weight=0.093 loss=10.491\n",
      "[2024-06-27 09:43:58,756 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_4\n",
      "[2024-06-27 09:43:58,760 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.120 loss=6.580\n",
      "[2024-06-27 09:43:58,761 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-27 09:43:58,763 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.138 loss=12.106\n",
      "[2024-06-27 09:43:58,764 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_6\n",
      "[2024-06-27 09:43:58,767 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.170 loss=7.320\n",
      "[2024-06-27 09:43:58,769 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.190 loss=4.525\n",
      "[2024-06-27 09:43:58,770 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-06-27 09:43:58,774 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.203 loss=11.684\n",
      "[2024-06-27 09:43:58,776 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.235 loss=14.120\n",
      "[2024-06-27 09:43:58,777 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-06-27 09:43:58,780 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-27 09:43:58,782 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])} weight=0.275 loss=14.663\n",
      "[2024-06-27 09:43:58,784 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.317 loss=9.254\n",
      "[2024-06-27 09:43:58,785 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-06-27 09:43:58,790 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([1, 4]), 'y': tensor([2, 8])} weight=0.345 loss=4.137\n",
      "[2024-06-27 09:43:58,792 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 9]), 'y': tensor([ 6, 18])} weight=0.357 loss=9.855\n",
      "[2024-06-27 09:43:58,793 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-06-27 09:43:58,797 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 8]), 'y': tensor([ 4, 16])} weight=0.387 loss=8.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[01;34m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8\u001b[0m\n",
      "└── \u001b[01;34mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[01;34mcheckpoints\u001b[0m\n",
      "        ├── \u001b[01;34miter_10\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_12\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_14\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_2\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_4\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_6\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34miter_8\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        └── \u001b[01;36mlatest\u001b[0m -> \u001b[01;34miter_14\u001b[0m\n",
      "\n",
      "11 directories, 35 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:44:00,881 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-27 09:44:00,883 34661:140704458489472][checkpoint.py:56 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_8\n",
      "[2024-06-27 09:44:00,889 34661:140704458489472][base.py:69 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-27 09:44:00,890 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:44:00,891 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-27 09:44:00,897 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.203 loss=13.481\n",
      "[2024-06-27 09:44:00,899 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.240 loss=8.800\n",
      "[2024-06-27 09:44:00,901 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-06-27 09:44:00,908 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-27 09:44:00,911 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([7, 3]), 'y': tensor([14,  6])} weight=0.265 loss=8.675\n",
      "[2024-06-27 09:44:00,913 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.290 loss=11.115\n",
      "[2024-06-27 09:44:00,914 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-06-27 09:44:00,922 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.322 loss=6.710\n",
      "[2024-06-27 09:44:00,925 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([10,  8]), 'y': tensor([20, 16])} weight=0.343 loss=14.918\n",
      "[2024-06-27 09:44:00,926 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-06-27 09:44:00,933 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([5, 1]), 'y': tensor([10,  2])} weight=0.387 loss=4.838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:44:02,898 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-27 09:44:02,900 34661:140704458489472][checkpoint.py:56 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-06-27 09:44:02,904 34661:140704458489472][base.py:69 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-27 09:44:02,905 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:44:02,906 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-27 09:44:02,909 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.265 loss=7.807\n",
      "[2024-06-27 09:44:02,900 34661:140704458489472][checkpoint.py:56 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_10\n",
      "[2024-06-27 09:44:02,904 34661:140704458489472][base.py:69 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-27 09:44:02,905 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:44:02,906 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-27 09:44:02,909 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([4, 5]), 'y': tensor([ 8, 10])} weight=0.265 loss=7.807\n",
      "[2024-06-27 09:44:02,911 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.287 loss=14.556\n",
      "[2024-06-27 09:44:02,912 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_12\n",
      "[2024-06-27 09:44:02,916 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([ 2, 10]), 'y': tensor([ 4, 20])} weight=0.330 loss=10.020\n",
      "[2024-06-27 09:44:02,918 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.360 loss=10.660\n",
      "[2024-06-27 09:44:02,919 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpe14g9bk8/checkpoint_callback/checkpoints/iter_14\n",
      "[2024-06-27 09:44:02,923 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.392 loss=3.215\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=2),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    iter_8 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_8'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_8),\n",
    "        )\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    iter_10 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'iter_10'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(iter_10),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:44:02,951 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:44:02,953 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:44:02,955 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-27 09:44:02,958 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [1/15] batch={'x': tensor([8, 9]), 'y': tensor([16, 18])} weight=0.000 loss=17.000\n",
      "[2024-06-27 09:44:02,959 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [2/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.043 loss=7.830\n",
      "[2024-06-27 09:44:02,961 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [3/15] batch={'x': tensor([2, 3]), 'y': tensor([4, 6])} weight=0.062 loss=4.844\n",
      "[2024-06-27 09:44:03,003 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [4/15] batch={'x': tensor([6, 4]), 'y': tensor([12,  8])} weight=0.075 loss=9.625\n",
      "[2024-06-27 09:44:03,006 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [5/15] batch={'x': tensor([10,  5]), 'y': tensor([20, 10])} weight=0.100 loss=14.250\n",
      "[2024-06-27 09:44:03,007 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpiztmcg4q/checkpoint_callback/checkpoints/epoch_1\n",
      "[2024-06-27 09:44:03,020 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-27 09:44:03,023 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [6/15] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.138 loss=2.794\n",
      "[2024-06-27 09:44:03,025 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [7/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.145 loss=10.202\n",
      "[2024-06-27 09:44:03,028 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [8/15] batch={'x': tensor([9, 8]), 'y': tensor([18, 16])} weight=0.172 loss=15.534\n",
      "[2024-06-27 09:44:03,030 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [9/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.215 loss=11.602\n",
      "[2024-06-27 09:44:03,033 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [10/15] batch={'x': tensor([5, 6]), 'y': tensor([10, 12])} weight=0.248 loss=9.639\n",
      "[2024-06-27 09:44:03,034 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpiztmcg4q/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-06-27 09:44:03,038 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-27 09:44:03,041 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([7, 6]), 'y': tensor([14, 12])} weight=0.275 loss=11.212\n",
      "[2024-06-27 09:44:03,043 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([9, 4]), 'y': tensor([18,  8])} weight=0.308 loss=11.001\n",
      "[2024-06-27 09:44:03,046 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.340 loss=2.490\n",
      "[2024-06-27 09:44:03,048 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([3, 5]), 'y': tensor([ 6, 10])} weight=0.347 loss=6.610\n",
      "[2024-06-27 09:44:03,051 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.368 loss=14.693\n",
      "[2024-06-27 09:44:03,053 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpiztmcg4q/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[01;34m/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpiztmcg4q\u001b[0m\n",
      "└── \u001b[01;34mcheckpoint_callback\u001b[0m\n",
      "    └── \u001b[01;34mcheckpoints\u001b[0m\n",
      "        ├── \u001b[01;34mepoch_1\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34mepoch_2\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        ├── \u001b[01;34mepoch_3\u001b[0m\n",
      "        │   ├── \u001b[00mcallbacks.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmeta.pth\u001b[0m\n",
      "        │   ├── \u001b[00mmodel.pth\u001b[0m\n",
      "        │   ├── \u001b[00moptim.pth\u001b[0m\n",
      "        │   └── \u001b[00mstrategy.pth\u001b[0m\n",
      "        └── \u001b[01;36mlatest\u001b[0m -> \u001b[01;34mepoch_3\u001b[0m\n",
      "\n",
      "7 directories, 15 files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:44:05,124 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "[2024-06-27 09:44:05,126 34661:140704458489472][checkpoint.py:56 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpiztmcg4q/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-06-27 09:44:05,131 34661:140704458489472][base.py:69 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-27 09:44:05,132 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:44:05,133 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-27 09:44:05,126 34661:140704458489472][checkpoint.py:56 todd.EpochBasedTrainer.checkpoint_callback init] INFO: Loading from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpiztmcg4q/checkpoint_callback/checkpoints/epoch_2\n",
      "[2024-06-27 09:44:05,131 34661:140704458489472][base.py:69 todd.EpochBasedTrainer.checkpoint_callback load_model_state_dict] INFO: <All keys matched successfully>\n",
      "\u001b[2m[2024-06-27 09:44:05,132 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.checkpoint_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:44:05,133 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.checkpoint_callback before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-27 09:44:05,139 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [11/15] batch={'x': tensor([5, 9]), 'y': tensor([10, 18])} weight=0.275 loss=12.075\n",
      "[2024-06-27 09:44:05,141 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [12/15] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.310 loss=9.295\n",
      "[2024-06-27 09:44:05,143 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [13/15] batch={'x': tensor([2, 1]), 'y': tensor([4, 2])} weight=0.338 loss=2.494\n",
      "[2024-06-27 09:44:05,148 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [14/15] batch={'x': tensor([6, 3]), 'y': tensor([12,  6])} weight=0.345 loss=7.448\n",
      "[2024-06-27 09:44:05,151 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.checkpoint_callback after_run_iter] INFO: Iter [15/15] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.368 loss=14.693\n",
      "[2024-06-27 09:44:05,154 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.checkpoint_callback _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpiztmcg4q/checkpoint_callback/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"checkpoint_callback\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !tree {work_dirs}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = pathlib.Path(work_dirs) / 'checkpoint_callback' / 'checkpoints' / 'epoch_2'\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(\n",
    "            config,\n",
    "            work_dir=dict(root=work_dirs),\n",
    "            load_from=str(epoch_2),\n",
    "        )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@todd.registries.RunnerRegistry.register_()\n",
    "class FaultyValidator(todd.runners.Validator):\n",
    "\n",
    "    def _run_iter(self, *args, **kwargs) -> NoReturn:\n",
    "        raise CustomError(\"faulty runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:44:05,264 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:44:05,268 34661:140704458489472][base.py:72 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-06-27 09:44:05,270 34661:140704458489472][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x1553e71d0>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 248, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_34661/454645826.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m[2024-06-27 09:44:05,268 34661:140704458489472][base.py:72 todd.FaultyValidator.monitor_callback __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "\u001b[1;31m[2024-06-27 09:44:05,270 34661:140704458489472][monitor.py:26 todd.FaultyValidator.monitor_callback __exit__] ERROR: Unable to run iter_=1\n",
      "batch={'x': tensor([1]), 'y': tensor([2])}\n",
      "memo={'dataloader': <torch.utils.data.dataloader.DataLoader object at 0x1553e71d0>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/.local/share/virtualenvs/todd-ARrcnwyq/lib/python3.11/site-packages/todd/runners/base.py\", line 248, in _run\n",
      "    memo = self._run_iter(batch, memo)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/ipykernel_34661/454645826.py\", line 5, in _run_iter\n",
      "    raise CustomError(\"faulty runner\")\n",
      "CustomError: faulty runner\u001b[m\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type='FaultyValidator',\n",
    "    name='monitor_callback',\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=20),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=1),\n",
    "    callbacks=[\n",
    "        dict(type='MonitorCallback'),\n",
    "        dict(type='LogCallback', interval=5, with_file_handler=True),\n",
    "    ],\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    try:\n",
    "        runner.run()\n",
    "    except CustomError as e:\n",
    "        pass\n",
    "\n",
    "    !echo\n",
    "    !cat {work_dirs}/monitor_callback/*.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:44:06,737 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:44:06,745 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:44:06,747 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-27 09:44:06,752 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([ 6, 10]), 'y': tensor([12, 20])} weight=0.000 loss=16.000\n",
      "[2024-06-27 09:44:06,756 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([7, 1]), 'y': tensor([14,  2])} weight=0.040 loss=7.840\n",
      "[2024-06-27 09:44:06,758 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([5, 4]), 'y': tensor([10,  8])} weight=0.060 loss=8.730\n",
      "[2024-06-27 09:44:06,761 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.082 loss=9.587\n",
      "[2024-06-27 09:44:06,764 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([9, 3]), 'y': tensor([18,  6])} weight=0.107 loss=11.355\n",
      "[2024-06-27 09:44:06,765 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfap2tj6v/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-06-27 09:44:06,770 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-27 09:44:06,774 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([3, 2]), 'y': tensor([6, 4])} weight=0.137 loss=4.656\n",
      "[2024-06-27 09:44:06,778 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([8, 4]), 'y': tensor([16,  8])} weight=0.150 loss=11.100\n",
      "[2024-06-27 09:44:06,780 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([9, 5]), 'y': tensor([18, 10])} weight=0.180 loss=12.740\n",
      "[2024-06-27 09:44:06,782 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([ 7, 10]), 'y': tensor([14, 20])} weight=0.215 loss=15.173\n",
      "[2024-06-27 09:44:06,785 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([6, 1]), 'y': tensor([12,  2])} weight=0.257 loss=6.099\n",
      "[2024-06-27 09:44:06,787 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfap2tj6v/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-06-27 09:44:06,792 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-27 09:44:06,795 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.275 loss=8.625\n",
      "[2024-06-27 09:44:06,797 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([4, 7]), 'y': tensor([ 8, 14])} weight=0.300 loss=9.350\n",
      "[2024-06-27 09:44:06,799 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([8, 5]), 'y': tensor([16, 10])} weight=0.328 loss=10.871\n",
      "[2024-06-27 09:44:06,801 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([ 3, 10]), 'y': tensor([ 6, 20])} weight=0.360 loss=10.660\n",
      "[2024-06-27 09:44:06,806 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.393 loss=6.430\n",
      "[2024-06-27 09:44:06,807 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfap2tj6v/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-27 09:44:08,837 34661:140704458489472][patches.py:190 todd _build] INFO: `worker_init_fn` is recommended to be `default`, instead of `None`.\n",
      "\u001b[2m[2024-06-27 09:44:08,839 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:44:08,840 34661:140704458489472][base.py:84 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfap2tj6v/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-06-27 09:44:08,842 34661:140704458489472][base.py:69 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "[2024-06-27 09:44:08,843 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-27 09:44:08,846 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.275 loss=8.625\n",
      "[2024-06-27 09:44:08,848 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.300 loss=6.800\n",
      "\u001b[2m[2024-06-27 09:44:08,839 34661:140704458489472][base.py:72 todd.EpochBasedTrainer.strategy_load_model_from __init__] DEBUG: Rank 0 initialized by bytedance@C02G870SMD6R\u001b[m\n",
      "[2024-06-27 09:44:08,840 34661:140704458489472][base.py:84 todd.EpochBasedTrainer.strategy_load_model_from load_model_from] INFO: Loading model from /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfap2tj6v/strategy_load_model_from/checkpoints/epoch_2/model.pth\n",
      "[2024-06-27 09:44:08,842 34661:140704458489472][base.py:69 todd.EpochBasedTrainer.strategy_load_model_from load_model_state_dict] INFO: <All keys matched successfully>\n",
      "[2024-06-27 09:44:08,843 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [1/3]\n",
      "[2024-06-27 09:44:08,846 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [1/15] batch={'x': tensor([1, 9]), 'y': tensor([ 2, 18])} weight=0.275 loss=8.625\n",
      "[2024-06-27 09:44:08,848 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [2/15] batch={'x': tensor([2, 6]), 'y': tensor([ 4, 12])} weight=0.300 loss=6.800\n",
      "[2024-06-27 09:44:08,850 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [3/15] batch={'x': tensor([8, 7]), 'y': tensor([16, 14])} weight=0.320 loss=12.600\n",
      "[2024-06-27 09:44:08,852 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [4/15] batch={'x': tensor([10,  5]), 'y': tensor([20, 10])} weight=0.358 loss=12.319\n",
      "[2024-06-27 09:44:08,854 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [5/15] batch={'x': tensor([4, 3]), 'y': tensor([8, 6])} weight=0.395 loss=5.618\n",
      "[2024-06-27 09:44:08,855 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfap2tj6v/strategy_load_model_from/checkpoints/epoch_1\n",
      "[2024-06-27 09:44:08,859 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [2/3]\n",
      "[2024-06-27 09:44:08,861 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [6/15] batch={'x': tensor([ 8, 10]), 'y': tensor([16, 20])} weight=0.413 loss=14.288\n",
      "[2024-06-27 09:44:08,862 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [7/15] batch={'x': tensor([4, 2]), 'y': tensor([8, 4])} weight=0.458 loss=4.628\n",
      "[2024-06-27 09:44:08,864 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [8/15] batch={'x': tensor([5, 3]), 'y': tensor([10,  6])} weight=0.472 loss=6.110\n",
      "[2024-06-27 09:44:08,866 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [9/15] batch={'x': tensor([1, 7]), 'y': tensor([ 2, 14])} weight=0.493 loss=6.030\n",
      "[2024-06-27 09:44:08,868 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [10/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.512 loss=11.156\n",
      "[2024-06-27 09:44:08,869 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfap2tj6v/strategy_load_model_from/checkpoints/epoch_2\n",
      "[2024-06-27 09:44:08,872 34661:140704458489472][log.py:93 todd.EpochBasedTrainer.strategy_load_model_from before_run_epoch] INFO: Epoch [3/3]\n",
      "[2024-06-27 09:44:08,874 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [11/15] batch={'x': tensor([1, 3]), 'y': tensor([2, 6])} weight=0.550 loss=2.900\n",
      "[2024-06-27 09:44:08,876 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [12/15] batch={'x': tensor([7, 4]), 'y': tensor([14,  8])} weight=0.560 loss=7.920\n",
      "[2024-06-27 09:44:08,878 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [13/15] batch={'x': tensor([8, 2]), 'y': tensor([16,  4])} weight=0.587 loss=7.062\n",
      "[2024-06-27 09:44:08,880 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [14/15] batch={'x': tensor([ 5, 10]), 'y': tensor([10, 20])} weight=0.612 loss=10.406\n",
      "[2024-06-27 09:44:08,881 34661:140704458489472][log.py:87 todd.EpochBasedTrainer.strategy_load_model_from after_run_iter] INFO: Iter [15/15] batch={'x': tensor([9, 6]), 'y': tensor([18, 12])} weight=0.650 loss=10.125\n",
      "[2024-06-27 09:44:08,882 34661:140704458489472][checkpoint.py:82 todd.EpochBasedTrainer.strategy_load_model_from _save] INFO: Saving state dict to /var/folders/v_/1kkfntxs5z74_rwvy1f3_mp80000gn/T/tmpfap2tj6v/strategy_load_model_from/checkpoints/epoch_3\n"
     ]
    }
   ],
   "source": [
    "config = todd.Config(\n",
    "    type=\"EpochBasedTrainer\",\n",
    "    name=\"strategy_load_model_from\",\n",
    "    strategy=dict(type='BaseStrategy'),\n",
    "    dataset=dict(type='RunnerDataset', n=10),\n",
    "    model=dict(type='RunnerModel'),\n",
    "    dataloader=dict(batch_size=2, shuffle=True),\n",
    "    callbacks=[\n",
    "        dict(type='OptimizeCallback'),\n",
    "        dict(type='LogCallback', interval=1),\n",
    "        dict(type=\"CheckpointCallback\", interval=1, by_epoch=True),\n",
    "    ],\n",
    "    optimizer=dict(type=\"SGD\", lr=0.005),\n",
    "    epochs=3,\n",
    "    logger=dict(),\n",
    ")\n",
    "with tempfile.TemporaryDirectory() as work_dirs:\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.run()\n",
    "\n",
    "    !echo\n",
    "    !echo {'-' * 20}\n",
    "    !echo\n",
    "\n",
    "    epoch_2 = (pathlib.Path(work_dirs) / 'strategy_load_model_from' / 'checkpoints' / 'epoch_2' / 'model.pth')\n",
    "    runner: todd.runners.BaseRunner = \\\n",
    "        todd.registries.RunnerRegistry.build(config, work_dir=dict(root=work_dirs))\n",
    "    runner.strategy.load_model_from(epoch_2)\n",
    "    runner.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "todd.Store.DRY_RUN = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "todd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe19504897982c0d86de0bd38ea30a541b47032e25039ac5ae6cd1de5b1a414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
